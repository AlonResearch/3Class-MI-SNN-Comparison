{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlonResearch/3Class-MI-SNN-Comparison/blob/main/MI3_SNNforMIeeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy9K5bJ9aB7D",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efa03f4-5b21-47ed-8dc8-c92006f6349d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m130.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "#Importing and installing required libs to run the SNN model (LENet converted)\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as da\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import scipy.io as scio\n",
        "!pip install spikingjelly -q\n",
        "from spikingjelly.activation_based import ann2snn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lAEV1BHaBp9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYNshZR5ZhDy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Data loader and other functions\n",
        "#Defining functions\n",
        "\n",
        "def data_loader(data, label, batch=64, shuffle=True, drop=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data to fit model.\n",
        "    Feed data into data_loader.\n",
        "    input:\n",
        "        data (float): samples*length*ch (samples*ch*length).\n",
        "        label (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        batch (int): batch size\n",
        "        shuffle (bool): shuffle data before input into decoder\n",
        "        drop (bool): drop the last samples if True\n",
        "    output:\n",
        "        data loader\n",
        "    \"\"\"\n",
        "    label = torch.LongTensor(label.flatten()).to(device)\n",
        "    if data.shape[1] >= data.shape[2]:\n",
        "        data = torch.tensor(data.swapaxes(1, 2))\n",
        "    data = torch.unsqueeze(data, dim=1).type('torch.FloatTensor').to(device)\n",
        "    data = da.TensorDataset(data, label)\n",
        "    loader = da.DataLoader(dataset=data, batch_size=batch, shuffle=shuffle, drop_last=drop)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def val_snn(Dec, test_loader, T=None):\n",
        "    Dec.eval().to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if T is not None:\n",
        "        corrects = np.zeros(T)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            if T is None:\n",
        "                outputs = Dec(inputs)\n",
        "                correct += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            else:\n",
        "                for m in Dec.modules():\n",
        "                    if hasattr(m, 'reset'):\n",
        "                        m.reset()\n",
        "                for t in range(T):\n",
        "                    if t == 0:\n",
        "                        outputs = Dec(inputs)\n",
        "                    else:\n",
        "                        outputs += Dec(inputs)\n",
        "                    corrects[t] += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            total += targets.shape[0]\n",
        "    return correct / total if T is None else corrects / total\n",
        "\n",
        "\n",
        "def anntosnn(cnn_model, train_x, train_y, test_x, test_y, batch=64, T=None):\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    print('---------------------------------------------')\n",
        "    print('Converting using MaxNorm')\n",
        "    model_converter = ann2snn.Converter(mode='max', dataloader=train_loader)\n",
        "    snn_model = model_converter(cnn_model)\n",
        "    mode_max_accs = val_snn(snn_model, test_loader, T=T)\n",
        "\n",
        "    return mode_max_accs\n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def train_ann(cnn_model, train_x, train_y, test_x, test_y, ep=500, batch=64):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        train_x, test_x (float): samples*length*ch (samples*ch*length).\n",
        "        train_y, test_y (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        ep (int): total train and test epoch\n",
        "        batch (int): batch size\n",
        "    output:\n",
        "        train acc, test acc, weight_file\n",
        "    \"\"\"\n",
        "    # Define training configuration\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.01)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ep)\n",
        "\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    for epoch in range(ep):\n",
        "        # Train ANN\n",
        "        cnn_model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        print('\\n')\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            print(batch_idx, len(train_loader), 'Epoch: %d | ANN: trainLoss: %.4f | trainAcc: %.4f%% (%d/%d)'\n",
        "                  % (epoch, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        train_acc.append(round(correct / total, 4))\n",
        "\n",
        "        # Test ANN\n",
        "        cnn_model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                outputs = cnn_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "                print(batch_idx, len(test_loader), 'Epoch: %d | ANN: testLoss: %.4f | testAcc: %.4f%% (%d/%d)'\n",
        "                      % (epoch, val_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        test_acc.append(round(correct / total, 4))\n",
        "\n",
        "    train_acc = np.asarray(train_acc[-1])\n",
        "    test_acc = np.asarray(test_acc[-1])\n",
        "    return train_acc, test_acc,cnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC8xjyPDaM6G",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Model definitions\n",
        "# Model 2a\n",
        "\n",
        "class LENet(nn.Module):\n",
        "    \"\"\"\n",
        "        LENet Model\n",
        "    input:\n",
        "         data shape as: batch_size*1*channel*length (64*1*22*1000) BCI IV-2a\n",
        "         batch_size：64\n",
        "         channel：22\n",
        "         length：1000\n",
        "    output:\n",
        "        classes_num\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes_num=3, channel_count=22, drop_out = 0.5):\n",
        "        super(LENet, self).__init__()\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,64) #\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,32) #\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,16) #\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            # Temporal Convolution block fusion kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            # Spatial Convolution block kernel_size (channel,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(channel_count, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            # Feature Fusion Convolution block kernel_size (1,16) and (1,1) #\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),  #\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.CCB = nn.Sequential(\n",
        "            # Classification Convolution block kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=classes_num,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "        x = self.CCB(x)\n",
        "        return x\n",
        "\n",
        "class LENet_FCL(nn.Module):\n",
        "    def __init__(self, classes_num=3, channel_count=60, drop_out=0.5):\n",
        "        super(LENet_FCL, self).__init__()\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        # Keep all the convolutional layers the same\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(channel_count, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        # We'll determine the size of the FC layer in the forward pass\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = None\n",
        "        self.classes_num = classes_num\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Create the FC layer on first forward pass if it doesn't exist\n",
        "        if self.fc is None:\n",
        "            in_features = x.shape[1]\n",
        "            self.fc = nn.Linear(in_features, self.classes_num).to(x.device)\n",
        "            # Initialize weights for the new layer\n",
        "            nn.init.kaiming_normal_(self.fc.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            if self.fc.bias is not None:\n",
        "                nn.init.constant_(self.fc.bias, 0)\n",
        "\n",
        "        # Apply the FC layer\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T0hmMue2aMqA",
        "outputId": "3104ad81-2a78-4149-9347-b4fe1292bd3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channel count: 62\n",
            "Data shape: (964, 62, 360)\n",
            "Label shape: (964, 1)\n",
            "Class distribution: Rest: 364, Elbow: 300, Hand: 300\n",
            "Balanced class distribution: Rest: 364, Elbow: 300, Hand: 300\n",
            "Balanced data shape: (964, 62, 360)\n",
            "Balanced label shape: (964, 1)\n"
          ]
        }
      ],
      "source": [
        "# @title Loading the data\n",
        "\"\"\"\n",
        "Loading the data\n",
        "\"\"\"\n",
        "\n",
        "# Getting real samples\n",
        "#Locally load the dataset\n",
        "#file = scio.loadmat('Datasets\\BCICIV_2a_gdf\\Derivatives\\A01T.mat')\n",
        "\n",
        "# Google Colab load the dataset\n",
        "file = scio.loadmat(\"/content/sub-003_eeg90hz_7-35.mat\")\n",
        "\n",
        "\n",
        "all_data = file['all_data']\n",
        "all_label = file['all_label']\n",
        "REDUCE_REST = 1 #In case your rest samples are being over represented\n",
        "# on the Confusion matrix and the other classes are not being properly\n",
        "# classified, reduce this value from 1 (100%), to an value that balances\n",
        "# the number of classes (e.g., 0.6 (60%))\n",
        "\n",
        "# Print data information\n",
        "channel_count = all_data.shape[1]\n",
        "num_classes = len(np.unique(all_label.flatten()))\n",
        "data_length = all_data.shape[2] # Assuming data is samples*ch*length\n",
        "print(f\"Channel count: {channel_count}\")\n",
        "print(f\"Data shape: {all_data.shape}\")\n",
        "print(f\"Label shape: {all_label.shape}\")\n",
        "print(f\"Class distribution: Rest: {np.sum(all_label == 0)}, Elbow: {np.sum(all_label == 1)}, Hand: {np.sum(all_label == 2)}\")\n",
        "\n",
        "# Address class imbalance for 'Rest' class (label 0)\n",
        "label_0_indices = np.where(all_label.flatten() == 0)[0]\n",
        "label_other_indices = np.where(all_label.flatten() != 0)[0]\n",
        "\n",
        "# Randomly select \"REDUCE_REST\"% of label 0 indices\n",
        "num_label_0_to_keep = int(len(label_0_indices) * REDUCE_REST)\n",
        "#np.random.seed(42) # for reproducibility\n",
        "selected_label_0_indices = np.random.choice(label_0_indices, size=num_label_0_to_keep, replace=False)\n",
        "\n",
        "# Combine selected label 0 indices with all other labels\n",
        "balanced_indices = np.concatenate((selected_label_0_indices, label_other_indices))\n",
        "np.random.shuffle(balanced_indices) # Shuffle the combined dataset\n",
        "\n",
        "all_data_balanced = all_data[balanced_indices]\n",
        "all_label_balanced = all_label[balanced_indices]\n",
        "\n",
        "print(f\"Balanced class distribution: Rest: {np.sum(all_label_balanced == 0)}, Elbow: {np.sum(all_label_balanced == 1)}, Hand: {np.sum(all_label_balanced == 2)}\")\n",
        "print(f\"Balanced data shape: {all_data_balanced.shape}\")\n",
        "print(f\"Balanced label shape: {all_label_balanced.shape}\")\n",
        "\n",
        "datasetX = torch.tensor(all_data_balanced, dtype=torch.float32)\n",
        "datasetY = torch.tensor(all_label_balanced, dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LENet to SNN Conversion Framework execution\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 64\n",
        "TIME_STEPS = 100  # T for SNN\n",
        "TEST_SIZE = 0.2\n",
        "DROP_OUT = 0.35\n",
        "\n",
        "\n",
        "# Split the data\n",
        "print(f\"{100 - (TEST_SIZE * 100)}% of the dataset is used for training and {TEST_SIZE * 100}% is used for testing.\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=TEST_SIZE, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# Initialize model\n",
        "cnn_model_lenet_ccb = LENet(classes_num=3, channel_count=channel_count, drop_out = DROP_OUT).to(device)\n",
        "cnn_model_lenet_ccb.apply(initialize_weights)\n",
        "\n",
        "# Train CNN model\n",
        "train_acc, test_acc,  cnn_model_lenet_ccb = train_ann(cnn_model_lenet_ccb, train_data, train_label, test_data, test_label,\n",
        "                                              ep=EPOCHS, batch=BATCH_SIZE)\n",
        "max_norm_acc = anntosnn( cnn_model_lenet_ccb, train_data, train_label, test_data, test_label,\n",
        "                        batch=BATCH_SIZE, T=TIME_STEPS)\n",
        "snn_model_lenet = ann2snn.Converter(mode='max', dataloader=data_loader(train_data, train_label, batch=BATCH_SIZE))( cnn_model_lenet_ccb)\n",
        "\n",
        "print('\\n')\n",
        "print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JiMa-ylsC2bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "262d05a0-3bf0-4d84-fd14-f02fdae5dc41",
        "cellView": "form"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "3 4 Epoch: 236 | ANN: testLoss: 0.7007 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 237 | ANN: trainLoss: 0.1912 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 237 | ANN: trainLoss: 0.1988 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 237 | ANN: trainLoss: 0.1975 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 237 | ANN: trainLoss: 0.2023 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 237 | ANN: trainLoss: 0.2019 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 237 | ANN: trainLoss: 0.2097 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 237 | ANN: trainLoss: 0.2290 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 237 | ANN: trainLoss: 0.2194 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 237 | ANN: trainLoss: 0.2240 | trainAcc: 89.5833% (516/576)\n",
            "9 13 Epoch: 237 | ANN: trainLoss: 0.2230 | trainAcc: 89.5312% (573/640)\n",
            "10 13 Epoch: 237 | ANN: trainLoss: 0.2238 | trainAcc: 89.7727% (632/704)\n",
            "11 13 Epoch: 237 | ANN: trainLoss: 0.2236 | trainAcc: 90.2344% (693/768)\n",
            "12 13 Epoch: 237 | ANN: trainLoss: 0.2068 | trainAcc: 90.2724% (696/771)\n",
            "0 4 Epoch: 237 | ANN: testLoss: 0.6078 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 237 | ANN: testLoss: 0.7767 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 237 | ANN: testLoss: 0.8793 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 237 | ANN: testLoss: 0.6599 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 238 | ANN: trainLoss: 0.3083 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 238 | ANN: trainLoss: 0.2801 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 238 | ANN: trainLoss: 0.2773 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 238 | ANN: trainLoss: 0.2707 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 238 | ANN: trainLoss: 0.2957 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 238 | ANN: trainLoss: 0.2760 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 238 | ANN: trainLoss: 0.2548 | trainAcc: 90.1786% (404/448)\n",
            "7 13 Epoch: 238 | ANN: trainLoss: 0.2475 | trainAcc: 90.2344% (462/512)\n",
            "8 13 Epoch: 238 | ANN: trainLoss: 0.2444 | trainAcc: 90.6250% (522/576)\n",
            "9 13 Epoch: 238 | ANN: trainLoss: 0.2447 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 238 | ANN: trainLoss: 0.2359 | trainAcc: 90.6250% (638/704)\n",
            "11 13 Epoch: 238 | ANN: trainLoss: 0.2318 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 238 | ANN: trainLoss: 0.2677 | trainAcc: 90.7912% (700/771)\n",
            "0 4 Epoch: 238 | ANN: testLoss: 0.7897 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 238 | ANN: testLoss: 0.7729 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 238 | ANN: testLoss: 0.8118 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 238 | ANN: testLoss: 0.6301 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 239 | ANN: trainLoss: 0.1960 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 239 | ANN: trainLoss: 0.1653 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 239 | ANN: trainLoss: 0.1631 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 239 | ANN: trainLoss: 0.1839 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 239 | ANN: trainLoss: 0.1846 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 239 | ANN: trainLoss: 0.1881 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 239 | ANN: trainLoss: 0.1857 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 239 | ANN: trainLoss: 0.1865 | trainAcc: 91.0156% (466/512)\n",
            "8 13 Epoch: 239 | ANN: trainLoss: 0.1875 | trainAcc: 90.9722% (524/576)\n",
            "9 13 Epoch: 239 | ANN: trainLoss: 0.1851 | trainAcc: 91.2500% (584/640)\n",
            "10 13 Epoch: 239 | ANN: trainLoss: 0.1883 | trainAcc: 91.3352% (643/704)\n",
            "11 13 Epoch: 239 | ANN: trainLoss: 0.1886 | trainAcc: 91.2760% (701/768)\n",
            "12 13 Epoch: 239 | ANN: trainLoss: 0.1940 | trainAcc: 91.1803% (703/771)\n",
            "0 4 Epoch: 239 | ANN: testLoss: 0.7772 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 239 | ANN: testLoss: 0.7815 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 239 | ANN: testLoss: 0.7648 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 239 | ANN: testLoss: 0.9341 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 240 | ANN: trainLoss: 0.1204 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 240 | ANN: trainLoss: 0.1361 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 240 | ANN: trainLoss: 0.1673 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 240 | ANN: trainLoss: 0.1619 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 240 | ANN: trainLoss: 0.1652 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 240 | ANN: trainLoss: 0.1670 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 240 | ANN: trainLoss: 0.1624 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 240 | ANN: trainLoss: 0.1668 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 240 | ANN: trainLoss: 0.1925 | trainAcc: 90.7986% (523/576)\n",
            "9 13 Epoch: 240 | ANN: trainLoss: 0.1969 | trainAcc: 90.1562% (577/640)\n",
            "10 13 Epoch: 240 | ANN: trainLoss: 0.1985 | trainAcc: 90.1989% (635/704)\n",
            "11 13 Epoch: 240 | ANN: trainLoss: 0.1916 | trainAcc: 90.6250% (696/768)\n",
            "12 13 Epoch: 240 | ANN: trainLoss: 0.2787 | trainAcc: 90.4021% (697/771)\n",
            "0 4 Epoch: 240 | ANN: testLoss: 0.8208 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 240 | ANN: testLoss: 0.7847 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 240 | ANN: testLoss: 0.7788 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 240 | ANN: testLoss: 0.5842 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 241 | ANN: trainLoss: 0.1762 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 241 | ANN: trainLoss: 0.1770 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 241 | ANN: trainLoss: 0.1527 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 241 | ANN: trainLoss: 0.1524 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 241 | ANN: trainLoss: 0.1514 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 241 | ANN: trainLoss: 0.1539 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 241 | ANN: trainLoss: 0.1500 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 241 | ANN: trainLoss: 0.1528 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 241 | ANN: trainLoss: 0.1577 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 241 | ANN: trainLoss: 0.1600 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 241 | ANN: trainLoss: 0.1701 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 241 | ANN: trainLoss: 0.1718 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 241 | ANN: trainLoss: 0.2452 | trainAcc: 92.4773% (713/771)\n",
            "0 4 Epoch: 241 | ANN: testLoss: 0.7935 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 241 | ANN: testLoss: 0.8417 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 241 | ANN: testLoss: 0.7563 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 241 | ANN: testLoss: 0.7366 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 242 | ANN: trainLoss: 0.1414 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 242 | ANN: trainLoss: 0.1427 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 242 | ANN: trainLoss: 0.1809 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 242 | ANN: trainLoss: 0.1729 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 242 | ANN: trainLoss: 0.1707 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 242 | ANN: trainLoss: 0.1677 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 242 | ANN: trainLoss: 0.1549 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 242 | ANN: trainLoss: 0.1540 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 242 | ANN: trainLoss: 0.1656 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 242 | ANN: trainLoss: 0.1654 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 242 | ANN: trainLoss: 0.1710 | trainAcc: 93.1818% (656/704)\n",
            "11 13 Epoch: 242 | ANN: trainLoss: 0.1738 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 242 | ANN: trainLoss: 0.3444 | trainAcc: 92.7367% (715/771)\n",
            "0 4 Epoch: 242 | ANN: testLoss: 0.7473 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 242 | ANN: testLoss: 0.6967 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 242 | ANN: testLoss: 0.7690 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 242 | ANN: testLoss: 0.5767 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 243 | ANN: trainLoss: 0.1995 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 243 | ANN: trainLoss: 0.1916 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 243 | ANN: trainLoss: 0.2022 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 243 | ANN: trainLoss: 0.1876 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 243 | ANN: trainLoss: 0.1770 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 243 | ANN: trainLoss: 0.1694 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 243 | ANN: trainLoss: 0.1798 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 243 | ANN: trainLoss: 0.1758 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 243 | ANN: trainLoss: 0.1837 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 243 | ANN: trainLoss: 0.1851 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 243 | ANN: trainLoss: 0.1921 | trainAcc: 91.0511% (641/704)\n",
            "11 13 Epoch: 243 | ANN: trainLoss: 0.1959 | trainAcc: 90.7552% (697/768)\n",
            "12 13 Epoch: 243 | ANN: trainLoss: 0.1827 | trainAcc: 90.7912% (700/771)\n",
            "0 4 Epoch: 243 | ANN: testLoss: 0.7319 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 243 | ANN: testLoss: 0.6773 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 243 | ANN: testLoss: 0.7782 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 243 | ANN: testLoss: 0.5846 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 244 | ANN: trainLoss: 0.2879 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 244 | ANN: trainLoss: 0.2128 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 244 | ANN: trainLoss: 0.2025 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 244 | ANN: trainLoss: 0.1945 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 244 | ANN: trainLoss: 0.1949 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 244 | ANN: trainLoss: 0.1883 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 244 | ANN: trainLoss: 0.1840 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 244 | ANN: trainLoss: 0.1827 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 244 | ANN: trainLoss: 0.1842 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 244 | ANN: trainLoss: 0.1831 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 244 | ANN: trainLoss: 0.1795 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 244 | ANN: trainLoss: 0.1870 | trainAcc: 92.0573% (707/768)\n",
            "12 13 Epoch: 244 | ANN: trainLoss: 0.1741 | trainAcc: 92.0882% (710/771)\n",
            "0 4 Epoch: 244 | ANN: testLoss: 0.6898 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 244 | ANN: testLoss: 0.7445 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 244 | ANN: testLoss: 0.8055 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 244 | ANN: testLoss: 0.6619 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 245 | ANN: trainLoss: 0.2233 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 245 | ANN: trainLoss: 0.2034 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 245 | ANN: trainLoss: 0.1653 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 245 | ANN: trainLoss: 0.1500 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 245 | ANN: trainLoss: 0.1567 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 245 | ANN: trainLoss: 0.1680 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 245 | ANN: trainLoss: 0.1778 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 245 | ANN: trainLoss: 0.1771 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 245 | ANN: trainLoss: 0.1749 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 245 | ANN: trainLoss: 0.1725 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 245 | ANN: trainLoss: 0.1693 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 245 | ANN: trainLoss: 0.1710 | trainAcc: 92.4479% (710/768)\n",
            "12 13 Epoch: 245 | ANN: trainLoss: 0.1715 | trainAcc: 92.4773% (713/771)\n",
            "0 4 Epoch: 245 | ANN: testLoss: 0.8918 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 245 | ANN: testLoss: 0.9080 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 245 | ANN: testLoss: 0.8183 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 245 | ANN: testLoss: 0.6919 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 246 | ANN: trainLoss: 0.1771 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 246 | ANN: trainLoss: 0.1879 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 246 | ANN: trainLoss: 0.1799 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 246 | ANN: trainLoss: 0.1613 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 246 | ANN: trainLoss: 0.1507 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 246 | ANN: trainLoss: 0.1655 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 246 | ANN: trainLoss: 0.1690 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 246 | ANN: trainLoss: 0.1678 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 246 | ANN: trainLoss: 0.1779 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 246 | ANN: trainLoss: 0.1709 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 246 | ANN: trainLoss: 0.1721 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 246 | ANN: trainLoss: 0.1743 | trainAcc: 93.3594% (717/768)\n",
            "12 13 Epoch: 246 | ANN: trainLoss: 0.2348 | trainAcc: 93.2555% (719/771)\n",
            "0 4 Epoch: 246 | ANN: testLoss: 0.9461 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 246 | ANN: testLoss: 0.8532 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 246 | ANN: testLoss: 0.8061 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 246 | ANN: testLoss: 0.6046 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 247 | ANN: trainLoss: 0.1699 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 247 | ANN: trainLoss: 0.1575 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 247 | ANN: trainLoss: 0.1623 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 247 | ANN: trainLoss: 0.1678 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 247 | ANN: trainLoss: 0.1687 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 247 | ANN: trainLoss: 0.1552 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 247 | ANN: trainLoss: 0.1612 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 247 | ANN: trainLoss: 0.1541 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 247 | ANN: trainLoss: 0.1666 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 247 | ANN: trainLoss: 0.1703 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 247 | ANN: trainLoss: 0.1679 | trainAcc: 93.1818% (656/704)\n",
            "11 13 Epoch: 247 | ANN: trainLoss: 0.1716 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 247 | ANN: trainLoss: 0.1774 | trainAcc: 93.2555% (719/771)\n",
            "0 4 Epoch: 247 | ANN: testLoss: 0.6867 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 247 | ANN: testLoss: 0.5922 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 247 | ANN: testLoss: 0.7975 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 247 | ANN: testLoss: 0.9578 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 248 | ANN: trainLoss: 0.1068 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 248 | ANN: trainLoss: 0.1171 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 248 | ANN: trainLoss: 0.1388 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 248 | ANN: trainLoss: 0.1400 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 248 | ANN: trainLoss: 0.1372 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 248 | ANN: trainLoss: 0.1443 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 248 | ANN: trainLoss: 0.1479 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 248 | ANN: trainLoss: 0.1674 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 248 | ANN: trainLoss: 0.1723 | trainAcc: 92.3611% (532/576)\n",
            "9 13 Epoch: 248 | ANN: trainLoss: 0.1820 | trainAcc: 92.3438% (591/640)\n",
            "10 13 Epoch: 248 | ANN: trainLoss: 0.1924 | trainAcc: 92.0455% (648/704)\n",
            "11 13 Epoch: 248 | ANN: trainLoss: 0.1913 | trainAcc: 91.9271% (706/768)\n",
            "12 13 Epoch: 248 | ANN: trainLoss: 0.4405 | trainAcc: 91.5694% (706/771)\n",
            "0 4 Epoch: 248 | ANN: testLoss: 0.8213 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 248 | ANN: testLoss: 0.8584 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 248 | ANN: testLoss: 0.8164 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 248 | ANN: testLoss: 0.6125 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 249 | ANN: trainLoss: 0.1456 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 249 | ANN: trainLoss: 0.1317 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 249 | ANN: trainLoss: 0.1492 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 249 | ANN: trainLoss: 0.1362 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 249 | ANN: trainLoss: 0.1526 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 249 | ANN: trainLoss: 0.1598 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 249 | ANN: trainLoss: 0.1630 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 249 | ANN: trainLoss: 0.1624 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 249 | ANN: trainLoss: 0.1546 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 249 | ANN: trainLoss: 0.1527 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 249 | ANN: trainLoss: 0.1504 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 249 | ANN: trainLoss: 0.1495 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 249 | ANN: trainLoss: 0.1678 | trainAcc: 93.6446% (722/771)\n",
            "0 4 Epoch: 249 | ANN: testLoss: 1.0322 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 249 | ANN: testLoss: 0.8917 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 249 | ANN: testLoss: 0.7941 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 249 | ANN: testLoss: 0.6082 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 250 | ANN: trainLoss: 0.1757 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 250 | ANN: trainLoss: 0.1867 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 250 | ANN: trainLoss: 0.1926 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 250 | ANN: trainLoss: 0.1749 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 250 | ANN: trainLoss: 0.1722 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 250 | ANN: trainLoss: 0.1731 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 250 | ANN: trainLoss: 0.1656 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 250 | ANN: trainLoss: 0.1791 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 250 | ANN: trainLoss: 0.1799 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 250 | ANN: trainLoss: 0.1862 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 250 | ANN: trainLoss: 0.1919 | trainAcc: 92.3295% (650/704)\n",
            "11 13 Epoch: 250 | ANN: trainLoss: 0.1919 | trainAcc: 92.0573% (707/768)\n",
            "12 13 Epoch: 250 | ANN: trainLoss: 0.1875 | trainAcc: 92.0882% (710/771)\n",
            "0 4 Epoch: 250 | ANN: testLoss: 0.7016 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 250 | ANN: testLoss: 0.7771 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 250 | ANN: testLoss: 0.8055 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 250 | ANN: testLoss: 0.6577 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 251 | ANN: trainLoss: 0.1288 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 251 | ANN: trainLoss: 0.1475 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 251 | ANN: trainLoss: 0.1789 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 251 | ANN: trainLoss: 0.1837 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 251 | ANN: trainLoss: 0.1776 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 251 | ANN: trainLoss: 0.1903 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 251 | ANN: trainLoss: 0.1852 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 251 | ANN: trainLoss: 0.2007 | trainAcc: 91.9922% (471/512)\n",
            "8 13 Epoch: 251 | ANN: trainLoss: 0.2030 | trainAcc: 91.4931% (527/576)\n",
            "9 13 Epoch: 251 | ANN: trainLoss: 0.1943 | trainAcc: 92.0312% (589/640)\n",
            "10 13 Epoch: 251 | ANN: trainLoss: 0.1871 | trainAcc: 92.4716% (651/704)\n",
            "11 13 Epoch: 251 | ANN: trainLoss: 0.1833 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 251 | ANN: trainLoss: 0.1815 | trainAcc: 92.7367% (715/771)\n",
            "0 4 Epoch: 251 | ANN: testLoss: 0.7745 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 251 | ANN: testLoss: 0.7388 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 251 | ANN: testLoss: 0.7598 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 251 | ANN: testLoss: 0.5699 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 252 | ANN: trainLoss: 0.0939 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 252 | ANN: trainLoss: 0.1617 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 252 | ANN: trainLoss: 0.1644 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 252 | ANN: trainLoss: 0.1719 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 252 | ANN: trainLoss: 0.1771 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 252 | ANN: trainLoss: 0.1701 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 252 | ANN: trainLoss: 0.1790 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 252 | ANN: trainLoss: 0.1794 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 252 | ANN: trainLoss: 0.1772 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 252 | ANN: trainLoss: 0.1718 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 252 | ANN: trainLoss: 0.1742 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 252 | ANN: trainLoss: 0.1742 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 252 | ANN: trainLoss: 0.3399 | trainAcc: 92.3476% (712/771)\n",
            "0 4 Epoch: 252 | ANN: testLoss: 0.7366 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 252 | ANN: testLoss: 0.7577 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 252 | ANN: testLoss: 0.7464 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 252 | ANN: testLoss: 1.6117 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 253 | ANN: trainLoss: 0.1282 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 253 | ANN: trainLoss: 0.1229 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 253 | ANN: trainLoss: 0.1278 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 253 | ANN: trainLoss: 0.1184 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 253 | ANN: trainLoss: 0.1322 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 253 | ANN: trainLoss: 0.1565 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 253 | ANN: trainLoss: 0.1645 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 253 | ANN: trainLoss: 0.1680 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 253 | ANN: trainLoss: 0.1713 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 253 | ANN: trainLoss: 0.1785 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 253 | ANN: trainLoss: 0.1790 | trainAcc: 92.4716% (651/704)\n",
            "11 13 Epoch: 253 | ANN: trainLoss: 0.1781 | trainAcc: 92.3177% (709/768)\n",
            "12 13 Epoch: 253 | ANN: trainLoss: 0.1785 | trainAcc: 92.3476% (712/771)\n",
            "0 4 Epoch: 253 | ANN: testLoss: 0.7847 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 253 | ANN: testLoss: 0.9656 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 253 | ANN: testLoss: 0.7981 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 253 | ANN: testLoss: 1.1882 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 254 | ANN: trainLoss: 0.1275 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 254 | ANN: trainLoss: 0.1281 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 254 | ANN: trainLoss: 0.1474 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 254 | ANN: trainLoss: 0.1510 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 254 | ANN: trainLoss: 0.1443 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 254 | ANN: trainLoss: 0.1600 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 254 | ANN: trainLoss: 0.1556 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 254 | ANN: trainLoss: 0.1633 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 254 | ANN: trainLoss: 0.1713 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 254 | ANN: trainLoss: 0.1773 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 254 | ANN: trainLoss: 0.1811 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 254 | ANN: trainLoss: 0.1765 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 254 | ANN: trainLoss: 0.1907 | trainAcc: 92.6070% (714/771)\n",
            "0 4 Epoch: 254 | ANN: testLoss: 0.4721 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 254 | ANN: testLoss: 0.7104 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 254 | ANN: testLoss: 0.7457 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 254 | ANN: testLoss: 1.1786 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 255 | ANN: trainLoss: 0.1503 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 255 | ANN: trainLoss: 0.1814 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 255 | ANN: trainLoss: 0.1840 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 255 | ANN: trainLoss: 0.1845 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 255 | ANN: trainLoss: 0.1818 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 255 | ANN: trainLoss: 0.1833 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 255 | ANN: trainLoss: 0.1817 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 255 | ANN: trainLoss: 0.1738 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 255 | ANN: trainLoss: 0.1732 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 255 | ANN: trainLoss: 0.1689 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 255 | ANN: trainLoss: 0.1626 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 255 | ANN: trainLoss: 0.1629 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 255 | ANN: trainLoss: 0.1852 | trainAcc: 93.6446% (722/771)\n",
            "0 4 Epoch: 255 | ANN: testLoss: 1.0006 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 255 | ANN: testLoss: 0.8594 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 255 | ANN: testLoss: 0.8045 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 255 | ANN: testLoss: 0.6035 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 256 | ANN: trainLoss: 0.1738 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 256 | ANN: trainLoss: 0.2006 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 256 | ANN: trainLoss: 0.1864 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 256 | ANN: trainLoss: 0.1776 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 256 | ANN: trainLoss: 0.1676 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 256 | ANN: trainLoss: 0.1600 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 256 | ANN: trainLoss: 0.1622 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 256 | ANN: trainLoss: 0.1614 | trainAcc: 91.7969% (470/512)\n",
            "8 13 Epoch: 256 | ANN: trainLoss: 0.1638 | trainAcc: 92.1875% (531/576)\n",
            "9 13 Epoch: 256 | ANN: trainLoss: 0.1638 | trainAcc: 92.1875% (590/640)\n",
            "10 13 Epoch: 256 | ANN: trainLoss: 0.1578 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 256 | ANN: trainLoss: 0.1569 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 256 | ANN: trainLoss: 0.2538 | trainAcc: 92.6070% (714/771)\n",
            "0 4 Epoch: 256 | ANN: testLoss: 0.8693 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 256 | ANN: testLoss: 0.7485 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 256 | ANN: testLoss: 0.7394 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 256 | ANN: testLoss: 0.5742 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 257 | ANN: trainLoss: 0.2161 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 257 | ANN: trainLoss: 0.1779 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 257 | ANN: trainLoss: 0.1651 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 257 | ANN: trainLoss: 0.1636 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 257 | ANN: trainLoss: 0.1622 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 257 | ANN: trainLoss: 0.1765 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 257 | ANN: trainLoss: 0.1854 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 257 | ANN: trainLoss: 0.1783 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 257 | ANN: trainLoss: 0.1730 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 257 | ANN: trainLoss: 0.1838 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 257 | ANN: trainLoss: 0.1770 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 257 | ANN: trainLoss: 0.1743 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 257 | ANN: trainLoss: 0.2002 | trainAcc: 93.1258% (718/771)\n",
            "0 4 Epoch: 257 | ANN: testLoss: 0.8085 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 257 | ANN: testLoss: 0.8548 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 257 | ANN: testLoss: 0.7666 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 257 | ANN: testLoss: 0.9928 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 258 | ANN: trainLoss: 0.1157 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 258 | ANN: trainLoss: 0.1330 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 258 | ANN: trainLoss: 0.1301 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 258 | ANN: trainLoss: 0.1483 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 258 | ANN: trainLoss: 0.1578 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 258 | ANN: trainLoss: 0.1513 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 258 | ANN: trainLoss: 0.1480 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 258 | ANN: trainLoss: 0.1496 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 258 | ANN: trainLoss: 0.1495 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 258 | ANN: trainLoss: 0.1554 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 258 | ANN: trainLoss: 0.1572 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 258 | ANN: trainLoss: 0.1603 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 258 | ANN: trainLoss: 0.1876 | trainAcc: 93.5149% (721/771)\n",
            "0 4 Epoch: 258 | ANN: testLoss: 0.9557 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 258 | ANN: testLoss: 0.8366 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 258 | ANN: testLoss: 0.7846 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 258 | ANN: testLoss: 1.2896 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 259 | ANN: trainLoss: 0.1830 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 259 | ANN: trainLoss: 0.1560 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 259 | ANN: trainLoss: 0.1454 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 259 | ANN: trainLoss: 0.1367 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 259 | ANN: trainLoss: 0.1413 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 259 | ANN: trainLoss: 0.1445 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 259 | ANN: trainLoss: 0.1418 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 259 | ANN: trainLoss: 0.1515 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 259 | ANN: trainLoss: 0.1476 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 259 | ANN: trainLoss: 0.1508 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 259 | ANN: trainLoss: 0.1585 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 259 | ANN: trainLoss: 0.1586 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 259 | ANN: trainLoss: 0.1550 | trainAcc: 92.9961% (717/771)\n",
            "0 4 Epoch: 259 | ANN: testLoss: 0.9403 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 259 | ANN: testLoss: 0.8479 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 259 | ANN: testLoss: 0.8301 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 259 | ANN: testLoss: 0.6227 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 260 | ANN: trainLoss: 0.1698 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 260 | ANN: trainLoss: 0.1571 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 260 | ANN: trainLoss: 0.1658 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 260 | ANN: trainLoss: 0.1671 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 260 | ANN: trainLoss: 0.1695 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 260 | ANN: trainLoss: 0.1709 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 260 | ANN: trainLoss: 0.1744 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 260 | ANN: trainLoss: 0.1673 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 260 | ANN: trainLoss: 0.1693 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 260 | ANN: trainLoss: 0.1716 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 260 | ANN: trainLoss: 0.1779 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 260 | ANN: trainLoss: 0.1693 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 260 | ANN: trainLoss: 0.3433 | trainAcc: 92.8664% (716/771)\n",
            "0 4 Epoch: 260 | ANN: testLoss: 0.7944 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 260 | ANN: testLoss: 0.9450 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 260 | ANN: testLoss: 0.8246 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 260 | ANN: testLoss: 0.6190 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 261 | ANN: trainLoss: 0.1047 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 261 | ANN: trainLoss: 0.1313 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 261 | ANN: trainLoss: 0.1390 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 261 | ANN: trainLoss: 0.1293 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 261 | ANN: trainLoss: 0.1321 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 261 | ANN: trainLoss: 0.1419 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 261 | ANN: trainLoss: 0.1465 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 261 | ANN: trainLoss: 0.1528 | trainAcc: 92.5781% (474/512)\n",
            "8 13 Epoch: 261 | ANN: trainLoss: 0.1513 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 261 | ANN: trainLoss: 0.1520 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 261 | ANN: trainLoss: 0.1503 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 261 | ANN: trainLoss: 0.1512 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 261 | ANN: trainLoss: 0.1405 | trainAcc: 92.7367% (715/771)\n",
            "0 4 Epoch: 261 | ANN: testLoss: 0.8657 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 261 | ANN: testLoss: 0.7931 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 261 | ANN: testLoss: 0.7938 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 261 | ANN: testLoss: 0.5954 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 262 | ANN: trainLoss: 0.1720 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 262 | ANN: trainLoss: 0.1507 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 262 | ANN: trainLoss: 0.1593 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 262 | ANN: trainLoss: 0.1727 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 262 | ANN: trainLoss: 0.1566 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 262 | ANN: trainLoss: 0.1541 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 262 | ANN: trainLoss: 0.1463 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 262 | ANN: trainLoss: 0.1426 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 262 | ANN: trainLoss: 0.1491 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 262 | ANN: trainLoss: 0.1531 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 262 | ANN: trainLoss: 0.1551 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 262 | ANN: trainLoss: 0.1545 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 262 | ANN: trainLoss: 0.1567 | trainAcc: 93.2555% (719/771)\n",
            "0 4 Epoch: 262 | ANN: testLoss: 0.8622 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 262 | ANN: testLoss: 0.7923 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 262 | ANN: testLoss: 0.7994 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 262 | ANN: testLoss: 0.5996 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 263 | ANN: trainLoss: 0.1612 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 263 | ANN: trainLoss: 0.1733 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 263 | ANN: trainLoss: 0.1674 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 263 | ANN: trainLoss: 0.1526 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 263 | ANN: trainLoss: 0.1599 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 263 | ANN: trainLoss: 0.1517 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 263 | ANN: trainLoss: 0.1588 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 263 | ANN: trainLoss: 0.1567 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 263 | ANN: trainLoss: 0.1584 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 263 | ANN: trainLoss: 0.1517 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 263 | ANN: trainLoss: 0.1547 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 263 | ANN: trainLoss: 0.1528 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 263 | ANN: trainLoss: 0.1630 | trainAcc: 93.9040% (724/771)\n",
            "0 4 Epoch: 263 | ANN: testLoss: 0.8063 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 263 | ANN: testLoss: 0.7629 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 263 | ANN: testLoss: 0.8387 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 263 | ANN: testLoss: 1.9687 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 264 | ANN: trainLoss: 0.1612 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 264 | ANN: trainLoss: 0.1378 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 264 | ANN: trainLoss: 0.1393 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 264 | ANN: trainLoss: 0.1334 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 264 | ANN: trainLoss: 0.1252 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 264 | ANN: trainLoss: 0.1205 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 264 | ANN: trainLoss: 0.1273 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 264 | ANN: trainLoss: 0.1255 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 264 | ANN: trainLoss: 0.1337 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 264 | ANN: trainLoss: 0.1515 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 264 | ANN: trainLoss: 0.1512 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 264 | ANN: trainLoss: 0.1481 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 264 | ANN: trainLoss: 0.1700 | trainAcc: 94.2931% (727/771)\n",
            "0 4 Epoch: 264 | ANN: testLoss: 0.8000 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 264 | ANN: testLoss: 0.8117 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 264 | ANN: testLoss: 0.8857 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 264 | ANN: testLoss: 0.6645 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 265 | ANN: trainLoss: 0.1790 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 265 | ANN: trainLoss: 0.1560 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 265 | ANN: trainLoss: 0.1501 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 265 | ANN: trainLoss: 0.1497 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 265 | ANN: trainLoss: 0.1513 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 265 | ANN: trainLoss: 0.1563 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 265 | ANN: trainLoss: 0.1628 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 265 | ANN: trainLoss: 0.1559 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 265 | ANN: trainLoss: 0.1639 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 265 | ANN: trainLoss: 0.1611 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 265 | ANN: trainLoss: 0.1561 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 265 | ANN: trainLoss: 0.1608 | trainAcc: 93.0990% (715/768)\n",
            "12 13 Epoch: 265 | ANN: trainLoss: 0.1499 | trainAcc: 93.1258% (718/771)\n",
            "0 4 Epoch: 265 | ANN: testLoss: 0.5935 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 265 | ANN: testLoss: 0.6536 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 265 | ANN: testLoss: 0.8103 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 265 | ANN: testLoss: 0.6592 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 266 | ANN: trainLoss: 0.1377 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 266 | ANN: trainLoss: 0.1318 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 266 | ANN: trainLoss: 0.1351 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 266 | ANN: trainLoss: 0.1377 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 266 | ANN: trainLoss: 0.1383 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 266 | ANN: trainLoss: 0.1396 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 266 | ANN: trainLoss: 0.1427 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 266 | ANN: trainLoss: 0.1328 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 266 | ANN: trainLoss: 0.1307 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 266 | ANN: trainLoss: 0.1311 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 266 | ANN: trainLoss: 0.1355 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 266 | ANN: trainLoss: 0.1392 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 266 | ANN: trainLoss: 0.2048 | trainAcc: 94.2931% (727/771)\n",
            "0 4 Epoch: 266 | ANN: testLoss: 0.7360 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 266 | ANN: testLoss: 0.8515 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 266 | ANN: testLoss: 0.7781 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 266 | ANN: testLoss: 0.5836 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 267 | ANN: trainLoss: 0.0679 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 267 | ANN: trainLoss: 0.0928 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 267 | ANN: trainLoss: 0.1257 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 267 | ANN: trainLoss: 0.1388 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 267 | ANN: trainLoss: 0.1623 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 267 | ANN: trainLoss: 0.1504 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 267 | ANN: trainLoss: 0.1505 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 267 | ANN: trainLoss: 0.1552 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 267 | ANN: trainLoss: 0.1562 | trainAcc: 92.3611% (532/576)\n",
            "9 13 Epoch: 267 | ANN: trainLoss: 0.1604 | trainAcc: 91.8750% (588/640)\n",
            "10 13 Epoch: 267 | ANN: trainLoss: 0.1693 | trainAcc: 91.4773% (644/704)\n",
            "11 13 Epoch: 267 | ANN: trainLoss: 0.1710 | trainAcc: 91.5365% (703/768)\n",
            "12 13 Epoch: 267 | ANN: trainLoss: 0.1598 | trainAcc: 91.5694% (706/771)\n",
            "0 4 Epoch: 267 | ANN: testLoss: 1.1451 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 267 | ANN: testLoss: 0.9509 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 267 | ANN: testLoss: 0.8349 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 267 | ANN: testLoss: 1.8688 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 268 | ANN: trainLoss: 0.1306 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 268 | ANN: trainLoss: 0.1313 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 268 | ANN: trainLoss: 0.1715 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 268 | ANN: trainLoss: 0.1668 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 268 | ANN: trainLoss: 0.1547 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 268 | ANN: trainLoss: 0.1541 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 268 | ANN: trainLoss: 0.1505 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 268 | ANN: trainLoss: 0.1513 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 268 | ANN: trainLoss: 0.1455 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 268 | ANN: trainLoss: 0.1533 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 268 | ANN: trainLoss: 0.1544 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 268 | ANN: trainLoss: 0.1698 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 268 | ANN: trainLoss: 0.3275 | trainAcc: 92.4773% (713/771)\n",
            "0 4 Epoch: 268 | ANN: testLoss: 0.7536 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 268 | ANN: testLoss: 0.8131 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 268 | ANN: testLoss: 0.8581 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 268 | ANN: testLoss: 1.4420 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 269 | ANN: trainLoss: 0.1495 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 269 | ANN: trainLoss: 0.1346 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 269 | ANN: trainLoss: 0.1434 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 269 | ANN: trainLoss: 0.1478 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 269 | ANN: trainLoss: 0.1551 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 269 | ANN: trainLoss: 0.1418 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 269 | ANN: trainLoss: 0.1404 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 269 | ANN: trainLoss: 0.1372 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 269 | ANN: trainLoss: 0.1370 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 269 | ANN: trainLoss: 0.1369 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 269 | ANN: trainLoss: 0.1320 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 269 | ANN: trainLoss: 0.1440 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 269 | ANN: trainLoss: 0.3013 | trainAcc: 93.3852% (720/771)\n",
            "0 4 Epoch: 269 | ANN: testLoss: 0.7503 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 269 | ANN: testLoss: 0.8287 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 269 | ANN: testLoss: 0.8497 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 269 | ANN: testLoss: 0.6374 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 270 | ANN: trainLoss: 0.1200 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 270 | ANN: trainLoss: 0.1894 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 270 | ANN: trainLoss: 0.1558 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 270 | ANN: trainLoss: 0.1931 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 270 | ANN: trainLoss: 0.1735 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 270 | ANN: trainLoss: 0.1773 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 270 | ANN: trainLoss: 0.1734 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 270 | ANN: trainLoss: 0.1724 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 270 | ANN: trainLoss: 0.1633 | trainAcc: 92.8819% (535/576)\n",
            "9 13 Epoch: 270 | ANN: trainLoss: 0.1613 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 270 | ANN: trainLoss: 0.1641 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 270 | ANN: trainLoss: 0.1588 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 270 | ANN: trainLoss: 0.1545 | trainAcc: 92.8664% (716/771)\n",
            "0 4 Epoch: 270 | ANN: testLoss: 0.7433 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 270 | ANN: testLoss: 0.8845 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 270 | ANN: testLoss: 0.8225 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 270 | ANN: testLoss: 0.8219 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 271 | ANN: trainLoss: 0.1062 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 271 | ANN: trainLoss: 0.1107 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 271 | ANN: trainLoss: 0.1235 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 271 | ANN: trainLoss: 0.1084 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 271 | ANN: trainLoss: 0.1211 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 271 | ANN: trainLoss: 0.1393 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 271 | ANN: trainLoss: 0.1452 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 271 | ANN: trainLoss: 0.1515 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 271 | ANN: trainLoss: 0.1518 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 271 | ANN: trainLoss: 0.1476 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 271 | ANN: trainLoss: 0.1487 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 271 | ANN: trainLoss: 0.1455 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 271 | ANN: trainLoss: 0.2405 | trainAcc: 93.5149% (721/771)\n",
            "0 4 Epoch: 271 | ANN: testLoss: 0.8320 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 271 | ANN: testLoss: 0.7585 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 271 | ANN: testLoss: 0.8223 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 271 | ANN: testLoss: 1.5016 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 272 | ANN: trainLoss: 0.1355 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 272 | ANN: trainLoss: 0.1771 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 272 | ANN: trainLoss: 0.1785 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 272 | ANN: trainLoss: 0.1857 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 272 | ANN: trainLoss: 0.1760 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 272 | ANN: trainLoss: 0.1610 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 272 | ANN: trainLoss: 0.1653 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 272 | ANN: trainLoss: 0.1679 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 272 | ANN: trainLoss: 0.1661 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 272 | ANN: trainLoss: 0.1612 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 272 | ANN: trainLoss: 0.1540 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 272 | ANN: trainLoss: 0.1515 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 272 | ANN: trainLoss: 0.1454 | trainAcc: 94.4228% (728/771)\n",
            "0 4 Epoch: 272 | ANN: testLoss: 0.6860 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 272 | ANN: testLoss: 0.8889 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 272 | ANN: testLoss: 0.7988 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 272 | ANN: testLoss: 0.5992 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 273 | ANN: trainLoss: 0.1527 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 273 | ANN: trainLoss: 0.1966 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 273 | ANN: trainLoss: 0.1626 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 273 | ANN: trainLoss: 0.1644 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 273 | ANN: trainLoss: 0.1496 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 273 | ANN: trainLoss: 0.1510 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 273 | ANN: trainLoss: 0.1551 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 273 | ANN: trainLoss: 0.1524 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 273 | ANN: trainLoss: 0.1488 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 273 | ANN: trainLoss: 0.1461 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 273 | ANN: trainLoss: 0.1435 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 273 | ANN: trainLoss: 0.1444 | trainAcc: 95.0521% (730/768)\n",
            "12 13 Epoch: 273 | ANN: trainLoss: 0.1679 | trainAcc: 94.9416% (732/771)\n",
            "0 4 Epoch: 273 | ANN: testLoss: 1.0490 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 273 | ANN: testLoss: 0.9006 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 273 | ANN: testLoss: 0.8255 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 273 | ANN: testLoss: 0.6410 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 274 | ANN: trainLoss: 0.0798 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 274 | ANN: trainLoss: 0.1234 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 274 | ANN: trainLoss: 0.1200 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 274 | ANN: trainLoss: 0.1435 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 274 | ANN: trainLoss: 0.1549 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 274 | ANN: trainLoss: 0.1696 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 274 | ANN: trainLoss: 0.1607 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 274 | ANN: trainLoss: 0.1523 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 274 | ANN: trainLoss: 0.1562 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 274 | ANN: trainLoss: 0.1616 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 274 | ANN: trainLoss: 0.1571 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 274 | ANN: trainLoss: 0.1510 | trainAcc: 93.3594% (717/768)\n",
            "12 13 Epoch: 274 | ANN: trainLoss: 0.1587 | trainAcc: 93.3852% (720/771)\n",
            "0 4 Epoch: 274 | ANN: testLoss: 0.7418 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 274 | ANN: testLoss: 0.9659 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 274 | ANN: testLoss: 0.8609 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 274 | ANN: testLoss: 1.9883 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 275 | ANN: trainLoss: 0.1067 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 275 | ANN: trainLoss: 0.1206 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 275 | ANN: trainLoss: 0.1090 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 275 | ANN: trainLoss: 0.1094 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 275 | ANN: trainLoss: 0.1033 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 275 | ANN: trainLoss: 0.1014 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 275 | ANN: trainLoss: 0.1218 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 275 | ANN: trainLoss: 0.1210 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 275 | ANN: trainLoss: 0.1236 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 275 | ANN: trainLoss: 0.1332 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 275 | ANN: trainLoss: 0.1323 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 275 | ANN: trainLoss: 0.1409 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 275 | ANN: trainLoss: 0.1316 | trainAcc: 94.2931% (727/771)\n",
            "0 4 Epoch: 275 | ANN: testLoss: 0.7656 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 275 | ANN: testLoss: 0.6790 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 275 | ANN: testLoss: 0.8294 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 275 | ANN: testLoss: 0.7326 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 276 | ANN: trainLoss: 0.1168 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 276 | ANN: trainLoss: 0.1406 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 276 | ANN: trainLoss: 0.1312 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 276 | ANN: trainLoss: 0.1237 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 276 | ANN: trainLoss: 0.1282 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 276 | ANN: trainLoss: 0.1233 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 276 | ANN: trainLoss: 0.1395 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 276 | ANN: trainLoss: 0.1371 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 276 | ANN: trainLoss: 0.1310 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 276 | ANN: trainLoss: 0.1282 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 276 | ANN: trainLoss: 0.1272 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 276 | ANN: trainLoss: 0.1319 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 276 | ANN: trainLoss: 0.2415 | trainAcc: 93.9040% (724/771)\n",
            "0 4 Epoch: 276 | ANN: testLoss: 0.9588 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 276 | ANN: testLoss: 0.9743 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 276 | ANN: testLoss: 0.8568 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 276 | ANN: testLoss: 0.6426 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 277 | ANN: trainLoss: 0.2073 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 277 | ANN: trainLoss: 0.1776 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 277 | ANN: trainLoss: 0.1799 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 277 | ANN: trainLoss: 0.1806 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 277 | ANN: trainLoss: 0.1886 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 277 | ANN: trainLoss: 0.1744 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 277 | ANN: trainLoss: 0.1701 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 277 | ANN: trainLoss: 0.1705 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 277 | ANN: trainLoss: 0.1789 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 277 | ANN: trainLoss: 0.1864 | trainAcc: 91.4062% (585/640)\n",
            "10 13 Epoch: 277 | ANN: trainLoss: 0.1797 | trainAcc: 91.7614% (646/704)\n",
            "11 13 Epoch: 277 | ANN: trainLoss: 0.1776 | trainAcc: 91.9271% (706/768)\n",
            "12 13 Epoch: 277 | ANN: trainLoss: 0.1724 | trainAcc: 91.9585% (709/771)\n",
            "0 4 Epoch: 277 | ANN: testLoss: 1.0410 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 277 | ANN: testLoss: 1.1005 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 277 | ANN: testLoss: 0.9776 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 277 | ANN: testLoss: 0.7332 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 278 | ANN: trainLoss: 0.0920 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 278 | ANN: trainLoss: 0.1071 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 278 | ANN: trainLoss: 0.1124 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 278 | ANN: trainLoss: 0.1314 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 278 | ANN: trainLoss: 0.1491 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 278 | ANN: trainLoss: 0.1603 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 278 | ANN: trainLoss: 0.1638 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 278 | ANN: trainLoss: 0.1673 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 278 | ANN: trainLoss: 0.1614 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 278 | ANN: trainLoss: 0.1536 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 278 | ANN: trainLoss: 0.1521 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 278 | ANN: trainLoss: 0.1507 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 278 | ANN: trainLoss: 0.2660 | trainAcc: 92.9961% (717/771)\n",
            "0 4 Epoch: 278 | ANN: testLoss: 1.1383 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 278 | ANN: testLoss: 0.8912 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 278 | ANN: testLoss: 0.9103 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 278 | ANN: testLoss: 0.6830 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 279 | ANN: trainLoss: 0.1326 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 279 | ANN: trainLoss: 0.1317 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 279 | ANN: trainLoss: 0.1496 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 279 | ANN: trainLoss: 0.1531 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 279 | ANN: trainLoss: 0.1424 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 279 | ANN: trainLoss: 0.1333 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 279 | ANN: trainLoss: 0.1364 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 279 | ANN: trainLoss: 0.1387 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 279 | ANN: trainLoss: 0.1455 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 279 | ANN: trainLoss: 0.1549 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 279 | ANN: trainLoss: 0.1569 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 279 | ANN: trainLoss: 0.1565 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 279 | ANN: trainLoss: 0.2650 | trainAcc: 94.2931% (727/771)\n",
            "0 4 Epoch: 279 | ANN: testLoss: 1.3489 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 279 | ANN: testLoss: 0.9422 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 279 | ANN: testLoss: 0.9112 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 279 | ANN: testLoss: 0.7149 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 280 | ANN: trainLoss: 0.1810 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 280 | ANN: trainLoss: 0.1735 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 280 | ANN: trainLoss: 0.1694 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 280 | ANN: trainLoss: 0.1422 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 280 | ANN: trainLoss: 0.1454 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 280 | ANN: trainLoss: 0.1663 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 280 | ANN: trainLoss: 0.1798 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 280 | ANN: trainLoss: 0.1669 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 280 | ANN: trainLoss: 0.1728 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 280 | ANN: trainLoss: 0.1697 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 280 | ANN: trainLoss: 0.1733 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 280 | ANN: trainLoss: 0.1834 | trainAcc: 92.3177% (709/768)\n",
            "12 13 Epoch: 280 | ANN: trainLoss: 0.2332 | trainAcc: 92.2179% (711/771)\n",
            "0 4 Epoch: 280 | ANN: testLoss: 0.5712 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 280 | ANN: testLoss: 1.0235 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 280 | ANN: testLoss: 1.0587 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 280 | ANN: testLoss: 0.7944 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 281 | ANN: trainLoss: 0.1023 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 281 | ANN: trainLoss: 0.1376 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 281 | ANN: trainLoss: 0.1301 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 281 | ANN: trainLoss: 0.1371 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 281 | ANN: trainLoss: 0.1352 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 281 | ANN: trainLoss: 0.1491 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 281 | ANN: trainLoss: 0.1774 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 281 | ANN: trainLoss: 0.1750 | trainAcc: 91.9922% (471/512)\n",
            "8 13 Epoch: 281 | ANN: trainLoss: 0.1795 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 281 | ANN: trainLoss: 0.1825 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 281 | ANN: trainLoss: 0.1897 | trainAcc: 91.4773% (644/704)\n",
            "11 13 Epoch: 281 | ANN: trainLoss: 0.1879 | trainAcc: 91.4062% (702/768)\n",
            "12 13 Epoch: 281 | ANN: trainLoss: 0.2259 | trainAcc: 91.3100% (704/771)\n",
            "0 4 Epoch: 281 | ANN: testLoss: 1.1872 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 281 | ANN: testLoss: 1.1797 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 281 | ANN: testLoss: 1.0674 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 281 | ANN: testLoss: 0.8088 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 282 | ANN: trainLoss: 0.1326 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 282 | ANN: trainLoss: 0.1713 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 282 | ANN: trainLoss: 0.1691 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 282 | ANN: trainLoss: 0.1564 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 282 | ANN: trainLoss: 0.1497 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 282 | ANN: trainLoss: 0.1525 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 282 | ANN: trainLoss: 0.1453 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 282 | ANN: trainLoss: 0.1486 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 282 | ANN: trainLoss: 0.1555 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 282 | ANN: trainLoss: 0.1516 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 282 | ANN: trainLoss: 0.1533 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 282 | ANN: trainLoss: 0.1581 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 282 | ANN: trainLoss: 0.1960 | trainAcc: 93.6446% (722/771)\n",
            "0 4 Epoch: 282 | ANN: testLoss: 1.0800 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 282 | ANN: testLoss: 0.9937 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 282 | ANN: testLoss: 0.9462 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 282 | ANN: testLoss: 0.8771 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 283 | ANN: trainLoss: 0.2320 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 283 | ANN: trainLoss: 0.1657 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 283 | ANN: trainLoss: 0.1429 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 283 | ANN: trainLoss: 0.1357 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 283 | ANN: trainLoss: 0.1331 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 283 | ANN: trainLoss: 0.1516 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 283 | ANN: trainLoss: 0.1527 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 283 | ANN: trainLoss: 0.1652 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 283 | ANN: trainLoss: 0.1674 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 283 | ANN: trainLoss: 0.1781 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 283 | ANN: trainLoss: 0.1823 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 283 | ANN: trainLoss: 0.1825 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 283 | ANN: trainLoss: 0.2131 | trainAcc: 93.1258% (718/771)\n",
            "0 4 Epoch: 283 | ANN: testLoss: 0.9849 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 283 | ANN: testLoss: 1.0992 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 283 | ANN: testLoss: 0.9756 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 283 | ANN: testLoss: 0.8458 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 284 | ANN: trainLoss: 0.0970 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 284 | ANN: trainLoss: 0.1400 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 284 | ANN: trainLoss: 0.1446 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 284 | ANN: trainLoss: 0.1466 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 284 | ANN: trainLoss: 0.1516 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 284 | ANN: trainLoss: 0.1626 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 284 | ANN: trainLoss: 0.1576 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 284 | ANN: trainLoss: 0.1563 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 284 | ANN: trainLoss: 0.1521 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 284 | ANN: trainLoss: 0.1464 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 284 | ANN: trainLoss: 0.1441 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 284 | ANN: trainLoss: 0.1475 | trainAcc: 93.3594% (717/768)\n",
            "12 13 Epoch: 284 | ANN: trainLoss: 0.1475 | trainAcc: 93.3852% (720/771)\n",
            "0 4 Epoch: 284 | ANN: testLoss: 0.7572 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 284 | ANN: testLoss: 0.8684 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 284 | ANN: testLoss: 0.9350 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 284 | ANN: testLoss: 1.1560 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 285 | ANN: trainLoss: 0.1236 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 285 | ANN: trainLoss: 0.1454 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 285 | ANN: trainLoss: 0.1557 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 285 | ANN: trainLoss: 0.1400 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 285 | ANN: trainLoss: 0.1372 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 285 | ANN: trainLoss: 0.1284 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 285 | ANN: trainLoss: 0.1382 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 285 | ANN: trainLoss: 0.1485 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 285 | ANN: trainLoss: 0.1438 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 285 | ANN: trainLoss: 0.1448 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 285 | ANN: trainLoss: 0.1487 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 285 | ANN: trainLoss: 0.1525 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 285 | ANN: trainLoss: 0.1455 | trainAcc: 93.5149% (721/771)\n",
            "0 4 Epoch: 285 | ANN: testLoss: 0.8057 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 285 | ANN: testLoss: 0.8980 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 285 | ANN: testLoss: 0.8642 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 285 | ANN: testLoss: 0.6674 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 286 | ANN: trainLoss: 0.1084 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 286 | ANN: trainLoss: 0.1469 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 286 | ANN: trainLoss: 0.1506 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 286 | ANN: trainLoss: 0.1410 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 286 | ANN: trainLoss: 0.1636 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 286 | ANN: trainLoss: 0.1722 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 286 | ANN: trainLoss: 0.1611 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 286 | ANN: trainLoss: 0.1583 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 286 | ANN: trainLoss: 0.1503 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 286 | ANN: trainLoss: 0.1504 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 286 | ANN: trainLoss: 0.1434 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 286 | ANN: trainLoss: 0.1428 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 286 | ANN: trainLoss: 0.1331 | trainAcc: 94.5525% (729/771)\n",
            "0 4 Epoch: 286 | ANN: testLoss: 0.8046 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 286 | ANN: testLoss: 0.9418 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 286 | ANN: testLoss: 0.9214 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 286 | ANN: testLoss: 0.8315 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 287 | ANN: trainLoss: 0.1589 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 287 | ANN: trainLoss: 0.1086 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 287 | ANN: trainLoss: 0.1341 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 287 | ANN: trainLoss: 0.1664 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 287 | ANN: trainLoss: 0.1605 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 287 | ANN: trainLoss: 0.1493 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 287 | ANN: trainLoss: 0.1517 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 287 | ANN: trainLoss: 0.1471 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 287 | ANN: trainLoss: 0.1460 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 287 | ANN: trainLoss: 0.1490 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 287 | ANN: trainLoss: 0.1456 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 287 | ANN: trainLoss: 0.1434 | trainAcc: 94.9219% (729/768)\n",
            "12 13 Epoch: 287 | ANN: trainLoss: 0.1994 | trainAcc: 94.8119% (731/771)\n",
            "0 4 Epoch: 287 | ANN: testLoss: 0.8812 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 287 | ANN: testLoss: 0.9375 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 287 | ANN: testLoss: 0.9707 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 287 | ANN: testLoss: 0.8330 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 288 | ANN: trainLoss: 0.1212 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 288 | ANN: trainLoss: 0.1106 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 288 | ANN: trainLoss: 0.1182 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 288 | ANN: trainLoss: 0.1216 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 288 | ANN: trainLoss: 0.1156 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 288 | ANN: trainLoss: 0.1204 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 288 | ANN: trainLoss: 0.1210 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 288 | ANN: trainLoss: 0.1363 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 288 | ANN: trainLoss: 0.1387 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 288 | ANN: trainLoss: 0.1404 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 288 | ANN: trainLoss: 0.1404 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 288 | ANN: trainLoss: 0.1426 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 288 | ANN: trainLoss: 0.1375 | trainAcc: 93.9040% (724/771)\n",
            "0 4 Epoch: 288 | ANN: testLoss: 1.0026 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 288 | ANN: testLoss: 0.8435 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 288 | ANN: testLoss: 0.9737 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 288 | ANN: testLoss: 2.2865 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 289 | ANN: trainLoss: 0.0667 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 289 | ANN: trainLoss: 0.1027 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 289 | ANN: trainLoss: 0.1041 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 289 | ANN: trainLoss: 0.1086 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 289 | ANN: trainLoss: 0.1114 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 289 | ANN: trainLoss: 0.1110 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 289 | ANN: trainLoss: 0.1133 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 289 | ANN: trainLoss: 0.1142 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 289 | ANN: trainLoss: 0.1151 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 289 | ANN: trainLoss: 0.1142 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 289 | ANN: trainLoss: 0.1164 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 289 | ANN: trainLoss: 0.1176 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 289 | ANN: trainLoss: 0.1383 | trainAcc: 95.3307% (735/771)\n",
            "0 4 Epoch: 289 | ANN: testLoss: 1.2247 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 289 | ANN: testLoss: 0.9298 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 289 | ANN: testLoss: 0.9597 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 289 | ANN: testLoss: 0.7207 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 290 | ANN: trainLoss: 0.0817 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 290 | ANN: trainLoss: 0.0775 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 290 | ANN: trainLoss: 0.1105 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 290 | ANN: trainLoss: 0.1102 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 290 | ANN: trainLoss: 0.1208 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 290 | ANN: trainLoss: 0.1368 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 290 | ANN: trainLoss: 0.1384 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 290 | ANN: trainLoss: 0.1374 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 290 | ANN: trainLoss: 0.1387 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 290 | ANN: trainLoss: 0.1366 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 290 | ANN: trainLoss: 0.1387 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 290 | ANN: trainLoss: 0.1482 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 290 | ANN: trainLoss: 0.1398 | trainAcc: 93.7743% (723/771)\n",
            "0 4 Epoch: 290 | ANN: testLoss: 0.9191 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 290 | ANN: testLoss: 0.8743 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 290 | ANN: testLoss: 0.9443 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 290 | ANN: testLoss: 0.8715 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 291 | ANN: trainLoss: 0.1151 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 291 | ANN: trainLoss: 0.1679 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 291 | ANN: trainLoss: 0.1413 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 291 | ANN: trainLoss: 0.1266 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 291 | ANN: trainLoss: 0.1394 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 291 | ANN: trainLoss: 0.1388 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 291 | ANN: trainLoss: 0.1339 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 291 | ANN: trainLoss: 0.1384 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 291 | ANN: trainLoss: 0.1421 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 291 | ANN: trainLoss: 0.1396 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 291 | ANN: trainLoss: 0.1421 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 291 | ANN: trainLoss: 0.1415 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 291 | ANN: trainLoss: 0.1522 | trainAcc: 94.2931% (727/771)\n",
            "0 4 Epoch: 291 | ANN: testLoss: 0.7511 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 291 | ANN: testLoss: 0.8011 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 291 | ANN: testLoss: 0.9209 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 291 | ANN: testLoss: 0.6908 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 292 | ANN: trainLoss: 0.0800 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 292 | ANN: trainLoss: 0.0757 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 292 | ANN: trainLoss: 0.1241 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 292 | ANN: trainLoss: 0.1365 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 292 | ANN: trainLoss: 0.1392 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 292 | ANN: trainLoss: 0.1436 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 292 | ANN: trainLoss: 0.1402 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 292 | ANN: trainLoss: 0.1365 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 292 | ANN: trainLoss: 0.1320 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 292 | ANN: trainLoss: 0.1280 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 292 | ANN: trainLoss: 0.1244 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 292 | ANN: trainLoss: 0.1225 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 292 | ANN: trainLoss: 0.1145 | trainAcc: 94.1634% (726/771)\n",
            "0 4 Epoch: 292 | ANN: testLoss: 0.8257 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 292 | ANN: testLoss: 0.9467 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 292 | ANN: testLoss: 0.9326 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 292 | ANN: testLoss: 0.7001 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 293 | ANN: trainLoss: 0.1542 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 293 | ANN: trainLoss: 0.1524 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 293 | ANN: trainLoss: 0.1322 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 293 | ANN: trainLoss: 0.1479 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 293 | ANN: trainLoss: 0.1639 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 293 | ANN: trainLoss: 0.1530 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 293 | ANN: trainLoss: 0.1476 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 293 | ANN: trainLoss: 0.1409 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 293 | ANN: trainLoss: 0.1390 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 293 | ANN: trainLoss: 0.1305 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 293 | ANN: trainLoss: 0.1327 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 293 | ANN: trainLoss: 0.1313 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 293 | ANN: trainLoss: 0.2923 | trainAcc: 92.8664% (716/771)\n",
            "0 4 Epoch: 293 | ANN: testLoss: 0.8424 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 293 | ANN: testLoss: 1.0449 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 293 | ANN: testLoss: 0.9335 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 293 | ANN: testLoss: 0.7002 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 294 | ANN: trainLoss: 0.1152 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 294 | ANN: trainLoss: 0.1188 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 294 | ANN: trainLoss: 0.1455 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 294 | ANN: trainLoss: 0.1455 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 294 | ANN: trainLoss: 0.1785 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 294 | ANN: trainLoss: 0.1651 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 294 | ANN: trainLoss: 0.1616 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 294 | ANN: trainLoss: 0.1618 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 294 | ANN: trainLoss: 0.1700 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 294 | ANN: trainLoss: 0.1691 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 294 | ANN: trainLoss: 0.1706 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 294 | ANN: trainLoss: 0.1826 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 294 | ANN: trainLoss: 0.1724 | trainAcc: 93.6446% (722/771)\n",
            "0 4 Epoch: 294 | ANN: testLoss: 1.3006 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 294 | ANN: testLoss: 1.0642 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 294 | ANN: testLoss: 1.0614 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 294 | ANN: testLoss: 1.2194 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 295 | ANN: trainLoss: 0.1834 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 295 | ANN: trainLoss: 0.1866 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 295 | ANN: trainLoss: 0.2178 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 295 | ANN: trainLoss: 0.2147 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 295 | ANN: trainLoss: 0.2051 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 295 | ANN: trainLoss: 0.2000 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 295 | ANN: trainLoss: 0.1872 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 295 | ANN: trainLoss: 0.1804 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 295 | ANN: trainLoss: 0.1825 | trainAcc: 92.0139% (530/576)\n",
            "9 13 Epoch: 295 | ANN: trainLoss: 0.1873 | trainAcc: 91.8750% (588/640)\n",
            "10 13 Epoch: 295 | ANN: trainLoss: 0.1801 | trainAcc: 92.3295% (650/704)\n",
            "11 13 Epoch: 295 | ANN: trainLoss: 0.1758 | trainAcc: 92.3177% (709/768)\n",
            "12 13 Epoch: 295 | ANN: trainLoss: 0.1695 | trainAcc: 92.3476% (712/771)\n",
            "0 4 Epoch: 295 | ANN: testLoss: 0.9835 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 295 | ANN: testLoss: 0.9766 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 295 | ANN: testLoss: 0.8381 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 295 | ANN: testLoss: 0.8208 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 296 | ANN: trainLoss: 0.1374 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 296 | ANN: trainLoss: 0.1166 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 296 | ANN: trainLoss: 0.1320 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 296 | ANN: trainLoss: 0.1511 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 296 | ANN: trainLoss: 0.1624 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 296 | ANN: trainLoss: 0.1594 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 296 | ANN: trainLoss: 0.1555 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 296 | ANN: trainLoss: 0.1512 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 296 | ANN: trainLoss: 0.1484 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 296 | ANN: trainLoss: 0.1455 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 296 | ANN: trainLoss: 0.1486 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 296 | ANN: trainLoss: 0.1514 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 296 | ANN: trainLoss: 0.2006 | trainAcc: 94.0337% (725/771)\n",
            "0 4 Epoch: 296 | ANN: testLoss: 1.0960 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 296 | ANN: testLoss: 0.7991 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 296 | ANN: testLoss: 0.8507 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 296 | ANN: testLoss: 0.6381 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 297 | ANN: trainLoss: 0.1366 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 297 | ANN: trainLoss: 0.1486 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 297 | ANN: trainLoss: 0.1476 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 297 | ANN: trainLoss: 0.1618 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 297 | ANN: trainLoss: 0.1491 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 297 | ANN: trainLoss: 0.1571 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 297 | ANN: trainLoss: 0.1485 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 297 | ANN: trainLoss: 0.1522 | trainAcc: 92.5781% (474/512)\n",
            "8 13 Epoch: 297 | ANN: trainLoss: 0.1472 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 297 | ANN: trainLoss: 0.1568 | trainAcc: 92.3438% (591/640)\n",
            "10 13 Epoch: 297 | ANN: trainLoss: 0.1518 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 297 | ANN: trainLoss: 0.1460 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 297 | ANN: trainLoss: 0.2769 | trainAcc: 92.6070% (714/771)\n",
            "0 4 Epoch: 297 | ANN: testLoss: 0.8003 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 297 | ANN: testLoss: 0.7159 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 297 | ANN: testLoss: 0.8087 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 297 | ANN: testLoss: 2.2028 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 298 | ANN: trainLoss: 0.1971 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 298 | ANN: trainLoss: 0.1755 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 298 | ANN: trainLoss: 0.1510 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 298 | ANN: trainLoss: 0.1427 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 298 | ANN: trainLoss: 0.1285 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 298 | ANN: trainLoss: 0.1291 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 298 | ANN: trainLoss: 0.1190 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 298 | ANN: trainLoss: 0.1326 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 298 | ANN: trainLoss: 0.1411 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 298 | ANN: trainLoss: 0.1331 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 298 | ANN: trainLoss: 0.1350 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 298 | ANN: trainLoss: 0.1346 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 298 | ANN: trainLoss: 0.1656 | trainAcc: 95.0713% (733/771)\n",
            "0 4 Epoch: 298 | ANN: testLoss: 0.9877 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 298 | ANN: testLoss: 0.8147 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 298 | ANN: testLoss: 0.9292 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 298 | ANN: testLoss: 0.6970 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 299 | ANN: trainLoss: 0.0743 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 299 | ANN: trainLoss: 0.0882 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 299 | ANN: trainLoss: 0.1337 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 299 | ANN: trainLoss: 0.1457 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 299 | ANN: trainLoss: 0.1378 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 299 | ANN: trainLoss: 0.1283 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 299 | ANN: trainLoss: 0.1422 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 299 | ANN: trainLoss: 0.1523 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 299 | ANN: trainLoss: 0.1649 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 299 | ANN: trainLoss: 0.1593 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 299 | ANN: trainLoss: 0.1554 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 299 | ANN: trainLoss: 0.1524 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 299 | ANN: trainLoss: 0.2035 | trainAcc: 93.9040% (724/771)\n",
            "0 4 Epoch: 299 | ANN: testLoss: 0.9730 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 299 | ANN: testLoss: 0.9765 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 299 | ANN: testLoss: 0.8804 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 299 | ANN: testLoss: 0.6609 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 300 | ANN: trainLoss: 0.1686 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 300 | ANN: trainLoss: 0.1795 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 300 | ANN: trainLoss: 0.1746 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 300 | ANN: trainLoss: 0.1621 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 300 | ANN: trainLoss: 0.1584 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 300 | ANN: trainLoss: 0.1690 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 300 | ANN: trainLoss: 0.1584 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 300 | ANN: trainLoss: 0.1544 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 300 | ANN: trainLoss: 0.1610 | trainAcc: 92.0139% (530/576)\n",
            "9 13 Epoch: 300 | ANN: trainLoss: 0.1852 | trainAcc: 91.2500% (584/640)\n",
            "10 13 Epoch: 300 | ANN: trainLoss: 0.1794 | trainAcc: 91.6193% (645/704)\n",
            "11 13 Epoch: 300 | ANN: trainLoss: 0.1792 | trainAcc: 91.6667% (704/768)\n",
            "12 13 Epoch: 300 | ANN: trainLoss: 0.1749 | trainAcc: 91.6991% (707/771)\n",
            "0 4 Epoch: 300 | ANN: testLoss: 0.9874 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 300 | ANN: testLoss: 0.9043 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 300 | ANN: testLoss: 0.9650 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 300 | ANN: testLoss: 0.7240 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 301 | ANN: trainLoss: 0.1241 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 301 | ANN: trainLoss: 0.1904 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 301 | ANN: trainLoss: 0.1985 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 301 | ANN: trainLoss: 0.1810 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 301 | ANN: trainLoss: 0.1820 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 301 | ANN: trainLoss: 0.1705 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 301 | ANN: trainLoss: 0.1577 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 301 | ANN: trainLoss: 0.1585 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 301 | ANN: trainLoss: 0.1522 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 301 | ANN: trainLoss: 0.1594 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 301 | ANN: trainLoss: 0.1653 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 301 | ANN: trainLoss: 0.1655 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 301 | ANN: trainLoss: 0.1579 | trainAcc: 92.8664% (716/771)\n",
            "0 4 Epoch: 301 | ANN: testLoss: 0.9445 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 301 | ANN: testLoss: 1.0215 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 301 | ANN: testLoss: 0.9403 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 301 | ANN: testLoss: 0.7055 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 302 | ANN: trainLoss: 0.2241 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 302 | ANN: trainLoss: 0.2239 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 302 | ANN: trainLoss: 0.1763 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 302 | ANN: trainLoss: 0.1603 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 302 | ANN: trainLoss: 0.1592 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 302 | ANN: trainLoss: 0.1688 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 302 | ANN: trainLoss: 0.1633 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 302 | ANN: trainLoss: 0.1574 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 302 | ANN: trainLoss: 0.1518 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 302 | ANN: trainLoss: 0.1493 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 302 | ANN: trainLoss: 0.1514 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 302 | ANN: trainLoss: 0.1490 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 302 | ANN: trainLoss: 0.1705 | trainAcc: 94.0337% (725/771)\n",
            "0 4 Epoch: 302 | ANN: testLoss: 0.7900 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 302 | ANN: testLoss: 0.8397 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 302 | ANN: testLoss: 0.8477 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 302 | ANN: testLoss: 0.6358 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 303 | ANN: trainLoss: 0.1494 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 303 | ANN: trainLoss: 0.1693 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 303 | ANN: trainLoss: 0.1574 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 303 | ANN: trainLoss: 0.1501 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 303 | ANN: trainLoss: 0.1493 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 303 | ANN: trainLoss: 0.1518 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 303 | ANN: trainLoss: 0.1574 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 303 | ANN: trainLoss: 0.1596 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 303 | ANN: trainLoss: 0.1625 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 303 | ANN: trainLoss: 0.1537 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 303 | ANN: trainLoss: 0.1472 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 303 | ANN: trainLoss: 0.1471 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 303 | ANN: trainLoss: 0.1436 | trainAcc: 93.6446% (722/771)\n",
            "0 4 Epoch: 303 | ANN: testLoss: 1.0003 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 303 | ANN: testLoss: 1.1557 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 303 | ANN: testLoss: 0.9304 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 303 | ANN: testLoss: 2.2080 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 304 | ANN: trainLoss: 0.1423 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 304 | ANN: trainLoss: 0.1618 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 304 | ANN: trainLoss: 0.1576 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 304 | ANN: trainLoss: 0.1485 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 304 | ANN: trainLoss: 0.1351 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 304 | ANN: trainLoss: 0.1274 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 304 | ANN: trainLoss: 0.1250 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 304 | ANN: trainLoss: 0.1311 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 304 | ANN: trainLoss: 0.1454 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 304 | ANN: trainLoss: 0.1467 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 304 | ANN: trainLoss: 0.1422 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 304 | ANN: trainLoss: 0.1389 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 304 | ANN: trainLoss: 0.1651 | trainAcc: 93.6446% (722/771)\n",
            "0 4 Epoch: 304 | ANN: testLoss: 0.7594 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 304 | ANN: testLoss: 0.8930 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 304 | ANN: testLoss: 0.9376 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 304 | ANN: testLoss: 0.7032 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 305 | ANN: trainLoss: 0.1248 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 305 | ANN: trainLoss: 0.1581 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 305 | ANN: trainLoss: 0.1267 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 305 | ANN: trainLoss: 0.1081 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 305 | ANN: trainLoss: 0.1096 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 305 | ANN: trainLoss: 0.1176 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 305 | ANN: trainLoss: 0.1316 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 305 | ANN: trainLoss: 0.1291 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 305 | ANN: trainLoss: 0.1294 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 305 | ANN: trainLoss: 0.1328 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 305 | ANN: trainLoss: 0.1308 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 305 | ANN: trainLoss: 0.1275 | trainAcc: 95.0521% (730/768)\n",
            "12 13 Epoch: 305 | ANN: trainLoss: 0.1202 | trainAcc: 95.0713% (733/771)\n",
            "0 4 Epoch: 305 | ANN: testLoss: 1.2041 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 305 | ANN: testLoss: 1.0518 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 305 | ANN: testLoss: 0.9786 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 305 | ANN: testLoss: 0.7340 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 306 | ANN: trainLoss: 0.1215 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 306 | ANN: trainLoss: 0.1090 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 306 | ANN: trainLoss: 0.1084 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 306 | ANN: trainLoss: 0.1282 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 306 | ANN: trainLoss: 0.1285 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 306 | ANN: trainLoss: 0.1247 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 306 | ANN: trainLoss: 0.1206 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 306 | ANN: trainLoss: 0.1150 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 306 | ANN: trainLoss: 0.1099 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 306 | ANN: trainLoss: 0.1135 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 306 | ANN: trainLoss: 0.1137 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 306 | ANN: trainLoss: 0.1168 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 306 | ANN: trainLoss: 0.1372 | trainAcc: 95.7198% (738/771)\n",
            "0 4 Epoch: 306 | ANN: testLoss: 1.0598 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 306 | ANN: testLoss: 1.0452 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 306 | ANN: testLoss: 0.9519 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 306 | ANN: testLoss: 0.7377 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 307 | ANN: trainLoss: 0.0909 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 307 | ANN: trainLoss: 0.1274 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 307 | ANN: trainLoss: 0.1187 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 307 | ANN: trainLoss: 0.1315 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 307 | ANN: trainLoss: 0.1389 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 307 | ANN: trainLoss: 0.1541 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 307 | ANN: trainLoss: 0.1485 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 307 | ANN: trainLoss: 0.1461 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 307 | ANN: trainLoss: 0.1406 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 307 | ANN: trainLoss: 0.1411 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 307 | ANN: trainLoss: 0.1388 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 307 | ANN: trainLoss: 0.1370 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 307 | ANN: trainLoss: 0.1769 | trainAcc: 93.6446% (722/771)\n",
            "0 4 Epoch: 307 | ANN: testLoss: 1.4535 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 307 | ANN: testLoss: 1.0697 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 307 | ANN: testLoss: 0.9644 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 307 | ANN: testLoss: 0.7234 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 308 | ANN: trainLoss: 0.1255 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 308 | ANN: trainLoss: 0.1250 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 308 | ANN: trainLoss: 0.1010 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 308 | ANN: trainLoss: 0.1004 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 308 | ANN: trainLoss: 0.1058 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 308 | ANN: trainLoss: 0.1210 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 308 | ANN: trainLoss: 0.1154 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 308 | ANN: trainLoss: 0.1185 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 308 | ANN: trainLoss: 0.1206 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 308 | ANN: trainLoss: 0.1185 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 308 | ANN: trainLoss: 0.1202 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 308 | ANN: trainLoss: 0.1216 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 308 | ANN: trainLoss: 0.2474 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 308 | ANN: testLoss: 1.4401 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 308 | ANN: testLoss: 0.9971 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 308 | ANN: testLoss: 0.9428 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 308 | ANN: testLoss: 0.8217 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 309 | ANN: trainLoss: 0.0608 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 309 | ANN: trainLoss: 0.0882 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 309 | ANN: trainLoss: 0.1146 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 309 | ANN: trainLoss: 0.1314 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 309 | ANN: trainLoss: 0.1409 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 309 | ANN: trainLoss: 0.1438 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 309 | ANN: trainLoss: 0.1396 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 309 | ANN: trainLoss: 0.1555 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 309 | ANN: trainLoss: 0.1610 | trainAcc: 92.8819% (535/576)\n",
            "9 13 Epoch: 309 | ANN: trainLoss: 0.1638 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 309 | ANN: trainLoss: 0.1602 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 309 | ANN: trainLoss: 0.1700 | trainAcc: 92.3177% (709/768)\n",
            "12 13 Epoch: 309 | ANN: trainLoss: 0.1583 | trainAcc: 92.3476% (712/771)\n",
            "0 4 Epoch: 309 | ANN: testLoss: 0.9781 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 309 | ANN: testLoss: 0.9336 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 309 | ANN: testLoss: 0.9913 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 309 | ANN: testLoss: 0.8718 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 310 | ANN: trainLoss: 0.1185 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 310 | ANN: trainLoss: 0.1197 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 310 | ANN: trainLoss: 0.1261 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 310 | ANN: trainLoss: 0.1241 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 310 | ANN: trainLoss: 0.1327 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 310 | ANN: trainLoss: 0.1411 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 310 | ANN: trainLoss: 0.1301 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 310 | ANN: trainLoss: 0.1445 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 310 | ANN: trainLoss: 0.1590 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 310 | ANN: trainLoss: 0.1598 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 310 | ANN: trainLoss: 0.1616 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 310 | ANN: trainLoss: 0.1616 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 310 | ANN: trainLoss: 0.2172 | trainAcc: 92.9961% (717/771)\n",
            "0 4 Epoch: 310 | ANN: testLoss: 0.6892 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 310 | ANN: testLoss: 0.7793 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 310 | ANN: testLoss: 0.8627 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 310 | ANN: testLoss: 1.8491 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 311 | ANN: trainLoss: 0.1011 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 311 | ANN: trainLoss: 0.0982 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 311 | ANN: trainLoss: 0.1257 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 311 | ANN: trainLoss: 0.1427 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 311 | ANN: trainLoss: 0.1598 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 311 | ANN: trainLoss: 0.1465 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 311 | ANN: trainLoss: 0.1471 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 311 | ANN: trainLoss: 0.1518 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 311 | ANN: trainLoss: 0.1465 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 311 | ANN: trainLoss: 0.1469 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 311 | ANN: trainLoss: 0.1419 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 311 | ANN: trainLoss: 0.1465 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 311 | ANN: trainLoss: 0.1643 | trainAcc: 93.6446% (722/771)\n",
            "0 4 Epoch: 311 | ANN: testLoss: 1.0121 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 311 | ANN: testLoss: 0.9179 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 311 | ANN: testLoss: 0.8392 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 311 | ANN: testLoss: 0.6295 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 312 | ANN: trainLoss: 0.2233 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 312 | ANN: trainLoss: 0.1502 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 312 | ANN: trainLoss: 0.1341 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 312 | ANN: trainLoss: 0.1409 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 312 | ANN: trainLoss: 0.1374 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 312 | ANN: trainLoss: 0.1380 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 312 | ANN: trainLoss: 0.1397 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 312 | ANN: trainLoss: 0.1302 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 312 | ANN: trainLoss: 0.1227 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 312 | ANN: trainLoss: 0.1201 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 312 | ANN: trainLoss: 0.1262 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 312 | ANN: trainLoss: 0.1206 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 312 | ANN: trainLoss: 0.2672 | trainAcc: 94.4228% (728/771)\n",
            "0 4 Epoch: 312 | ANN: testLoss: 0.7319 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 312 | ANN: testLoss: 0.7703 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 312 | ANN: testLoss: 0.7878 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 312 | ANN: testLoss: 0.5991 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 313 | ANN: trainLoss: 0.1473 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 313 | ANN: trainLoss: 0.1709 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 313 | ANN: trainLoss: 0.1651 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 313 | ANN: trainLoss: 0.1643 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 313 | ANN: trainLoss: 0.1614 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 313 | ANN: trainLoss: 0.1630 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 313 | ANN: trainLoss: 0.1523 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 313 | ANN: trainLoss: 0.1521 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 313 | ANN: trainLoss: 0.1462 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 313 | ANN: trainLoss: 0.1406 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 313 | ANN: trainLoss: 0.1407 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 313 | ANN: trainLoss: 0.1361 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 313 | ANN: trainLoss: 0.1260 | trainAcc: 94.5525% (729/771)\n",
            "0 4 Epoch: 313 | ANN: testLoss: 0.8270 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 313 | ANN: testLoss: 0.6730 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 313 | ANN: testLoss: 0.7885 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 313 | ANN: testLoss: 1.8126 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 314 | ANN: trainLoss: 0.1095 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 314 | ANN: trainLoss: 0.1448 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 314 | ANN: trainLoss: 0.1224 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 314 | ANN: trainLoss: 0.1174 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 314 | ANN: trainLoss: 0.1266 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 314 | ANN: trainLoss: 0.1242 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 314 | ANN: trainLoss: 0.1189 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 314 | ANN: trainLoss: 0.1152 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 314 | ANN: trainLoss: 0.1123 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 314 | ANN: trainLoss: 0.1105 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 314 | ANN: trainLoss: 0.1153 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 314 | ANN: trainLoss: 0.1146 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 314 | ANN: trainLoss: 0.2384 | trainAcc: 96.1089% (741/771)\n",
            "0 4 Epoch: 314 | ANN: testLoss: 0.7723 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 314 | ANN: testLoss: 0.7869 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 314 | ANN: testLoss: 0.8839 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 314 | ANN: testLoss: 0.8566 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 315 | ANN: trainLoss: 0.0592 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 315 | ANN: trainLoss: 0.1014 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 315 | ANN: trainLoss: 0.1013 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 315 | ANN: trainLoss: 0.1246 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 315 | ANN: trainLoss: 0.1217 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 315 | ANN: trainLoss: 0.1168 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 315 | ANN: trainLoss: 0.1253 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 315 | ANN: trainLoss: 0.1367 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 315 | ANN: trainLoss: 0.1370 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 315 | ANN: trainLoss: 0.1402 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 315 | ANN: trainLoss: 0.1422 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 315 | ANN: trainLoss: 0.1416 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 315 | ANN: trainLoss: 0.2272 | trainAcc: 93.7743% (723/771)\n",
            "0 4 Epoch: 315 | ANN: testLoss: 0.9869 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 315 | ANN: testLoss: 1.0136 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 315 | ANN: testLoss: 0.8594 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 315 | ANN: testLoss: 0.8998 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 316 | ANN: trainLoss: 0.1208 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 316 | ANN: trainLoss: 0.1330 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 316 | ANN: trainLoss: 0.1182 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 316 | ANN: trainLoss: 0.1235 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 316 | ANN: trainLoss: 0.1161 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 316 | ANN: trainLoss: 0.1114 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 316 | ANN: trainLoss: 0.1155 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 316 | ANN: trainLoss: 0.1141 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 316 | ANN: trainLoss: 0.1262 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 316 | ANN: trainLoss: 0.1291 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 316 | ANN: trainLoss: 0.1279 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 316 | ANN: trainLoss: 0.1340 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 316 | ANN: trainLoss: 0.2551 | trainAcc: 94.1634% (726/771)\n",
            "0 4 Epoch: 316 | ANN: testLoss: 1.1890 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 316 | ANN: testLoss: 1.0184 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 316 | ANN: testLoss: 0.8590 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 316 | ANN: testLoss: 0.6903 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 317 | ANN: trainLoss: 0.1070 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 317 | ANN: trainLoss: 0.0917 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 317 | ANN: trainLoss: 0.1165 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 317 | ANN: trainLoss: 0.1305 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 317 | ANN: trainLoss: 0.1293 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 317 | ANN: trainLoss: 0.1216 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 317 | ANN: trainLoss: 0.1206 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 317 | ANN: trainLoss: 0.1249 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 317 | ANN: trainLoss: 0.1251 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 317 | ANN: trainLoss: 0.1236 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 317 | ANN: trainLoss: 0.1203 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 317 | ANN: trainLoss: 0.1233 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 317 | ANN: trainLoss: 0.1192 | trainAcc: 95.4604% (736/771)\n",
            "0 4 Epoch: 317 | ANN: testLoss: 0.6947 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 317 | ANN: testLoss: 0.8075 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 317 | ANN: testLoss: 0.8376 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 317 | ANN: testLoss: 1.0215 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 318 | ANN: trainLoss: 0.1102 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 318 | ANN: trainLoss: 0.1196 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 318 | ANN: trainLoss: 0.1379 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 318 | ANN: trainLoss: 0.1439 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 318 | ANN: trainLoss: 0.1461 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 318 | ANN: trainLoss: 0.1438 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 318 | ANN: trainLoss: 0.1540 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 318 | ANN: trainLoss: 0.1647 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 318 | ANN: trainLoss: 0.1622 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 318 | ANN: trainLoss: 0.1673 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 318 | ANN: trainLoss: 0.1627 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 318 | ANN: trainLoss: 0.1660 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 318 | ANN: trainLoss: 0.2204 | trainAcc: 93.1258% (718/771)\n",
            "0 4 Epoch: 318 | ANN: testLoss: 0.8187 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 318 | ANN: testLoss: 0.9562 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 318 | ANN: testLoss: 0.9001 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 318 | ANN: testLoss: 0.6801 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 319 | ANN: trainLoss: 0.1399 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 319 | ANN: trainLoss: 0.1259 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 319 | ANN: trainLoss: 0.1462 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 319 | ANN: trainLoss: 0.1533 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 319 | ANN: trainLoss: 0.1501 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 319 | ANN: trainLoss: 0.1441 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 319 | ANN: trainLoss: 0.1504 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 319 | ANN: trainLoss: 0.1528 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 319 | ANN: trainLoss: 0.1486 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 319 | ANN: trainLoss: 0.1457 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 319 | ANN: trainLoss: 0.1489 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 319 | ANN: trainLoss: 0.1467 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 319 | ANN: trainLoss: 0.2165 | trainAcc: 93.7743% (723/771)\n",
            "0 4 Epoch: 319 | ANN: testLoss: 0.8012 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 319 | ANN: testLoss: 0.8058 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 319 | ANN: testLoss: 0.8953 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 319 | ANN: testLoss: 0.6716 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 320 | ANN: trainLoss: 0.1294 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 320 | ANN: trainLoss: 0.0979 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 320 | ANN: trainLoss: 0.0984 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 320 | ANN: trainLoss: 0.1034 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 320 | ANN: trainLoss: 0.1033 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 320 | ANN: trainLoss: 0.1212 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 320 | ANN: trainLoss: 0.1256 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 320 | ANN: trainLoss: 0.1271 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 320 | ANN: trainLoss: 0.1300 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 320 | ANN: trainLoss: 0.1247 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 320 | ANN: trainLoss: 0.1230 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 320 | ANN: trainLoss: 0.1256 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 320 | ANN: trainLoss: 0.2195 | trainAcc: 94.9416% (732/771)\n",
            "0 4 Epoch: 320 | ANN: testLoss: 0.7400 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 320 | ANN: testLoss: 0.7723 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 320 | ANN: testLoss: 0.8477 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 320 | ANN: testLoss: 1.5214 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 321 | ANN: trainLoss: 0.1093 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 321 | ANN: trainLoss: 0.1305 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 321 | ANN: trainLoss: 0.1564 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 321 | ANN: trainLoss: 0.1712 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 321 | ANN: trainLoss: 0.1517 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 321 | ANN: trainLoss: 0.1474 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 321 | ANN: trainLoss: 0.1411 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 321 | ANN: trainLoss: 0.1325 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 321 | ANN: trainLoss: 0.1365 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 321 | ANN: trainLoss: 0.1392 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 321 | ANN: trainLoss: 0.1415 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 321 | ANN: trainLoss: 0.1378 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 321 | ANN: trainLoss: 0.1403 | trainAcc: 94.1634% (726/771)\n",
            "0 4 Epoch: 321 | ANN: testLoss: 1.0094 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 321 | ANN: testLoss: 0.9010 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 321 | ANN: testLoss: 0.8593 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 321 | ANN: testLoss: 0.6446 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 322 | ANN: trainLoss: 0.0758 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 322 | ANN: trainLoss: 0.1052 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 322 | ANN: trainLoss: 0.1342 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 322 | ANN: trainLoss: 0.1427 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 322 | ANN: trainLoss: 0.1383 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 322 | ANN: trainLoss: 0.1318 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 322 | ANN: trainLoss: 0.1260 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 322 | ANN: trainLoss: 0.1266 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 322 | ANN: trainLoss: 0.1345 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 322 | ANN: trainLoss: 0.1320 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 322 | ANN: trainLoss: 0.1323 | trainAcc: 95.0284% (669/704)\n",
            "11 13 Epoch: 322 | ANN: trainLoss: 0.1272 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 322 | ANN: trainLoss: 0.1260 | trainAcc: 95.4604% (736/771)\n",
            "0 4 Epoch: 322 | ANN: testLoss: 1.2127 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 322 | ANN: testLoss: 0.9215 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 322 | ANN: testLoss: 0.8828 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 322 | ANN: testLoss: 0.6622 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 323 | ANN: trainLoss: 0.1595 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 323 | ANN: trainLoss: 0.1207 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 323 | ANN: trainLoss: 0.1237 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 323 | ANN: trainLoss: 0.1062 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 323 | ANN: trainLoss: 0.1073 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 323 | ANN: trainLoss: 0.1316 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 323 | ANN: trainLoss: 0.1284 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 323 | ANN: trainLoss: 0.1273 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 323 | ANN: trainLoss: 0.1292 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 323 | ANN: trainLoss: 0.1407 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 323 | ANN: trainLoss: 0.1337 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 323 | ANN: trainLoss: 0.1353 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 323 | ANN: trainLoss: 0.1294 | trainAcc: 94.5525% (729/771)\n",
            "0 4 Epoch: 323 | ANN: testLoss: 0.7739 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 323 | ANN: testLoss: 0.9036 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 323 | ANN: testLoss: 0.8680 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 323 | ANN: testLoss: 0.9648 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 324 | ANN: trainLoss: 0.1466 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 324 | ANN: trainLoss: 0.1485 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 324 | ANN: trainLoss: 0.1224 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 324 | ANN: trainLoss: 0.1338 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 324 | ANN: trainLoss: 0.1398 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 324 | ANN: trainLoss: 0.1320 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 324 | ANN: trainLoss: 0.1335 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 324 | ANN: trainLoss: 0.1287 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 324 | ANN: trainLoss: 0.1296 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 324 | ANN: trainLoss: 0.1302 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 324 | ANN: trainLoss: 0.1309 | trainAcc: 95.0284% (669/704)\n",
            "11 13 Epoch: 324 | ANN: trainLoss: 0.1309 | trainAcc: 94.9219% (729/768)\n",
            "12 13 Epoch: 324 | ANN: trainLoss: 0.1262 | trainAcc: 94.9416% (732/771)\n",
            "0 4 Epoch: 324 | ANN: testLoss: 1.0262 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 324 | ANN: testLoss: 0.7460 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 324 | ANN: testLoss: 0.7951 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 324 | ANN: testLoss: 2.2102 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 325 | ANN: trainLoss: 0.0835 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 325 | ANN: trainLoss: 0.0941 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 325 | ANN: trainLoss: 0.0913 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 325 | ANN: trainLoss: 0.1067 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 325 | ANN: trainLoss: 0.1142 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 325 | ANN: trainLoss: 0.1119 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 325 | ANN: trainLoss: 0.1115 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 325 | ANN: trainLoss: 0.1157 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 325 | ANN: trainLoss: 0.1153 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 325 | ANN: trainLoss: 0.1161 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 325 | ANN: trainLoss: 0.1176 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 325 | ANN: trainLoss: 0.1143 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 325 | ANN: trainLoss: 0.1311 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 325 | ANN: testLoss: 0.8264 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 325 | ANN: testLoss: 0.8336 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 325 | ANN: testLoss: 0.7958 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 325 | ANN: testLoss: 0.9112 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 326 | ANN: trainLoss: 0.0964 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 326 | ANN: trainLoss: 0.0916 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 326 | ANN: trainLoss: 0.0928 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 326 | ANN: trainLoss: 0.1003 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 326 | ANN: trainLoss: 0.0991 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 326 | ANN: trainLoss: 0.1013 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 326 | ANN: trainLoss: 0.1090 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 326 | ANN: trainLoss: 0.1098 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 326 | ANN: trainLoss: 0.1095 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 326 | ANN: trainLoss: 0.1139 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 326 | ANN: trainLoss: 0.1139 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 326 | ANN: trainLoss: 0.1148 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 326 | ANN: trainLoss: 0.1272 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 326 | ANN: testLoss: 1.1353 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 326 | ANN: testLoss: 0.9111 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 326 | ANN: testLoss: 0.8449 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 326 | ANN: testLoss: 0.6338 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 327 | ANN: trainLoss: 0.1302 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 327 | ANN: trainLoss: 0.1756 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 327 | ANN: trainLoss: 0.1605 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 327 | ANN: trainLoss: 0.1474 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 327 | ANN: trainLoss: 0.1471 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 327 | ANN: trainLoss: 0.1329 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 327 | ANN: trainLoss: 0.1301 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 327 | ANN: trainLoss: 0.1238 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 327 | ANN: trainLoss: 0.1238 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 327 | ANN: trainLoss: 0.1219 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 327 | ANN: trainLoss: 0.1256 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 327 | ANN: trainLoss: 0.1285 | trainAcc: 95.3125% (732/768)\n",
            "12 13 Epoch: 327 | ANN: trainLoss: 0.2963 | trainAcc: 95.2010% (734/771)\n",
            "0 4 Epoch: 327 | ANN: testLoss: 1.0472 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 327 | ANN: testLoss: 0.9377 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 327 | ANN: testLoss: 0.8618 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 327 | ANN: testLoss: 0.6464 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 328 | ANN: trainLoss: 0.0867 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 328 | ANN: trainLoss: 0.1237 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 328 | ANN: trainLoss: 0.1202 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 328 | ANN: trainLoss: 0.1040 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 328 | ANN: trainLoss: 0.0976 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 328 | ANN: trainLoss: 0.0972 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 328 | ANN: trainLoss: 0.0965 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 328 | ANN: trainLoss: 0.0954 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 328 | ANN: trainLoss: 0.1022 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 328 | ANN: trainLoss: 0.1031 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 328 | ANN: trainLoss: 0.1209 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 328 | ANN: trainLoss: 0.1256 | trainAcc: 94.9219% (729/768)\n",
            "12 13 Epoch: 328 | ANN: trainLoss: 0.1357 | trainAcc: 94.9416% (732/771)\n",
            "0 4 Epoch: 328 | ANN: testLoss: 0.7338 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 328 | ANN: testLoss: 0.9216 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 328 | ANN: testLoss: 0.8210 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 328 | ANN: testLoss: 0.8023 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 329 | ANN: trainLoss: 0.1119 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 329 | ANN: trainLoss: 0.1123 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 329 | ANN: trainLoss: 0.1061 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 329 | ANN: trainLoss: 0.0996 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 329 | ANN: trainLoss: 0.0957 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 329 | ANN: trainLoss: 0.1042 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 329 | ANN: trainLoss: 0.1158 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 329 | ANN: trainLoss: 0.1091 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 329 | ANN: trainLoss: 0.1167 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 329 | ANN: trainLoss: 0.1255 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 329 | ANN: trainLoss: 0.1286 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 329 | ANN: trainLoss: 0.1279 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 329 | ANN: trainLoss: 0.1931 | trainAcc: 94.4228% (728/771)\n",
            "0 4 Epoch: 329 | ANN: testLoss: 0.7206 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 329 | ANN: testLoss: 0.8606 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 329 | ANN: testLoss: 0.8553 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 329 | ANN: testLoss: 0.6521 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 330 | ANN: trainLoss: 0.0923 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 330 | ANN: trainLoss: 0.1062 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 330 | ANN: trainLoss: 0.1199 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 330 | ANN: trainLoss: 0.1219 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 330 | ANN: trainLoss: 0.1146 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 330 | ANN: trainLoss: 0.1170 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 330 | ANN: trainLoss: 0.1277 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 330 | ANN: trainLoss: 0.1335 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 330 | ANN: trainLoss: 0.1279 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 330 | ANN: trainLoss: 0.1223 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 330 | ANN: trainLoss: 0.1180 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 330 | ANN: trainLoss: 0.1187 | trainAcc: 95.5729% (734/768)\n",
            "12 13 Epoch: 330 | ANN: trainLoss: 0.1340 | trainAcc: 95.5901% (737/771)\n",
            "0 4 Epoch: 330 | ANN: testLoss: 1.0259 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 330 | ANN: testLoss: 0.8497 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 330 | ANN: testLoss: 0.8595 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 330 | ANN: testLoss: 1.1651 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 331 | ANN: trainLoss: 0.1357 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 331 | ANN: trainLoss: 0.1038 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 331 | ANN: trainLoss: 0.1231 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 331 | ANN: trainLoss: 0.1312 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 331 | ANN: trainLoss: 0.1253 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 331 | ANN: trainLoss: 0.1299 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 331 | ANN: trainLoss: 0.1225 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 331 | ANN: trainLoss: 0.1197 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 331 | ANN: trainLoss: 0.1194 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 331 | ANN: trainLoss: 0.1141 | trainAcc: 95.6250% (612/640)\n",
            "10 13 Epoch: 331 | ANN: trainLoss: 0.1153 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 331 | ANN: trainLoss: 0.1156 | trainAcc: 95.3125% (732/768)\n",
            "12 13 Epoch: 331 | ANN: trainLoss: 0.1148 | trainAcc: 95.3307% (735/771)\n",
            "0 4 Epoch: 331 | ANN: testLoss: 0.7591 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 331 | ANN: testLoss: 0.8472 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 331 | ANN: testLoss: 0.9087 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 331 | ANN: testLoss: 0.7099 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 332 | ANN: trainLoss: 0.0471 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 332 | ANN: trainLoss: 0.1161 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 332 | ANN: trainLoss: 0.1095 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 332 | ANN: trainLoss: 0.1189 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 332 | ANN: trainLoss: 0.1326 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 332 | ANN: trainLoss: 0.1211 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 332 | ANN: trainLoss: 0.1183 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 332 | ANN: trainLoss: 0.1116 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 332 | ANN: trainLoss: 0.1102 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 332 | ANN: trainLoss: 0.1081 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 332 | ANN: trainLoss: 0.1053 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 332 | ANN: trainLoss: 0.1037 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 332 | ANN: trainLoss: 0.1040 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 332 | ANN: testLoss: 0.9263 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 332 | ANN: testLoss: 0.9334 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 332 | ANN: testLoss: 0.9057 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 332 | ANN: testLoss: 0.8308 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 333 | ANN: trainLoss: 0.0622 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 333 | ANN: trainLoss: 0.1097 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 333 | ANN: trainLoss: 0.1105 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 333 | ANN: trainLoss: 0.1072 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 333 | ANN: trainLoss: 0.1051 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 333 | ANN: trainLoss: 0.1016 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 333 | ANN: trainLoss: 0.1001 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 333 | ANN: trainLoss: 0.1014 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 333 | ANN: trainLoss: 0.1044 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 333 | ANN: trainLoss: 0.1090 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 333 | ANN: trainLoss: 0.1047 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 333 | ANN: trainLoss: 0.1079 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 333 | ANN: trainLoss: 0.3132 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 333 | ANN: testLoss: 0.7524 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 333 | ANN: testLoss: 0.7976 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 333 | ANN: testLoss: 0.9087 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 333 | ANN: testLoss: 0.8406 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 334 | ANN: trainLoss: 0.1707 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 334 | ANN: trainLoss: 0.1267 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 334 | ANN: trainLoss: 0.1280 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 334 | ANN: trainLoss: 0.1206 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 334 | ANN: trainLoss: 0.1135 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 334 | ANN: trainLoss: 0.1083 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 334 | ANN: trainLoss: 0.1131 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 334 | ANN: trainLoss: 0.1096 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 334 | ANN: trainLoss: 0.1160 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 334 | ANN: trainLoss: 0.1193 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 334 | ANN: trainLoss: 0.1184 | trainAcc: 95.8807% (675/704)\n",
            "11 13 Epoch: 334 | ANN: trainLoss: 0.1161 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 334 | ANN: trainLoss: 0.1093 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 334 | ANN: testLoss: 0.8521 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 334 | ANN: testLoss: 0.9002 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 334 | ANN: testLoss: 0.9021 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 334 | ANN: testLoss: 0.6766 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 335 | ANN: trainLoss: 0.0689 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 335 | ANN: trainLoss: 0.0836 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 335 | ANN: trainLoss: 0.0820 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 335 | ANN: trainLoss: 0.0818 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 335 | ANN: trainLoss: 0.0881 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 335 | ANN: trainLoss: 0.0915 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 335 | ANN: trainLoss: 0.0927 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 335 | ANN: trainLoss: 0.0968 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 335 | ANN: trainLoss: 0.0974 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 335 | ANN: trainLoss: 0.0995 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 335 | ANN: trainLoss: 0.1023 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 335 | ANN: trainLoss: 0.1069 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 335 | ANN: trainLoss: 0.3046 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 335 | ANN: testLoss: 0.8196 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 335 | ANN: testLoss: 0.9100 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 335 | ANN: testLoss: 0.8457 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 335 | ANN: testLoss: 0.7855 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 336 | ANN: trainLoss: 0.1485 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 336 | ANN: trainLoss: 0.1128 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 336 | ANN: trainLoss: 0.0965 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 336 | ANN: trainLoss: 0.0935 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 336 | ANN: trainLoss: 0.0945 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 336 | ANN: trainLoss: 0.0973 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 336 | ANN: trainLoss: 0.0986 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 336 | ANN: trainLoss: 0.1005 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 336 | ANN: trainLoss: 0.1053 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 336 | ANN: trainLoss: 0.1045 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 336 | ANN: trainLoss: 0.1093 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 336 | ANN: trainLoss: 0.1050 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 336 | ANN: trainLoss: 0.0982 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 336 | ANN: testLoss: 0.9612 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 336 | ANN: testLoss: 0.8091 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 336 | ANN: testLoss: 0.8575 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 336 | ANN: testLoss: 1.1934 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 337 | ANN: trainLoss: 0.1727 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 337 | ANN: trainLoss: 0.1341 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 337 | ANN: trainLoss: 0.1127 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 337 | ANN: trainLoss: 0.1033 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 337 | ANN: trainLoss: 0.1009 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 337 | ANN: trainLoss: 0.1080 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 337 | ANN: trainLoss: 0.1163 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 337 | ANN: trainLoss: 0.1272 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 337 | ANN: trainLoss: 0.1343 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 337 | ANN: trainLoss: 0.1348 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 337 | ANN: trainLoss: 0.1260 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 337 | ANN: trainLoss: 0.1211 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 337 | ANN: trainLoss: 0.1331 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 337 | ANN: testLoss: 0.8164 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 337 | ANN: testLoss: 0.9442 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 337 | ANN: testLoss: 0.8505 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 337 | ANN: testLoss: 1.2750 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 338 | ANN: trainLoss: 0.0759 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 338 | ANN: trainLoss: 0.1052 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 338 | ANN: trainLoss: 0.0946 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 338 | ANN: trainLoss: 0.1111 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 338 | ANN: trainLoss: 0.1129 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 338 | ANN: trainLoss: 0.1056 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 338 | ANN: trainLoss: 0.1106 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 338 | ANN: trainLoss: 0.1088 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 338 | ANN: trainLoss: 0.1116 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 338 | ANN: trainLoss: 0.1094 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 338 | ANN: trainLoss: 0.1143 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 338 | ANN: trainLoss: 0.1203 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 338 | ANN: trainLoss: 0.1184 | trainAcc: 95.2010% (734/771)\n",
            "0 4 Epoch: 338 | ANN: testLoss: 0.8052 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 338 | ANN: testLoss: 1.0366 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 338 | ANN: testLoss: 0.8778 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 338 | ANN: testLoss: 1.1026 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 339 | ANN: trainLoss: 0.0801 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 339 | ANN: trainLoss: 0.0784 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 339 | ANN: trainLoss: 0.0827 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 339 | ANN: trainLoss: 0.1129 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 339 | ANN: trainLoss: 0.1166 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 339 | ANN: trainLoss: 0.1191 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 339 | ANN: trainLoss: 0.1209 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 339 | ANN: trainLoss: 0.1287 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 339 | ANN: trainLoss: 0.1221 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 339 | ANN: trainLoss: 0.1177 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 339 | ANN: trainLoss: 0.1154 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 339 | ANN: trainLoss: 0.1174 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 339 | ANN: trainLoss: 0.1109 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 339 | ANN: testLoss: 1.0506 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 339 | ANN: testLoss: 0.9945 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 339 | ANN: testLoss: 0.8896 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 339 | ANN: testLoss: 0.7635 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 340 | ANN: trainLoss: 0.1323 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 340 | ANN: trainLoss: 0.1672 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 340 | ANN: trainLoss: 0.1334 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 340 | ANN: trainLoss: 0.1198 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 340 | ANN: trainLoss: 0.1201 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 340 | ANN: trainLoss: 0.1341 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 340 | ANN: trainLoss: 0.1272 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 340 | ANN: trainLoss: 0.1184 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 340 | ANN: trainLoss: 0.1197 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 340 | ANN: trainLoss: 0.1197 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 340 | ANN: trainLoss: 0.1207 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 340 | ANN: trainLoss: 0.1202 | trainAcc: 95.5729% (734/768)\n",
            "12 13 Epoch: 340 | ANN: trainLoss: 0.1145 | trainAcc: 95.5901% (737/771)\n",
            "0 4 Epoch: 340 | ANN: testLoss: 0.8151 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 340 | ANN: testLoss: 0.7480 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 340 | ANN: testLoss: 0.9070 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 340 | ANN: testLoss: 0.6803 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 341 | ANN: trainLoss: 0.0947 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 341 | ANN: trainLoss: 0.0868 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 341 | ANN: trainLoss: 0.0881 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 341 | ANN: trainLoss: 0.0903 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 341 | ANN: trainLoss: 0.0847 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 341 | ANN: trainLoss: 0.0888 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 341 | ANN: trainLoss: 0.0925 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 341 | ANN: trainLoss: 0.0973 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 341 | ANN: trainLoss: 0.0930 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 341 | ANN: trainLoss: 0.0999 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 341 | ANN: trainLoss: 0.0986 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 341 | ANN: trainLoss: 0.0978 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 341 | ANN: trainLoss: 0.0908 | trainAcc: 96.6278% (745/771)\n",
            "0 4 Epoch: 341 | ANN: testLoss: 0.8266 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 341 | ANN: testLoss: 1.0203 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 341 | ANN: testLoss: 0.8980 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 341 | ANN: testLoss: 0.6766 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 342 | ANN: trainLoss: 0.1395 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 342 | ANN: trainLoss: 0.1115 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 342 | ANN: trainLoss: 0.1065 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 342 | ANN: trainLoss: 0.1150 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 342 | ANN: trainLoss: 0.1115 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 342 | ANN: trainLoss: 0.1102 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 342 | ANN: trainLoss: 0.1211 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 342 | ANN: trainLoss: 0.1235 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 342 | ANN: trainLoss: 0.1228 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 342 | ANN: trainLoss: 0.1287 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 342 | ANN: trainLoss: 0.1258 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 342 | ANN: trainLoss: 0.1272 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 342 | ANN: trainLoss: 0.1447 | trainAcc: 94.0337% (725/771)\n",
            "0 4 Epoch: 342 | ANN: testLoss: 0.9574 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 342 | ANN: testLoss: 0.8881 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 342 | ANN: testLoss: 0.8621 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 342 | ANN: testLoss: 2.6865 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 343 | ANN: trainLoss: 0.0511 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 343 | ANN: trainLoss: 0.0649 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 343 | ANN: trainLoss: 0.0712 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 343 | ANN: trainLoss: 0.0802 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 343 | ANN: trainLoss: 0.0935 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 343 | ANN: trainLoss: 0.0938 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 343 | ANN: trainLoss: 0.0997 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 343 | ANN: trainLoss: 0.0970 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 343 | ANN: trainLoss: 0.0972 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 343 | ANN: trainLoss: 0.0976 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 343 | ANN: trainLoss: 0.0928 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 343 | ANN: trainLoss: 0.0967 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 343 | ANN: trainLoss: 0.2067 | trainAcc: 96.6278% (745/771)\n",
            "0 4 Epoch: 343 | ANN: testLoss: 0.7259 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 343 | ANN: testLoss: 0.8634 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 343 | ANN: testLoss: 0.9052 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 343 | ANN: testLoss: 1.1746 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 344 | ANN: trainLoss: 0.1958 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 344 | ANN: trainLoss: 0.1882 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 344 | ANN: trainLoss: 0.1686 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 344 | ANN: trainLoss: 0.1458 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 344 | ANN: trainLoss: 0.1298 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 344 | ANN: trainLoss: 0.1251 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 344 | ANN: trainLoss: 0.1164 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 344 | ANN: trainLoss: 0.1146 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 344 | ANN: trainLoss: 0.1097 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 344 | ANN: trainLoss: 0.1056 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 344 | ANN: trainLoss: 0.1024 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 344 | ANN: trainLoss: 0.1040 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 344 | ANN: trainLoss: 0.0983 | trainAcc: 96.1089% (741/771)\n",
            "0 4 Epoch: 344 | ANN: testLoss: 0.9086 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 344 | ANN: testLoss: 0.9103 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 344 | ANN: testLoss: 0.8485 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 344 | ANN: testLoss: 2.5335 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 345 | ANN: trainLoss: 0.1241 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 345 | ANN: trainLoss: 0.1143 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 345 | ANN: trainLoss: 0.1130 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 345 | ANN: trainLoss: 0.1109 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 345 | ANN: trainLoss: 0.1051 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 345 | ANN: trainLoss: 0.1008 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 345 | ANN: trainLoss: 0.0950 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 345 | ANN: trainLoss: 0.0954 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 345 | ANN: trainLoss: 0.0950 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 345 | ANN: trainLoss: 0.0948 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 345 | ANN: trainLoss: 0.0958 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 345 | ANN: trainLoss: 0.0953 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 345 | ANN: trainLoss: 0.1453 | trainAcc: 96.2387% (742/771)\n",
            "0 4 Epoch: 345 | ANN: testLoss: 0.7327 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 345 | ANN: testLoss: 0.7229 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 345 | ANN: testLoss: 0.8687 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 345 | ANN: testLoss: 1.2610 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 346 | ANN: trainLoss: 0.0534 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 346 | ANN: trainLoss: 0.0881 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 346 | ANN: trainLoss: 0.1020 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 346 | ANN: trainLoss: 0.1176 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 346 | ANN: trainLoss: 0.1281 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 346 | ANN: trainLoss: 0.1290 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 346 | ANN: trainLoss: 0.1216 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 346 | ANN: trainLoss: 0.1174 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 346 | ANN: trainLoss: 0.1218 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 346 | ANN: trainLoss: 0.1151 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 346 | ANN: trainLoss: 0.1133 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 346 | ANN: trainLoss: 0.1119 | trainAcc: 95.3125% (732/768)\n",
            "12 13 Epoch: 346 | ANN: trainLoss: 0.2020 | trainAcc: 95.2010% (734/771)\n",
            "0 4 Epoch: 346 | ANN: testLoss: 1.0027 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 346 | ANN: testLoss: 0.8619 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 346 | ANN: testLoss: 0.8489 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 346 | ANN: testLoss: 1.3644 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 347 | ANN: trainLoss: 0.1693 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 347 | ANN: trainLoss: 0.1409 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 347 | ANN: trainLoss: 0.1418 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 347 | ANN: trainLoss: 0.1309 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 347 | ANN: trainLoss: 0.1311 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 347 | ANN: trainLoss: 0.1310 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 347 | ANN: trainLoss: 0.1238 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 347 | ANN: trainLoss: 0.1254 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 347 | ANN: trainLoss: 0.1255 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 347 | ANN: trainLoss: 0.1311 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 347 | ANN: trainLoss: 0.1323 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 347 | ANN: trainLoss: 0.1329 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 347 | ANN: trainLoss: 0.2219 | trainAcc: 94.5525% (729/771)\n",
            "0 4 Epoch: 347 | ANN: testLoss: 0.8707 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 347 | ANN: testLoss: 0.8454 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 347 | ANN: testLoss: 0.8580 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 347 | ANN: testLoss: 1.7914 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 348 | ANN: trainLoss: 0.1163 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 348 | ANN: trainLoss: 0.0912 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 348 | ANN: trainLoss: 0.1097 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 348 | ANN: trainLoss: 0.1146 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 348 | ANN: trainLoss: 0.1249 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 348 | ANN: trainLoss: 0.1176 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 348 | ANN: trainLoss: 0.1167 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 348 | ANN: trainLoss: 0.1149 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 348 | ANN: trainLoss: 0.1188 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 348 | ANN: trainLoss: 0.1153 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 348 | ANN: trainLoss: 0.1174 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 348 | ANN: trainLoss: 0.1240 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 348 | ANN: trainLoss: 0.1552 | trainAcc: 95.3307% (735/771)\n",
            "0 4 Epoch: 348 | ANN: testLoss: 0.9715 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 348 | ANN: testLoss: 0.7690 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 348 | ANN: testLoss: 0.8534 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 348 | ANN: testLoss: 0.6402 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 349 | ANN: trainLoss: 0.1846 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 349 | ANN: trainLoss: 0.1440 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 349 | ANN: trainLoss: 0.1449 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 349 | ANN: trainLoss: 0.1449 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 349 | ANN: trainLoss: 0.1355 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 349 | ANN: trainLoss: 0.1426 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 349 | ANN: trainLoss: 0.1362 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 349 | ANN: trainLoss: 0.1393 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 349 | ANN: trainLoss: 0.1378 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 349 | ANN: trainLoss: 0.1431 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 349 | ANN: trainLoss: 0.1360 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 349 | ANN: trainLoss: 0.1372 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 349 | ANN: trainLoss: 0.1966 | trainAcc: 94.2931% (727/771)\n",
            "0 4 Epoch: 349 | ANN: testLoss: 0.6628 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 349 | ANN: testLoss: 0.6134 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 349 | ANN: testLoss: 0.8243 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 349 | ANN: testLoss: 0.9701 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 350 | ANN: trainLoss: 0.1078 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 350 | ANN: trainLoss: 0.1168 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 350 | ANN: trainLoss: 0.1097 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 350 | ANN: trainLoss: 0.1153 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 350 | ANN: trainLoss: 0.1056 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 350 | ANN: trainLoss: 0.1107 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 350 | ANN: trainLoss: 0.1061 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 350 | ANN: trainLoss: 0.1056 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 350 | ANN: trainLoss: 0.1063 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 350 | ANN: trainLoss: 0.1066 | trainAcc: 95.6250% (612/640)\n",
            "10 13 Epoch: 350 | ANN: trainLoss: 0.1095 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 350 | ANN: trainLoss: 0.1110 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 350 | ANN: trainLoss: 0.3920 | trainAcc: 94.9416% (732/771)\n",
            "0 4 Epoch: 350 | ANN: testLoss: 0.7335 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 350 | ANN: testLoss: 0.9124 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 350 | ANN: testLoss: 0.8726 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 350 | ANN: testLoss: 0.9554 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 351 | ANN: trainLoss: 0.0809 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 351 | ANN: trainLoss: 0.0859 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 351 | ANN: trainLoss: 0.1045 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 351 | ANN: trainLoss: 0.1033 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 351 | ANN: trainLoss: 0.1013 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 351 | ANN: trainLoss: 0.1058 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 351 | ANN: trainLoss: 0.1041 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 351 | ANN: trainLoss: 0.1052 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 351 | ANN: trainLoss: 0.1034 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 351 | ANN: trainLoss: 0.1075 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 351 | ANN: trainLoss: 0.1096 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 351 | ANN: trainLoss: 0.1158 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 351 | ANN: trainLoss: 0.1116 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 351 | ANN: testLoss: 0.9667 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 351 | ANN: testLoss: 0.9515 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 351 | ANN: testLoss: 0.8518 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 351 | ANN: testLoss: 0.9723 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 352 | ANN: trainLoss: 0.1285 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 352 | ANN: trainLoss: 0.1012 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 352 | ANN: trainLoss: 0.1056 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 352 | ANN: trainLoss: 0.1067 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 352 | ANN: trainLoss: 0.1002 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 352 | ANN: trainLoss: 0.1050 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 352 | ANN: trainLoss: 0.1049 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 352 | ANN: trainLoss: 0.1046 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 352 | ANN: trainLoss: 0.1096 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 352 | ANN: trainLoss: 0.1100 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 352 | ANN: trainLoss: 0.1086 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 352 | ANN: trainLoss: 0.1124 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 352 | ANN: trainLoss: 0.1052 | trainAcc: 96.3684% (743/771)\n",
            "0 4 Epoch: 352 | ANN: testLoss: 0.8756 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 352 | ANN: testLoss: 0.9152 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 352 | ANN: testLoss: 0.8160 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 352 | ANN: testLoss: 3.1078 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 353 | ANN: trainLoss: 0.1754 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 353 | ANN: trainLoss: 0.1450 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 353 | ANN: trainLoss: 0.1718 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 353 | ANN: trainLoss: 0.1506 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 353 | ANN: trainLoss: 0.1384 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 353 | ANN: trainLoss: 0.1368 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 353 | ANN: trainLoss: 0.1291 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 353 | ANN: trainLoss: 0.1270 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 353 | ANN: trainLoss: 0.1254 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 353 | ANN: trainLoss: 0.1331 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 353 | ANN: trainLoss: 0.1286 | trainAcc: 95.0284% (669/704)\n",
            "11 13 Epoch: 353 | ANN: trainLoss: 0.1350 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 353 | ANN: trainLoss: 0.1254 | trainAcc: 94.5525% (729/771)\n",
            "0 4 Epoch: 353 | ANN: testLoss: 1.0196 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 353 | ANN: testLoss: 0.8161 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 353 | ANN: testLoss: 0.8685 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 353 | ANN: testLoss: 0.6839 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 354 | ANN: trainLoss: 0.1934 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 354 | ANN: trainLoss: 0.1586 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 354 | ANN: trainLoss: 0.1329 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 354 | ANN: trainLoss: 0.1452 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 354 | ANN: trainLoss: 0.1431 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 354 | ANN: trainLoss: 0.1506 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 354 | ANN: trainLoss: 0.1473 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 354 | ANN: trainLoss: 0.1419 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 354 | ANN: trainLoss: 0.1380 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 354 | ANN: trainLoss: 0.1323 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 354 | ANN: trainLoss: 0.1319 | trainAcc: 95.0284% (669/704)\n",
            "11 13 Epoch: 354 | ANN: trainLoss: 0.1342 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 354 | ANN: trainLoss: 0.1446 | trainAcc: 94.4228% (728/771)\n",
            "0 4 Epoch: 354 | ANN: testLoss: 1.1494 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 354 | ANN: testLoss: 1.0168 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 354 | ANN: testLoss: 0.9148 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 354 | ANN: testLoss: 0.7163 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 355 | ANN: trainLoss: 0.1582 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 355 | ANN: trainLoss: 0.1427 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 355 | ANN: trainLoss: 0.1577 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 355 | ANN: trainLoss: 0.1329 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 355 | ANN: trainLoss: 0.1489 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 355 | ANN: trainLoss: 0.1351 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 355 | ANN: trainLoss: 0.1283 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 355 | ANN: trainLoss: 0.1246 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 355 | ANN: trainLoss: 0.1223 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 355 | ANN: trainLoss: 0.1202 | trainAcc: 95.6250% (612/640)\n",
            "10 13 Epoch: 355 | ANN: trainLoss: 0.1201 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 355 | ANN: trainLoss: 0.1153 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 355 | ANN: trainLoss: 0.1094 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 355 | ANN: testLoss: 0.9396 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 355 | ANN: testLoss: 0.9138 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 355 | ANN: testLoss: 0.9105 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 355 | ANN: testLoss: 0.9506 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 356 | ANN: trainLoss: 0.0901 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 356 | ANN: trainLoss: 0.1460 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 356 | ANN: trainLoss: 0.1458 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 356 | ANN: trainLoss: 0.1379 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 356 | ANN: trainLoss: 0.1292 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 356 | ANN: trainLoss: 0.1256 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 356 | ANN: trainLoss: 0.1156 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 356 | ANN: trainLoss: 0.1085 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 356 | ANN: trainLoss: 0.1125 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 356 | ANN: trainLoss: 0.1149 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 356 | ANN: trainLoss: 0.1133 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 356 | ANN: trainLoss: 0.1088 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 356 | ANN: trainLoss: 0.1081 | trainAcc: 96.2387% (742/771)\n",
            "0 4 Epoch: 356 | ANN: testLoss: 0.9268 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 356 | ANN: testLoss: 0.8170 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 356 | ANN: testLoss: 0.8819 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 356 | ANN: testLoss: 1.8593 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 357 | ANN: trainLoss: 0.1632 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 357 | ANN: trainLoss: 0.1772 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 357 | ANN: trainLoss: 0.1565 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 357 | ANN: trainLoss: 0.1618 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 357 | ANN: trainLoss: 0.1435 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 357 | ANN: trainLoss: 0.1353 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 357 | ANN: trainLoss: 0.1264 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 357 | ANN: trainLoss: 0.1264 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 357 | ANN: trainLoss: 0.1266 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 357 | ANN: trainLoss: 0.1217 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 357 | ANN: trainLoss: 0.1183 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 357 | ANN: trainLoss: 0.1131 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 357 | ANN: trainLoss: 0.1046 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 357 | ANN: testLoss: 0.6846 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 357 | ANN: testLoss: 0.8458 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 357 | ANN: testLoss: 0.8553 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 357 | ANN: testLoss: 0.6416 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 358 | ANN: trainLoss: 0.0889 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 358 | ANN: trainLoss: 0.0930 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 358 | ANN: trainLoss: 0.0813 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 358 | ANN: trainLoss: 0.1057 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 358 | ANN: trainLoss: 0.1129 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 358 | ANN: trainLoss: 0.1080 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 358 | ANN: trainLoss: 0.1144 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 358 | ANN: trainLoss: 0.1190 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 358 | ANN: trainLoss: 0.1140 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 358 | ANN: trainLoss: 0.1176 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 358 | ANN: trainLoss: 0.1139 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 358 | ANN: trainLoss: 0.1149 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 358 | ANN: trainLoss: 0.1652 | trainAcc: 95.0713% (733/771)\n",
            "0 4 Epoch: 358 | ANN: testLoss: 0.8678 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 358 | ANN: testLoss: 0.8266 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 358 | ANN: testLoss: 0.8286 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 358 | ANN: testLoss: 1.9472 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 359 | ANN: trainLoss: 0.0692 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 359 | ANN: trainLoss: 0.1271 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 359 | ANN: trainLoss: 0.1073 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 359 | ANN: trainLoss: 0.1026 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 359 | ANN: trainLoss: 0.0939 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 359 | ANN: trainLoss: 0.1086 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 359 | ANN: trainLoss: 0.1028 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 359 | ANN: trainLoss: 0.0976 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 359 | ANN: trainLoss: 0.0919 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 359 | ANN: trainLoss: 0.0966 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 359 | ANN: trainLoss: 0.0975 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 359 | ANN: trainLoss: 0.0958 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 359 | ANN: trainLoss: 0.0891 | trainAcc: 96.8872% (747/771)\n",
            "0 4 Epoch: 359 | ANN: testLoss: 0.8619 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 359 | ANN: testLoss: 1.0007 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 359 | ANN: testLoss: 0.8889 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 359 | ANN: testLoss: 0.9172 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 360 | ANN: trainLoss: 0.1583 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 360 | ANN: trainLoss: 0.1390 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 360 | ANN: trainLoss: 0.1386 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 360 | ANN: trainLoss: 0.1266 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 360 | ANN: trainLoss: 0.1187 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 360 | ANN: trainLoss: 0.1127 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 360 | ANN: trainLoss: 0.1102 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 360 | ANN: trainLoss: 0.1131 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 360 | ANN: trainLoss: 0.1099 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 360 | ANN: trainLoss: 0.1063 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 360 | ANN: trainLoss: 0.1023 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 360 | ANN: trainLoss: 0.0978 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 360 | ANN: trainLoss: 0.0981 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 360 | ANN: testLoss: 0.8273 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 360 | ANN: testLoss: 0.9432 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 360 | ANN: testLoss: 0.8788 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 360 | ANN: testLoss: 0.8918 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 361 | ANN: trainLoss: 0.1662 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 361 | ANN: trainLoss: 0.1331 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 361 | ANN: trainLoss: 0.1138 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 361 | ANN: trainLoss: 0.1064 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 361 | ANN: trainLoss: 0.1207 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 361 | ANN: trainLoss: 0.1268 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 361 | ANN: trainLoss: 0.1248 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 361 | ANN: trainLoss: 0.1202 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 361 | ANN: trainLoss: 0.1119 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 361 | ANN: trainLoss: 0.1103 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 361 | ANN: trainLoss: 0.1058 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 361 | ANN: trainLoss: 0.1083 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 361 | ANN: trainLoss: 0.1305 | trainAcc: 95.3307% (735/771)\n",
            "0 4 Epoch: 361 | ANN: testLoss: 0.7881 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 361 | ANN: testLoss: 1.0179 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 361 | ANN: testLoss: 0.8878 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 361 | ANN: testLoss: 1.9048 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 362 | ANN: trainLoss: 0.0678 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 362 | ANN: trainLoss: 0.0783 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 362 | ANN: trainLoss: 0.0735 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 362 | ANN: trainLoss: 0.0787 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 362 | ANN: trainLoss: 0.0892 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 362 | ANN: trainLoss: 0.0894 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 362 | ANN: trainLoss: 0.0897 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 362 | ANN: trainLoss: 0.0848 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 362 | ANN: trainLoss: 0.0926 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 362 | ANN: trainLoss: 0.0954 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 362 | ANN: trainLoss: 0.0942 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 362 | ANN: trainLoss: 0.0972 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 362 | ANN: trainLoss: 0.1171 | trainAcc: 96.3684% (743/771)\n",
            "0 4 Epoch: 362 | ANN: testLoss: 0.8979 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 362 | ANN: testLoss: 0.9129 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 362 | ANN: testLoss: 0.9129 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 362 | ANN: testLoss: 0.7109 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 363 | ANN: trainLoss: 0.1072 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 363 | ANN: trainLoss: 0.1275 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 363 | ANN: trainLoss: 0.1024 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 363 | ANN: trainLoss: 0.0906 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 363 | ANN: trainLoss: 0.1012 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 363 | ANN: trainLoss: 0.1008 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 363 | ANN: trainLoss: 0.1058 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 363 | ANN: trainLoss: 0.1010 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 363 | ANN: trainLoss: 0.1021 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 363 | ANN: trainLoss: 0.1013 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 363 | ANN: trainLoss: 0.0992 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 363 | ANN: trainLoss: 0.1029 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 363 | ANN: trainLoss: 0.1167 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 363 | ANN: testLoss: 0.7779 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 363 | ANN: testLoss: 0.8901 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 363 | ANN: testLoss: 0.9425 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 363 | ANN: testLoss: 0.7070 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 364 | ANN: trainLoss: 0.1000 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 364 | ANN: trainLoss: 0.1064 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 364 | ANN: trainLoss: 0.0977 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 364 | ANN: trainLoss: 0.0956 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 364 | ANN: trainLoss: 0.0976 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 364 | ANN: trainLoss: 0.1066 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 364 | ANN: trainLoss: 0.1198 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 364 | ANN: trainLoss: 0.1215 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 364 | ANN: trainLoss: 0.1163 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 364 | ANN: trainLoss: 0.1185 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 364 | ANN: trainLoss: 0.1215 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 364 | ANN: trainLoss: 0.1215 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 364 | ANN: trainLoss: 0.1151 | trainAcc: 94.8119% (731/771)\n",
            "0 4 Epoch: 364 | ANN: testLoss: 0.8182 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 364 | ANN: testLoss: 0.8715 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 364 | ANN: testLoss: 0.9163 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 364 | ANN: testLoss: 1.8120 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 365 | ANN: trainLoss: 0.0846 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 365 | ANN: trainLoss: 0.0986 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 365 | ANN: trainLoss: 0.1160 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 365 | ANN: trainLoss: 0.1300 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 365 | ANN: trainLoss: 0.1182 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 365 | ANN: trainLoss: 0.1157 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 365 | ANN: trainLoss: 0.1172 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 365 | ANN: trainLoss: 0.1154 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 365 | ANN: trainLoss: 0.1089 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 365 | ANN: trainLoss: 0.1091 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 365 | ANN: trainLoss: 0.1188 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 365 | ANN: trainLoss: 0.1152 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 365 | ANN: trainLoss: 0.1186 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 365 | ANN: testLoss: 0.8989 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 365 | ANN: testLoss: 0.8201 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 365 | ANN: testLoss: 0.9386 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 365 | ANN: testLoss: 0.7081 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 366 | ANN: trainLoss: 0.0836 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 366 | ANN: trainLoss: 0.0899 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 366 | ANN: trainLoss: 0.0943 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 366 | ANN: trainLoss: 0.1244 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 366 | ANN: trainLoss: 0.1177 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 366 | ANN: trainLoss: 0.1160 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 366 | ANN: trainLoss: 0.1135 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 366 | ANN: trainLoss: 0.1114 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 366 | ANN: trainLoss: 0.1046 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 366 | ANN: trainLoss: 0.1021 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 366 | ANN: trainLoss: 0.1011 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 366 | ANN: trainLoss: 0.1024 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 366 | ANN: trainLoss: 0.1179 | trainAcc: 96.1089% (741/771)\n",
            "0 4 Epoch: 366 | ANN: testLoss: 1.0020 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 366 | ANN: testLoss: 0.9691 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 366 | ANN: testLoss: 0.9675 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 366 | ANN: testLoss: 0.9239 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 367 | ANN: trainLoss: 0.0821 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 367 | ANN: trainLoss: 0.1244 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 367 | ANN: trainLoss: 0.1058 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 367 | ANN: trainLoss: 0.1242 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 367 | ANN: trainLoss: 0.1149 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 367 | ANN: trainLoss: 0.1258 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 367 | ANN: trainLoss: 0.1317 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 367 | ANN: trainLoss: 0.1334 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 367 | ANN: trainLoss: 0.1282 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 367 | ANN: trainLoss: 0.1314 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 367 | ANN: trainLoss: 0.1274 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 367 | ANN: trainLoss: 0.1240 | trainAcc: 95.0521% (730/768)\n",
            "12 13 Epoch: 367 | ANN: trainLoss: 0.2354 | trainAcc: 94.8119% (731/771)\n",
            "0 4 Epoch: 367 | ANN: testLoss: 0.7914 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 367 | ANN: testLoss: 0.8651 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 367 | ANN: testLoss: 0.9405 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 367 | ANN: testLoss: 0.7054 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 368 | ANN: trainLoss: 0.1310 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 368 | ANN: trainLoss: 0.1543 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 368 | ANN: trainLoss: 0.1205 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 368 | ANN: trainLoss: 0.1155 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 368 | ANN: trainLoss: 0.1136 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 368 | ANN: trainLoss: 0.1124 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 368 | ANN: trainLoss: 0.1079 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 368 | ANN: trainLoss: 0.1100 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 368 | ANN: trainLoss: 0.1073 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 368 | ANN: trainLoss: 0.1009 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 368 | ANN: trainLoss: 0.1049 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 368 | ANN: trainLoss: 0.1068 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 368 | ANN: trainLoss: 0.1185 | trainAcc: 96.2387% (742/771)\n",
            "0 4 Epoch: 368 | ANN: testLoss: 0.8092 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 368 | ANN: testLoss: 0.7946 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 368 | ANN: testLoss: 0.9211 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 368 | ANN: testLoss: 2.1829 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 369 | ANN: trainLoss: 0.1085 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 369 | ANN: trainLoss: 0.1152 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 369 | ANN: trainLoss: 0.1130 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 369 | ANN: trainLoss: 0.1126 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 369 | ANN: trainLoss: 0.1035 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 369 | ANN: trainLoss: 0.1078 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 369 | ANN: trainLoss: 0.1066 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 369 | ANN: trainLoss: 0.1117 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 369 | ANN: trainLoss: 0.1148 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 369 | ANN: trainLoss: 0.1111 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 369 | ANN: trainLoss: 0.1076 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 369 | ANN: trainLoss: 0.1045 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 369 | ANN: trainLoss: 0.1670 | trainAcc: 95.2010% (734/771)\n",
            "0 4 Epoch: 369 | ANN: testLoss: 0.9076 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 369 | ANN: testLoss: 0.9062 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 369 | ANN: testLoss: 0.9803 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 369 | ANN: testLoss: 0.7353 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 370 | ANN: trainLoss: 0.1462 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 370 | ANN: trainLoss: 0.0974 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 370 | ANN: trainLoss: 0.1069 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 370 | ANN: trainLoss: 0.1050 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 370 | ANN: trainLoss: 0.0982 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 370 | ANN: trainLoss: 0.1079 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 370 | ANN: trainLoss: 0.1120 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 370 | ANN: trainLoss: 0.1079 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 370 | ANN: trainLoss: 0.1099 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 370 | ANN: trainLoss: 0.1150 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 370 | ANN: trainLoss: 0.1125 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 370 | ANN: trainLoss: 0.1079 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 370 | ANN: trainLoss: 0.1283 | trainAcc: 95.5901% (737/771)\n",
            "0 4 Epoch: 370 | ANN: testLoss: 0.9010 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 370 | ANN: testLoss: 0.8875 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 370 | ANN: testLoss: 0.9263 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 370 | ANN: testLoss: 0.6948 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 371 | ANN: trainLoss: 0.1201 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 371 | ANN: trainLoss: 0.1186 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 371 | ANN: trainLoss: 0.1070 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 371 | ANN: trainLoss: 0.1212 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 371 | ANN: trainLoss: 0.1072 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 371 | ANN: trainLoss: 0.1206 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 371 | ANN: trainLoss: 0.1241 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 371 | ANN: trainLoss: 0.1182 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 371 | ANN: trainLoss: 0.1224 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 371 | ANN: trainLoss: 0.1218 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 371 | ANN: trainLoss: 0.1169 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 371 | ANN: trainLoss: 0.1125 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 371 | ANN: trainLoss: 0.1504 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 371 | ANN: testLoss: 1.0727 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 371 | ANN: testLoss: 1.0220 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 371 | ANN: testLoss: 0.9093 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 371 | ANN: testLoss: 0.6821 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 372 | ANN: trainLoss: 0.0760 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 372 | ANN: trainLoss: 0.0719 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 372 | ANN: trainLoss: 0.0782 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 372 | ANN: trainLoss: 0.0990 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 372 | ANN: trainLoss: 0.0965 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 372 | ANN: trainLoss: 0.1080 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 372 | ANN: trainLoss: 0.1042 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 372 | ANN: trainLoss: 0.1056 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 372 | ANN: trainLoss: 0.1120 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 372 | ANN: trainLoss: 0.1110 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 372 | ANN: trainLoss: 0.1131 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 372 | ANN: trainLoss: 0.1098 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 372 | ANN: trainLoss: 0.1224 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 372 | ANN: testLoss: 0.8838 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 372 | ANN: testLoss: 0.9055 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 372 | ANN: testLoss: 0.8782 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 372 | ANN: testLoss: 0.6761 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 373 | ANN: trainLoss: 0.1070 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 373 | ANN: trainLoss: 0.1083 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 373 | ANN: trainLoss: 0.0850 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 373 | ANN: trainLoss: 0.0939 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 373 | ANN: trainLoss: 0.0863 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 373 | ANN: trainLoss: 0.0929 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 373 | ANN: trainLoss: 0.0889 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 373 | ANN: trainLoss: 0.0865 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 373 | ANN: trainLoss: 0.0826 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 373 | ANN: trainLoss: 0.0901 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 373 | ANN: trainLoss: 0.0922 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 373 | ANN: trainLoss: 0.0897 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 373 | ANN: trainLoss: 0.0880 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 373 | ANN: testLoss: 0.6172 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 373 | ANN: testLoss: 0.8137 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 373 | ANN: testLoss: 0.9055 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 373 | ANN: testLoss: 0.9279 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 374 | ANN: trainLoss: 0.1353 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 374 | ANN: trainLoss: 0.1114 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 374 | ANN: trainLoss: 0.1103 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 374 | ANN: trainLoss: 0.1035 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 374 | ANN: trainLoss: 0.1136 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 374 | ANN: trainLoss: 0.1104 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 374 | ANN: trainLoss: 0.1082 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 374 | ANN: trainLoss: 0.1095 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 374 | ANN: trainLoss: 0.1168 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 374 | ANN: trainLoss: 0.1127 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 374 | ANN: trainLoss: 0.1194 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 374 | ANN: trainLoss: 0.1174 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 374 | ANN: trainLoss: 0.1206 | trainAcc: 95.4604% (736/771)\n",
            "0 4 Epoch: 374 | ANN: testLoss: 1.0027 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 374 | ANN: testLoss: 0.9631 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 374 | ANN: testLoss: 0.9275 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 374 | ANN: testLoss: 0.6957 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 375 | ANN: trainLoss: 0.0888 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 375 | ANN: trainLoss: 0.0932 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 375 | ANN: trainLoss: 0.1176 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 375 | ANN: trainLoss: 0.1021 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 375 | ANN: trainLoss: 0.0966 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 375 | ANN: trainLoss: 0.1053 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 375 | ANN: trainLoss: 0.1046 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 375 | ANN: trainLoss: 0.1102 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 375 | ANN: trainLoss: 0.1079 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 375 | ANN: trainLoss: 0.1125 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 375 | ANN: trainLoss: 0.1072 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 375 | ANN: trainLoss: 0.1082 | trainAcc: 95.3125% (732/768)\n",
            "12 13 Epoch: 375 | ANN: trainLoss: 0.1189 | trainAcc: 95.3307% (735/771)\n",
            "0 4 Epoch: 375 | ANN: testLoss: 0.6992 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 375 | ANN: testLoss: 0.7986 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 375 | ANN: testLoss: 0.9085 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 375 | ANN: testLoss: 1.7557 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 376 | ANN: trainLoss: 0.0944 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 376 | ANN: trainLoss: 0.0839 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 376 | ANN: trainLoss: 0.0904 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 376 | ANN: trainLoss: 0.0798 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 376 | ANN: trainLoss: 0.0767 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 376 | ANN: trainLoss: 0.0725 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 376 | ANN: trainLoss: 0.0781 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 376 | ANN: trainLoss: 0.0828 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 376 | ANN: trainLoss: 0.0895 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 376 | ANN: trainLoss: 0.0989 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 376 | ANN: trainLoss: 0.0985 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 376 | ANN: trainLoss: 0.0959 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 376 | ANN: trainLoss: 0.1353 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 376 | ANN: testLoss: 0.9494 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 376 | ANN: testLoss: 0.9036 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 376 | ANN: testLoss: 0.9047 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 376 | ANN: testLoss: 0.6785 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 377 | ANN: trainLoss: 0.0778 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 377 | ANN: trainLoss: 0.0616 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 377 | ANN: trainLoss: 0.0717 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 377 | ANN: trainLoss: 0.0765 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 377 | ANN: trainLoss: 0.0864 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 377 | ANN: trainLoss: 0.0972 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 377 | ANN: trainLoss: 0.0962 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 377 | ANN: trainLoss: 0.0952 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 377 | ANN: trainLoss: 0.0914 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 377 | ANN: trainLoss: 0.0928 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 377 | ANN: trainLoss: 0.0965 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 377 | ANN: trainLoss: 0.0939 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 377 | ANN: trainLoss: 0.0891 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 377 | ANN: testLoss: 1.0462 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 377 | ANN: testLoss: 0.8314 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 377 | ANN: testLoss: 0.9111 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 377 | ANN: testLoss: 1.2377 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 378 | ANN: trainLoss: 0.0399 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 378 | ANN: trainLoss: 0.0729 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 378 | ANN: trainLoss: 0.0638 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 378 | ANN: trainLoss: 0.0791 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 378 | ANN: trainLoss: 0.0834 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 378 | ANN: trainLoss: 0.0850 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 378 | ANN: trainLoss: 0.0815 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 378 | ANN: trainLoss: 0.0882 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 378 | ANN: trainLoss: 0.0899 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 378 | ANN: trainLoss: 0.0987 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 378 | ANN: trainLoss: 0.1003 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 378 | ANN: trainLoss: 0.1065 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 378 | ANN: trainLoss: 0.1141 | trainAcc: 95.7198% (738/771)\n",
            "0 4 Epoch: 378 | ANN: testLoss: 0.8381 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 378 | ANN: testLoss: 0.9277 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 378 | ANN: testLoss: 0.9076 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 378 | ANN: testLoss: 0.7440 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 379 | ANN: trainLoss: 0.0808 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 379 | ANN: trainLoss: 0.0904 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 379 | ANN: trainLoss: 0.0770 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 379 | ANN: trainLoss: 0.0879 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 379 | ANN: trainLoss: 0.0863 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 379 | ANN: trainLoss: 0.0941 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 379 | ANN: trainLoss: 0.0879 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 379 | ANN: trainLoss: 0.1003 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 379 | ANN: trainLoss: 0.1019 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 379 | ANN: trainLoss: 0.0998 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 379 | ANN: trainLoss: 0.0958 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 379 | ANN: trainLoss: 0.0983 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 379 | ANN: trainLoss: 0.0987 | trainAcc: 96.3684% (743/771)\n",
            "0 4 Epoch: 379 | ANN: testLoss: 0.8683 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 379 | ANN: testLoss: 0.8971 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 379 | ANN: testLoss: 0.8910 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 379 | ANN: testLoss: 0.6882 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 380 | ANN: trainLoss: 0.1938 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 380 | ANN: trainLoss: 0.1465 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 380 | ANN: trainLoss: 0.1278 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 380 | ANN: trainLoss: 0.1144 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 380 | ANN: trainLoss: 0.1011 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 380 | ANN: trainLoss: 0.1025 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 380 | ANN: trainLoss: 0.1037 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 380 | ANN: trainLoss: 0.1040 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 380 | ANN: trainLoss: 0.1014 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 380 | ANN: trainLoss: 0.0998 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 380 | ANN: trainLoss: 0.0952 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 380 | ANN: trainLoss: 0.0974 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 380 | ANN: trainLoss: 0.0949 | trainAcc: 96.1089% (741/771)\n",
            "0 4 Epoch: 380 | ANN: testLoss: 0.7642 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 380 | ANN: testLoss: 0.8256 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 380 | ANN: testLoss: 0.8940 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 380 | ANN: testLoss: 2.0317 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 381 | ANN: trainLoss: 0.0901 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 381 | ANN: trainLoss: 0.0968 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 381 | ANN: trainLoss: 0.0738 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 381 | ANN: trainLoss: 0.0745 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 381 | ANN: trainLoss: 0.0700 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 381 | ANN: trainLoss: 0.0761 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 381 | ANN: trainLoss: 0.0702 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 381 | ANN: trainLoss: 0.0718 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 381 | ANN: trainLoss: 0.0759 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 381 | ANN: trainLoss: 0.0788 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 381 | ANN: trainLoss: 0.0792 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 381 | ANN: trainLoss: 0.0835 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 381 | ANN: trainLoss: 0.0771 | trainAcc: 97.2763% (750/771)\n",
            "0 4 Epoch: 381 | ANN: testLoss: 1.0794 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 381 | ANN: testLoss: 1.0503 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 381 | ANN: testLoss: 0.9292 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 381 | ANN: testLoss: 1.2512 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 382 | ANN: trainLoss: 0.1066 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 382 | ANN: trainLoss: 0.1000 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 382 | ANN: trainLoss: 0.0886 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 382 | ANN: trainLoss: 0.0933 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 382 | ANN: trainLoss: 0.1036 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 382 | ANN: trainLoss: 0.0928 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 382 | ANN: trainLoss: 0.0881 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 382 | ANN: trainLoss: 0.0847 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 382 | ANN: trainLoss: 0.0829 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 382 | ANN: trainLoss: 0.0856 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 382 | ANN: trainLoss: 0.0948 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 382 | ANN: trainLoss: 0.0933 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 382 | ANN: trainLoss: 0.1012 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 382 | ANN: testLoss: 0.9190 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 382 | ANN: testLoss: 0.8943 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 382 | ANN: testLoss: 0.9532 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 382 | ANN: testLoss: 0.7150 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 383 | ANN: trainLoss: 0.1363 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 383 | ANN: trainLoss: 0.1353 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 383 | ANN: trainLoss: 0.1249 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 383 | ANN: trainLoss: 0.1122 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 383 | ANN: trainLoss: 0.1079 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 383 | ANN: trainLoss: 0.1002 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 383 | ANN: trainLoss: 0.0952 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 383 | ANN: trainLoss: 0.0999 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 383 | ANN: trainLoss: 0.0994 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 383 | ANN: trainLoss: 0.0997 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 383 | ANN: trainLoss: 0.0973 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 383 | ANN: trainLoss: 0.0942 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 383 | ANN: trainLoss: 0.2784 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 383 | ANN: testLoss: 0.9323 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 383 | ANN: testLoss: 1.0948 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 383 | ANN: testLoss: 1.0136 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 383 | ANN: testLoss: 0.7603 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 384 | ANN: trainLoss: 0.0444 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 384 | ANN: trainLoss: 0.0520 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 384 | ANN: trainLoss: 0.0594 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 384 | ANN: trainLoss: 0.0697 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 384 | ANN: trainLoss: 0.0726 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 384 | ANN: trainLoss: 0.0765 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 384 | ANN: trainLoss: 0.0787 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 384 | ANN: trainLoss: 0.0889 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 384 | ANN: trainLoss: 0.0885 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 384 | ANN: trainLoss: 0.0844 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 384 | ANN: trainLoss: 0.0811 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 384 | ANN: trainLoss: 0.0796 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 384 | ANN: trainLoss: 0.1139 | trainAcc: 97.6654% (753/771)\n",
            "0 4 Epoch: 384 | ANN: testLoss: 1.2430 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 384 | ANN: testLoss: 0.8533 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 384 | ANN: testLoss: 0.9202 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 384 | ANN: testLoss: 1.8541 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 385 | ANN: trainLoss: 0.0971 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 385 | ANN: trainLoss: 0.0999 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 385 | ANN: trainLoss: 0.0849 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 385 | ANN: trainLoss: 0.0948 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 385 | ANN: trainLoss: 0.0834 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 385 | ANN: trainLoss: 0.0877 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 385 | ANN: trainLoss: 0.0913 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 385 | ANN: trainLoss: 0.0858 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 385 | ANN: trainLoss: 0.0858 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 385 | ANN: trainLoss: 0.0914 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 385 | ANN: trainLoss: 0.0904 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 385 | ANN: trainLoss: 0.0906 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 385 | ANN: trainLoss: 0.0853 | trainAcc: 96.6278% (745/771)\n",
            "0 4 Epoch: 385 | ANN: testLoss: 1.2369 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 385 | ANN: testLoss: 1.0436 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 385 | ANN: testLoss: 0.9986 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 385 | ANN: testLoss: 0.7491 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 386 | ANN: trainLoss: 0.1400 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 386 | ANN: trainLoss: 0.1201 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 386 | ANN: trainLoss: 0.1221 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 386 | ANN: trainLoss: 0.1260 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 386 | ANN: trainLoss: 0.1103 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 386 | ANN: trainLoss: 0.1083 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 386 | ANN: trainLoss: 0.1028 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 386 | ANN: trainLoss: 0.1083 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 386 | ANN: trainLoss: 0.1050 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 386 | ANN: trainLoss: 0.1026 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 386 | ANN: trainLoss: 0.0978 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 386 | ANN: trainLoss: 0.1016 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 386 | ANN: trainLoss: 0.1114 | trainAcc: 97.0169% (748/771)\n",
            "0 4 Epoch: 386 | ANN: testLoss: 1.2030 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 386 | ANN: testLoss: 0.9314 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 386 | ANN: testLoss: 0.9558 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 386 | ANN: testLoss: 0.7410 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 387 | ANN: trainLoss: 0.1179 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 387 | ANN: trainLoss: 0.1020 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 387 | ANN: trainLoss: 0.0848 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 387 | ANN: trainLoss: 0.0890 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 387 | ANN: trainLoss: 0.0820 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 387 | ANN: trainLoss: 0.0929 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 387 | ANN: trainLoss: 0.0960 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 387 | ANN: trainLoss: 0.0999 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 387 | ANN: trainLoss: 0.0969 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 387 | ANN: trainLoss: 0.0965 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 387 | ANN: trainLoss: 0.0934 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 387 | ANN: trainLoss: 0.1021 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 387 | ANN: trainLoss: 0.1188 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 387 | ANN: testLoss: 0.9271 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 387 | ANN: testLoss: 1.0747 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 387 | ANN: testLoss: 0.9458 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 387 | ANN: testLoss: 0.7182 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 388 | ANN: trainLoss: 0.0414 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 388 | ANN: trainLoss: 0.0496 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 388 | ANN: trainLoss: 0.0583 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 388 | ANN: trainLoss: 0.0699 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 388 | ANN: trainLoss: 0.0712 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 388 | ANN: trainLoss: 0.0647 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 388 | ANN: trainLoss: 0.0727 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 388 | ANN: trainLoss: 0.0704 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 388 | ANN: trainLoss: 0.0759 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 388 | ANN: trainLoss: 0.0813 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 388 | ANN: trainLoss: 0.0846 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 388 | ANN: trainLoss: 0.0845 | trainAcc: 97.5260% (749/768)\n",
            "12 13 Epoch: 388 | ANN: trainLoss: 0.1419 | trainAcc: 97.4060% (751/771)\n",
            "0 4 Epoch: 388 | ANN: testLoss: 0.8132 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 388 | ANN: testLoss: 1.0115 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 388 | ANN: testLoss: 0.9485 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 388 | ANN: testLoss: 0.7114 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 389 | ANN: trainLoss: 0.0489 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 389 | ANN: trainLoss: 0.1066 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 389 | ANN: trainLoss: 0.1330 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 389 | ANN: trainLoss: 0.1244 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 389 | ANN: trainLoss: 0.1138 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 389 | ANN: trainLoss: 0.1210 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 389 | ANN: trainLoss: 0.1118 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 389 | ANN: trainLoss: 0.1124 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 389 | ANN: trainLoss: 0.1096 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 389 | ANN: trainLoss: 0.1042 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 389 | ANN: trainLoss: 0.1051 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 389 | ANN: trainLoss: 0.1090 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 389 | ANN: trainLoss: 0.1442 | trainAcc: 95.3307% (735/771)\n",
            "0 4 Epoch: 389 | ANN: testLoss: 1.1163 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 389 | ANN: testLoss: 0.8984 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 389 | ANN: testLoss: 0.9568 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 389 | ANN: testLoss: 0.7296 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 390 | ANN: trainLoss: 0.1403 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 390 | ANN: trainLoss: 0.1435 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 390 | ANN: trainLoss: 0.1188 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 390 | ANN: trainLoss: 0.1129 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 390 | ANN: trainLoss: 0.1068 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 390 | ANN: trainLoss: 0.1114 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 390 | ANN: trainLoss: 0.1123 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 390 | ANN: trainLoss: 0.1168 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 390 | ANN: trainLoss: 0.1180 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 390 | ANN: trainLoss: 0.1118 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 390 | ANN: trainLoss: 0.1181 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 390 | ANN: trainLoss: 0.1141 | trainAcc: 95.3125% (732/768)\n",
            "12 13 Epoch: 390 | ANN: trainLoss: 0.1102 | trainAcc: 95.3307% (735/771)\n",
            "0 4 Epoch: 390 | ANN: testLoss: 0.8551 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 390 | ANN: testLoss: 0.7431 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 390 | ANN: testLoss: 0.9763 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 390 | ANN: testLoss: 0.7322 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 391 | ANN: trainLoss: 0.0804 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 391 | ANN: trainLoss: 0.0952 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 391 | ANN: trainLoss: 0.0890 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 391 | ANN: trainLoss: 0.0742 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 391 | ANN: trainLoss: 0.0790 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 391 | ANN: trainLoss: 0.0835 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 391 | ANN: trainLoss: 0.0836 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 391 | ANN: trainLoss: 0.0980 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 391 | ANN: trainLoss: 0.0967 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 391 | ANN: trainLoss: 0.0970 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 391 | ANN: trainLoss: 0.0976 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 391 | ANN: trainLoss: 0.0979 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 391 | ANN: trainLoss: 0.0962 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 391 | ANN: testLoss: 0.7648 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 391 | ANN: testLoss: 0.8802 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 391 | ANN: testLoss: 1.0001 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 391 | ANN: testLoss: 0.7501 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 392 | ANN: trainLoss: 0.0813 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 392 | ANN: trainLoss: 0.0905 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 392 | ANN: trainLoss: 0.0818 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 392 | ANN: trainLoss: 0.0842 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 392 | ANN: trainLoss: 0.0972 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 392 | ANN: trainLoss: 0.0952 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 392 | ANN: trainLoss: 0.0973 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 392 | ANN: trainLoss: 0.0952 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 392 | ANN: trainLoss: 0.0912 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 392 | ANN: trainLoss: 0.0912 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 392 | ANN: trainLoss: 0.0917 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 392 | ANN: trainLoss: 0.0905 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 392 | ANN: trainLoss: 0.2921 | trainAcc: 97.0169% (748/771)\n",
            "0 4 Epoch: 392 | ANN: testLoss: 0.8634 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 392 | ANN: testLoss: 1.0102 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 392 | ANN: testLoss: 0.9699 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 392 | ANN: testLoss: 0.7275 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 393 | ANN: trainLoss: 0.0887 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 393 | ANN: trainLoss: 0.1133 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 393 | ANN: trainLoss: 0.1074 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 393 | ANN: trainLoss: 0.1051 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 393 | ANN: trainLoss: 0.1013 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 393 | ANN: trainLoss: 0.1012 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 393 | ANN: trainLoss: 0.1065 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 393 | ANN: trainLoss: 0.1054 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 393 | ANN: trainLoss: 0.0979 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 393 | ANN: trainLoss: 0.0971 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 393 | ANN: trainLoss: 0.0947 | trainAcc: 95.8807% (675/704)\n",
            "11 13 Epoch: 393 | ANN: trainLoss: 0.0901 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 393 | ANN: trainLoss: 0.1163 | trainAcc: 96.1089% (741/771)\n",
            "0 4 Epoch: 393 | ANN: testLoss: 1.2009 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 393 | ANN: testLoss: 0.9419 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 393 | ANN: testLoss: 0.9852 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 393 | ANN: testLoss: 0.7390 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 394 | ANN: trainLoss: 0.0860 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 394 | ANN: trainLoss: 0.0822 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 394 | ANN: trainLoss: 0.1488 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 394 | ANN: trainLoss: 0.1323 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 394 | ANN: trainLoss: 0.1272 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 394 | ANN: trainLoss: 0.1220 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 394 | ANN: trainLoss: 0.1182 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 394 | ANN: trainLoss: 0.1179 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 394 | ANN: trainLoss: 0.1159 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 394 | ANN: trainLoss: 0.1140 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 394 | ANN: trainLoss: 0.1114 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 394 | ANN: trainLoss: 0.1143 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 394 | ANN: trainLoss: 0.1497 | trainAcc: 95.7198% (738/771)\n",
            "0 4 Epoch: 394 | ANN: testLoss: 0.8020 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 394 | ANN: testLoss: 0.8938 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 394 | ANN: testLoss: 0.9619 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 394 | ANN: testLoss: 0.7220 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 395 | ANN: trainLoss: 0.1032 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 395 | ANN: trainLoss: 0.0992 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 395 | ANN: trainLoss: 0.1031 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 395 | ANN: trainLoss: 0.0934 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 395 | ANN: trainLoss: 0.1043 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 395 | ANN: trainLoss: 0.1116 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 395 | ANN: trainLoss: 0.1097 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 395 | ANN: trainLoss: 0.1123 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 395 | ANN: trainLoss: 0.1156 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 395 | ANN: trainLoss: 0.1140 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 395 | ANN: trainLoss: 0.1148 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 395 | ANN: trainLoss: 0.1110 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 395 | ANN: trainLoss: 0.1322 | trainAcc: 96.3684% (743/771)\n",
            "0 4 Epoch: 395 | ANN: testLoss: 1.1007 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 395 | ANN: testLoss: 1.1050 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 395 | ANN: testLoss: 1.0090 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 395 | ANN: testLoss: 0.7569 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 396 | ANN: trainLoss: 0.1829 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 396 | ANN: trainLoss: 0.1584 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 396 | ANN: trainLoss: 0.1281 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 396 | ANN: trainLoss: 0.1270 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 396 | ANN: trainLoss: 0.1225 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 396 | ANN: trainLoss: 0.1116 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 396 | ANN: trainLoss: 0.1088 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 396 | ANN: trainLoss: 0.1165 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 396 | ANN: trainLoss: 0.1241 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 396 | ANN: trainLoss: 0.1199 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 396 | ANN: trainLoss: 0.1203 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 396 | ANN: trainLoss: 0.1167 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 396 | ANN: trainLoss: 0.1082 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 396 | ANN: testLoss: 0.8715 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 396 | ANN: testLoss: 0.9856 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 396 | ANN: testLoss: 1.0009 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 396 | ANN: testLoss: 0.7515 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 397 | ANN: trainLoss: 0.1510 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 397 | ANN: trainLoss: 0.1059 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 397 | ANN: trainLoss: 0.0991 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 397 | ANN: trainLoss: 0.0964 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 397 | ANN: trainLoss: 0.0862 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 397 | ANN: trainLoss: 0.0812 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 397 | ANN: trainLoss: 0.0930 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 397 | ANN: trainLoss: 0.0937 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 397 | ANN: trainLoss: 0.0990 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 397 | ANN: trainLoss: 0.1013 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 397 | ANN: trainLoss: 0.0968 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 397 | ANN: trainLoss: 0.0997 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 397 | ANN: trainLoss: 0.3731 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 397 | ANN: testLoss: 0.8451 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 397 | ANN: testLoss: 1.1063 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 397 | ANN: testLoss: 1.0019 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 397 | ANN: testLoss: 0.7515 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 398 | ANN: trainLoss: 0.0783 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 398 | ANN: trainLoss: 0.0726 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 398 | ANN: trainLoss: 0.0749 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 398 | ANN: trainLoss: 0.1217 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 398 | ANN: trainLoss: 0.1163 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 398 | ANN: trainLoss: 0.1181 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 398 | ANN: trainLoss: 0.1258 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 398 | ANN: trainLoss: 0.1170 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 398 | ANN: trainLoss: 0.1101 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 398 | ANN: trainLoss: 0.1048 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 398 | ANN: trainLoss: 0.1031 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 398 | ANN: trainLoss: 0.1016 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 398 | ANN: trainLoss: 0.0940 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 398 | ANN: testLoss: 1.1122 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 398 | ANN: testLoss: 1.1680 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 398 | ANN: testLoss: 0.9496 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 398 | ANN: testLoss: 0.8774 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 399 | ANN: trainLoss: 0.1233 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 399 | ANN: trainLoss: 0.1133 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 399 | ANN: trainLoss: 0.1151 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 399 | ANN: trainLoss: 0.1090 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 399 | ANN: trainLoss: 0.1038 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 399 | ANN: trainLoss: 0.1105 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 399 | ANN: trainLoss: 0.1122 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 399 | ANN: trainLoss: 0.1092 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 399 | ANN: trainLoss: 0.1051 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 399 | ANN: trainLoss: 0.1091 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 399 | ANN: trainLoss: 0.1083 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 399 | ANN: trainLoss: 0.1121 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 399 | ANN: trainLoss: 0.1383 | trainAcc: 95.0713% (733/771)\n",
            "0 4 Epoch: 399 | ANN: testLoss: 1.0362 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 399 | ANN: testLoss: 0.8584 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 399 | ANN: testLoss: 0.8996 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 399 | ANN: testLoss: 0.6750 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 400 | ANN: trainLoss: 0.0919 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 400 | ANN: trainLoss: 0.0785 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 400 | ANN: trainLoss: 0.0828 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 400 | ANN: trainLoss: 0.0953 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 400 | ANN: trainLoss: 0.1053 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 400 | ANN: trainLoss: 0.0969 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 400 | ANN: trainLoss: 0.0897 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 400 | ANN: trainLoss: 0.0855 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 400 | ANN: trainLoss: 0.0931 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 400 | ANN: trainLoss: 0.0997 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 400 | ANN: trainLoss: 0.0978 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 400 | ANN: trainLoss: 0.0968 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 400 | ANN: trainLoss: 0.0992 | trainAcc: 96.2387% (742/771)\n",
            "0 4 Epoch: 400 | ANN: testLoss: 0.8801 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 400 | ANN: testLoss: 0.7968 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 400 | ANN: testLoss: 0.9191 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 400 | ANN: testLoss: 1.3278 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 401 | ANN: trainLoss: 0.1759 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 401 | ANN: trainLoss: 0.1540 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 401 | ANN: trainLoss: 0.1346 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 401 | ANN: trainLoss: 0.1166 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 401 | ANN: trainLoss: 0.1243 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 401 | ANN: trainLoss: 0.1160 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 401 | ANN: trainLoss: 0.1177 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 401 | ANN: trainLoss: 0.1183 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 401 | ANN: trainLoss: 0.1155 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 401 | ANN: trainLoss: 0.1112 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 401 | ANN: trainLoss: 0.1145 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 401 | ANN: trainLoss: 0.1207 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 401 | ANN: trainLoss: 0.1221 | trainAcc: 95.2010% (734/771)\n",
            "0 4 Epoch: 401 | ANN: testLoss: 0.7020 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 401 | ANN: testLoss: 0.7510 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 401 | ANN: testLoss: 0.9381 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 401 | ANN: testLoss: 0.7040 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 402 | ANN: trainLoss: 0.0651 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 402 | ANN: trainLoss: 0.0804 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 402 | ANN: trainLoss: 0.1131 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 402 | ANN: trainLoss: 0.1010 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 402 | ANN: trainLoss: 0.0990 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 402 | ANN: trainLoss: 0.1035 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 402 | ANN: trainLoss: 0.1013 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 402 | ANN: trainLoss: 0.0977 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 402 | ANN: trainLoss: 0.1011 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 402 | ANN: trainLoss: 0.1005 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 402 | ANN: trainLoss: 0.0984 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 402 | ANN: trainLoss: 0.1033 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 402 | ANN: trainLoss: 0.0954 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 402 | ANN: testLoss: 0.6767 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 402 | ANN: testLoss: 0.8161 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 402 | ANN: testLoss: 0.9260 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 402 | ANN: testLoss: 0.6947 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 403 | ANN: trainLoss: 0.0723 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 403 | ANN: trainLoss: 0.0779 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 403 | ANN: trainLoss: 0.0742 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 403 | ANN: trainLoss: 0.0871 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 403 | ANN: trainLoss: 0.0975 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 403 | ANN: trainLoss: 0.0975 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 403 | ANN: trainLoss: 0.1019 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 403 | ANN: trainLoss: 0.0998 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 403 | ANN: trainLoss: 0.0934 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 403 | ANN: trainLoss: 0.0883 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 403 | ANN: trainLoss: 0.0898 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 403 | ANN: trainLoss: 0.0879 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 403 | ANN: trainLoss: 0.1009 | trainAcc: 96.6278% (745/771)\n",
            "0 4 Epoch: 403 | ANN: testLoss: 1.0211 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 403 | ANN: testLoss: 0.9848 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 403 | ANN: testLoss: 0.9625 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 403 | ANN: testLoss: 0.7285 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 404 | ANN: trainLoss: 0.0931 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 404 | ANN: trainLoss: 0.1068 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 404 | ANN: trainLoss: 0.1024 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 404 | ANN: trainLoss: 0.0953 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 404 | ANN: trainLoss: 0.0994 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 404 | ANN: trainLoss: 0.1075 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 404 | ANN: trainLoss: 0.1018 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 404 | ANN: trainLoss: 0.0962 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 404 | ANN: trainLoss: 0.0895 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 404 | ANN: trainLoss: 0.0887 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 404 | ANN: trainLoss: 0.0928 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 404 | ANN: trainLoss: 0.0907 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 404 | ANN: trainLoss: 0.0986 | trainAcc: 97.0169% (748/771)\n",
            "0 4 Epoch: 404 | ANN: testLoss: 0.8285 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 404 | ANN: testLoss: 1.0023 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 404 | ANN: testLoss: 0.9827 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 404 | ANN: testLoss: 0.8138 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 405 | ANN: trainLoss: 0.0663 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 405 | ANN: trainLoss: 0.0812 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 405 | ANN: trainLoss: 0.0901 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 405 | ANN: trainLoss: 0.0825 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 405 | ANN: trainLoss: 0.0782 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 405 | ANN: trainLoss: 0.0773 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 405 | ANN: trainLoss: 0.0771 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 405 | ANN: trainLoss: 0.0762 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 405 | ANN: trainLoss: 0.0731 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 405 | ANN: trainLoss: 0.0745 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 405 | ANN: trainLoss: 0.0741 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 405 | ANN: trainLoss: 0.0758 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 405 | ANN: trainLoss: 0.3316 | trainAcc: 97.4060% (751/771)\n",
            "0 4 Epoch: 405 | ANN: testLoss: 0.6348 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 405 | ANN: testLoss: 0.8597 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 405 | ANN: testLoss: 0.9770 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 405 | ANN: testLoss: 1.0461 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 406 | ANN: trainLoss: 0.0494 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 406 | ANN: trainLoss: 0.0710 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 406 | ANN: trainLoss: 0.0735 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 406 | ANN: trainLoss: 0.0733 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 406 | ANN: trainLoss: 0.0664 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 406 | ANN: trainLoss: 0.0677 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 406 | ANN: trainLoss: 0.0704 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 406 | ANN: trainLoss: 0.0758 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 406 | ANN: trainLoss: 0.0744 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 406 | ANN: trainLoss: 0.0826 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 406 | ANN: trainLoss: 0.0785 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 406 | ANN: trainLoss: 0.0788 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 406 | ANN: trainLoss: 0.2133 | trainAcc: 96.8872% (747/771)\n",
            "0 4 Epoch: 406 | ANN: testLoss: 0.7433 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 406 | ANN: testLoss: 0.9115 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 406 | ANN: testLoss: 0.8984 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 406 | ANN: testLoss: 0.6738 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 407 | ANN: trainLoss: 0.1987 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 407 | ANN: trainLoss: 0.1565 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 407 | ANN: trainLoss: 0.1176 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 407 | ANN: trainLoss: 0.1060 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 407 | ANN: trainLoss: 0.1085 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 407 | ANN: trainLoss: 0.1071 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 407 | ANN: trainLoss: 0.0991 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 407 | ANN: trainLoss: 0.0971 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 407 | ANN: trainLoss: 0.0942 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 407 | ANN: trainLoss: 0.0966 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 407 | ANN: trainLoss: 0.0980 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 407 | ANN: trainLoss: 0.0977 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 407 | ANN: trainLoss: 0.0994 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 407 | ANN: testLoss: 0.8859 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 407 | ANN: testLoss: 0.9035 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 407 | ANN: testLoss: 0.9573 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 407 | ANN: testLoss: 0.7525 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 408 | ANN: trainLoss: 0.1289 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 408 | ANN: trainLoss: 0.1018 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 408 | ANN: trainLoss: 0.1009 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 408 | ANN: trainLoss: 0.0965 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 408 | ANN: trainLoss: 0.0991 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 408 | ANN: trainLoss: 0.1027 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 408 | ANN: trainLoss: 0.1041 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 408 | ANN: trainLoss: 0.1050 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 408 | ANN: trainLoss: 0.1016 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 408 | ANN: trainLoss: 0.0990 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 408 | ANN: trainLoss: 0.0999 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 408 | ANN: trainLoss: 0.1022 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 408 | ANN: trainLoss: 0.1032 | trainAcc: 96.2387% (742/771)\n",
            "0 4 Epoch: 408 | ANN: testLoss: 1.1194 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 408 | ANN: testLoss: 0.9943 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 408 | ANN: testLoss: 0.9972 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 408 | ANN: testLoss: 0.7479 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 409 | ANN: trainLoss: 0.0729 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 409 | ANN: trainLoss: 0.0703 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 409 | ANN: trainLoss: 0.0713 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 409 | ANN: trainLoss: 0.0809 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 409 | ANN: trainLoss: 0.0887 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 409 | ANN: trainLoss: 0.0904 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 409 | ANN: trainLoss: 0.0826 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 409 | ANN: trainLoss: 0.0814 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 409 | ANN: trainLoss: 0.0828 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 409 | ANN: trainLoss: 0.0862 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 409 | ANN: trainLoss: 0.0847 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 409 | ANN: trainLoss: 0.0834 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 409 | ANN: trainLoss: 0.0771 | trainAcc: 97.6654% (753/771)\n",
            "0 4 Epoch: 409 | ANN: testLoss: 1.1403 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 409 | ANN: testLoss: 0.8741 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 409 | ANN: testLoss: 0.9599 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 409 | ANN: testLoss: 0.7199 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 410 | ANN: trainLoss: 0.0434 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 410 | ANN: trainLoss: 0.0764 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 410 | ANN: trainLoss: 0.0765 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 410 | ANN: trainLoss: 0.0763 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 410 | ANN: trainLoss: 0.0775 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 410 | ANN: trainLoss: 0.0848 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 410 | ANN: trainLoss: 0.0873 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 410 | ANN: trainLoss: 0.0884 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 410 | ANN: trainLoss: 0.0859 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 410 | ANN: trainLoss: 0.0885 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 410 | ANN: trainLoss: 0.0896 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 410 | ANN: trainLoss: 0.0863 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 410 | ANN: trainLoss: 0.0798 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 410 | ANN: testLoss: 0.7852 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 410 | ANN: testLoss: 0.8536 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 410 | ANN: testLoss: 0.9399 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 410 | ANN: testLoss: 0.7050 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 411 | ANN: trainLoss: 0.1044 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 411 | ANN: trainLoss: 0.0894 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 411 | ANN: trainLoss: 0.0903 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 411 | ANN: trainLoss: 0.0903 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 411 | ANN: trainLoss: 0.0895 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 411 | ANN: trainLoss: 0.0872 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 411 | ANN: trainLoss: 0.0899 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 411 | ANN: trainLoss: 0.0965 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 411 | ANN: trainLoss: 0.0934 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 411 | ANN: trainLoss: 0.0930 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 411 | ANN: trainLoss: 0.0918 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 411 | ANN: trainLoss: 0.0891 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 411 | ANN: trainLoss: 0.2191 | trainAcc: 96.3684% (743/771)\n",
            "0 4 Epoch: 411 | ANN: testLoss: 0.9181 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 411 | ANN: testLoss: 1.0567 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 411 | ANN: testLoss: 0.9532 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 411 | ANN: testLoss: 1.3066 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 412 | ANN: trainLoss: 0.0797 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 412 | ANN: trainLoss: 0.0816 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 412 | ANN: trainLoss: 0.0859 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 412 | ANN: trainLoss: 0.0907 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 412 | ANN: trainLoss: 0.0787 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 412 | ANN: trainLoss: 0.0704 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 412 | ANN: trainLoss: 0.0760 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 412 | ANN: trainLoss: 0.0824 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 412 | ANN: trainLoss: 0.0836 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 412 | ANN: trainLoss: 0.0864 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 412 | ANN: trainLoss: 0.0849 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 412 | ANN: trainLoss: 0.0867 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 412 | ANN: trainLoss: 0.1023 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 412 | ANN: testLoss: 1.1980 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 412 | ANN: testLoss: 1.0366 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 412 | ANN: testLoss: 0.9405 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 412 | ANN: testLoss: 2.3809 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 413 | ANN: trainLoss: 0.0745 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 413 | ANN: trainLoss: 0.0804 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 413 | ANN: trainLoss: 0.1037 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 413 | ANN: trainLoss: 0.1159 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 413 | ANN: trainLoss: 0.1143 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 413 | ANN: trainLoss: 0.1049 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 413 | ANN: trainLoss: 0.1047 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 413 | ANN: trainLoss: 0.0967 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 413 | ANN: trainLoss: 0.0943 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 413 | ANN: trainLoss: 0.1003 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 413 | ANN: trainLoss: 0.1019 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 413 | ANN: trainLoss: 0.0978 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 413 | ANN: trainLoss: 0.0920 | trainAcc: 96.6278% (745/771)\n",
            "0 4 Epoch: 413 | ANN: testLoss: 0.8402 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 413 | ANN: testLoss: 0.8104 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 413 | ANN: testLoss: 0.9559 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 413 | ANN: testLoss: 1.1267 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 414 | ANN: trainLoss: 0.0676 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 414 | ANN: trainLoss: 0.1088 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 414 | ANN: trainLoss: 0.0947 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 414 | ANN: trainLoss: 0.0915 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 414 | ANN: trainLoss: 0.1005 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 414 | ANN: trainLoss: 0.1008 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 414 | ANN: trainLoss: 0.1002 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 414 | ANN: trainLoss: 0.0979 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 414 | ANN: trainLoss: 0.0972 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 414 | ANN: trainLoss: 0.0914 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 414 | ANN: trainLoss: 0.0910 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 414 | ANN: trainLoss: 0.0909 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 414 | ANN: trainLoss: 0.0979 | trainAcc: 95.7198% (738/771)\n",
            "0 4 Epoch: 414 | ANN: testLoss: 0.9539 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 414 | ANN: testLoss: 1.1517 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 414 | ANN: testLoss: 0.9556 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 414 | ANN: testLoss: 1.2384 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 415 | ANN: trainLoss: 0.1086 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 415 | ANN: trainLoss: 0.1262 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 415 | ANN: trainLoss: 0.1097 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 415 | ANN: trainLoss: 0.1089 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 415 | ANN: trainLoss: 0.1124 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 415 | ANN: trainLoss: 0.1108 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 415 | ANN: trainLoss: 0.1049 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 415 | ANN: trainLoss: 0.0963 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 415 | ANN: trainLoss: 0.0925 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 415 | ANN: trainLoss: 0.0912 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 415 | ANN: trainLoss: 0.0960 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 415 | ANN: trainLoss: 0.0985 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 415 | ANN: trainLoss: 0.0987 | trainAcc: 96.3684% (743/771)\n",
            "0 4 Epoch: 415 | ANN: testLoss: 1.0283 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 415 | ANN: testLoss: 0.8612 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 415 | ANN: testLoss: 0.9918 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 415 | ANN: testLoss: 0.7439 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 416 | ANN: trainLoss: 0.0891 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 416 | ANN: trainLoss: 0.0782 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 416 | ANN: trainLoss: 0.0631 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 416 | ANN: trainLoss: 0.0687 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 416 | ANN: trainLoss: 0.0734 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 416 | ANN: trainLoss: 0.0756 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 416 | ANN: trainLoss: 0.0729 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 416 | ANN: trainLoss: 0.0740 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 416 | ANN: trainLoss: 0.0813 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 416 | ANN: trainLoss: 0.0781 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 416 | ANN: trainLoss: 0.0816 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 416 | ANN: trainLoss: 0.0832 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 416 | ANN: trainLoss: 0.0795 | trainAcc: 97.4060% (751/771)\n",
            "0 4 Epoch: 416 | ANN: testLoss: 1.0958 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 416 | ANN: testLoss: 1.1282 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 416 | ANN: testLoss: 0.9986 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 416 | ANN: testLoss: 0.7536 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 417 | ANN: trainLoss: 0.0725 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 417 | ANN: trainLoss: 0.0802 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 417 | ANN: trainLoss: 0.0719 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 417 | ANN: trainLoss: 0.0783 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 417 | ANN: trainLoss: 0.0844 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 417 | ANN: trainLoss: 0.0773 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 417 | ANN: trainLoss: 0.0844 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 417 | ANN: trainLoss: 0.0808 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 417 | ANN: trainLoss: 0.0896 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 417 | ANN: trainLoss: 0.0867 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 417 | ANN: trainLoss: 0.0915 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 417 | ANN: trainLoss: 0.0891 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 417 | ANN: trainLoss: 0.0822 | trainAcc: 97.2763% (750/771)\n",
            "0 4 Epoch: 417 | ANN: testLoss: 0.5201 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 417 | ANN: testLoss: 0.9357 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 417 | ANN: testLoss: 0.9377 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 417 | ANN: testLoss: 0.7849 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 418 | ANN: trainLoss: 0.0489 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 418 | ANN: trainLoss: 0.0679 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 418 | ANN: trainLoss: 0.0924 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 418 | ANN: trainLoss: 0.1161 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 418 | ANN: trainLoss: 0.1167 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 418 | ANN: trainLoss: 0.1162 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 418 | ANN: trainLoss: 0.1076 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 418 | ANN: trainLoss: 0.1175 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 418 | ANN: trainLoss: 0.1129 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 418 | ANN: trainLoss: 0.1059 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 418 | ANN: trainLoss: 0.1051 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 418 | ANN: trainLoss: 0.1055 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 418 | ANN: trainLoss: 0.1700 | trainAcc: 96.1089% (741/771)\n",
            "0 4 Epoch: 418 | ANN: testLoss: 0.7790 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 418 | ANN: testLoss: 0.8449 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 418 | ANN: testLoss: 0.9666 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 418 | ANN: testLoss: 0.7249 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 419 | ANN: trainLoss: 0.0618 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 419 | ANN: trainLoss: 0.0719 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 419 | ANN: trainLoss: 0.0920 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 419 | ANN: trainLoss: 0.0883 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 419 | ANN: trainLoss: 0.1045 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 419 | ANN: trainLoss: 0.0925 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 419 | ANN: trainLoss: 0.0870 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 419 | ANN: trainLoss: 0.0888 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 419 | ANN: trainLoss: 0.0851 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 419 | ANN: trainLoss: 0.0846 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 419 | ANN: trainLoss: 0.0847 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 419 | ANN: trainLoss: 0.0862 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 419 | ANN: trainLoss: 0.0797 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 419 | ANN: testLoss: 0.8607 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 419 | ANN: testLoss: 1.0409 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 419 | ANN: testLoss: 0.9869 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 419 | ANN: testLoss: 0.8468 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 420 | ANN: trainLoss: 0.1122 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 420 | ANN: trainLoss: 0.1187 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 420 | ANN: trainLoss: 0.1182 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 420 | ANN: trainLoss: 0.1053 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 420 | ANN: trainLoss: 0.0939 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 420 | ANN: trainLoss: 0.0929 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 420 | ANN: trainLoss: 0.0844 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 420 | ANN: trainLoss: 0.0821 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 420 | ANN: trainLoss: 0.0836 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 420 | ANN: trainLoss: 0.0811 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 420 | ANN: trainLoss: 0.0859 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 420 | ANN: trainLoss: 0.0885 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 420 | ANN: trainLoss: 0.0846 | trainAcc: 96.8872% (747/771)\n",
            "0 4 Epoch: 420 | ANN: testLoss: 0.7827 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 420 | ANN: testLoss: 0.8885 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 420 | ANN: testLoss: 0.9850 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 420 | ANN: testLoss: 0.7805 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 421 | ANN: trainLoss: 0.0997 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 421 | ANN: trainLoss: 0.0968 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 421 | ANN: trainLoss: 0.1006 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 421 | ANN: trainLoss: 0.0963 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 421 | ANN: trainLoss: 0.0916 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 421 | ANN: trainLoss: 0.0846 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 421 | ANN: trainLoss: 0.0869 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 421 | ANN: trainLoss: 0.0822 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 421 | ANN: trainLoss: 0.0816 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 421 | ANN: trainLoss: 0.0890 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 421 | ANN: trainLoss: 0.0893 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 421 | ANN: trainLoss: 0.0931 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 421 | ANN: trainLoss: 0.0860 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 421 | ANN: testLoss: 0.9964 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 421 | ANN: testLoss: 0.8697 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 421 | ANN: testLoss: 0.9482 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 421 | ANN: testLoss: 0.7113 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 422 | ANN: trainLoss: 0.0792 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 422 | ANN: trainLoss: 0.1027 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 422 | ANN: trainLoss: 0.0999 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 422 | ANN: trainLoss: 0.1088 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 422 | ANN: trainLoss: 0.1119 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 422 | ANN: trainLoss: 0.1142 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 422 | ANN: trainLoss: 0.1103 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 422 | ANN: trainLoss: 0.1073 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 422 | ANN: trainLoss: 0.1013 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 422 | ANN: trainLoss: 0.0972 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 422 | ANN: trainLoss: 0.0981 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 422 | ANN: trainLoss: 0.0957 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 422 | ANN: trainLoss: 0.0896 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 422 | ANN: testLoss: 0.7309 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 422 | ANN: testLoss: 0.9055 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 422 | ANN: testLoss: 0.9735 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 422 | ANN: testLoss: 0.9857 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 423 | ANN: trainLoss: 0.0454 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 423 | ANN: trainLoss: 0.0686 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 423 | ANN: trainLoss: 0.0833 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 423 | ANN: trainLoss: 0.0833 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 423 | ANN: trainLoss: 0.0992 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 423 | ANN: trainLoss: 0.0981 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 423 | ANN: trainLoss: 0.0941 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 423 | ANN: trainLoss: 0.0987 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 423 | ANN: trainLoss: 0.0986 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 423 | ANN: trainLoss: 0.1025 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 423 | ANN: trainLoss: 0.1040 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 423 | ANN: trainLoss: 0.1010 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 423 | ANN: trainLoss: 0.0956 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 423 | ANN: testLoss: 0.9044 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 423 | ANN: testLoss: 0.8351 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 423 | ANN: testLoss: 0.9539 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 423 | ANN: testLoss: 0.7155 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 424 | ANN: trainLoss: 0.0459 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 424 | ANN: trainLoss: 0.0324 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 424 | ANN: trainLoss: 0.0473 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 424 | ANN: trainLoss: 0.0801 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 424 | ANN: trainLoss: 0.0707 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 424 | ANN: trainLoss: 0.0773 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 424 | ANN: trainLoss: 0.0813 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 424 | ANN: trainLoss: 0.0861 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 424 | ANN: trainLoss: 0.0886 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 424 | ANN: trainLoss: 0.0948 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 424 | ANN: trainLoss: 0.0990 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 424 | ANN: trainLoss: 0.0988 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 424 | ANN: trainLoss: 0.0919 | trainAcc: 96.1089% (741/771)\n",
            "0 4 Epoch: 424 | ANN: testLoss: 0.8320 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 424 | ANN: testLoss: 0.9991 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 424 | ANN: testLoss: 0.9655 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 424 | ANN: testLoss: 0.8882 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 425 | ANN: trainLoss: 0.0612 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 425 | ANN: trainLoss: 0.0908 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 425 | ANN: trainLoss: 0.1067 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 425 | ANN: trainLoss: 0.1001 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 425 | ANN: trainLoss: 0.0910 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 425 | ANN: trainLoss: 0.0826 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 425 | ANN: trainLoss: 0.0825 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 425 | ANN: trainLoss: 0.0867 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 425 | ANN: trainLoss: 0.0830 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 425 | ANN: trainLoss: 0.0783 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 425 | ANN: trainLoss: 0.0798 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 425 | ANN: trainLoss: 0.0780 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 425 | ANN: trainLoss: 0.0875 | trainAcc: 97.6654% (753/771)\n",
            "0 4 Epoch: 425 | ANN: testLoss: 0.8262 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 425 | ANN: testLoss: 1.0446 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 425 | ANN: testLoss: 0.9676 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 425 | ANN: testLoss: 0.7257 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 426 | ANN: trainLoss: 0.0833 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 426 | ANN: trainLoss: 0.0836 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 426 | ANN: trainLoss: 0.0727 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 426 | ANN: trainLoss: 0.0821 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 426 | ANN: trainLoss: 0.0919 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 426 | ANN: trainLoss: 0.0891 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 426 | ANN: trainLoss: 0.0918 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 426 | ANN: trainLoss: 0.1022 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 426 | ANN: trainLoss: 0.1012 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 426 | ANN: trainLoss: 0.0981 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 426 | ANN: trainLoss: 0.0986 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 426 | ANN: trainLoss: 0.0955 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 426 | ANN: trainLoss: 0.1087 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 426 | ANN: testLoss: 0.8550 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 426 | ANN: testLoss: 0.9854 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 426 | ANN: testLoss: 0.9568 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 426 | ANN: testLoss: 0.7238 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 427 | ANN: trainLoss: 0.0695 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 427 | ANN: trainLoss: 0.0795 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 427 | ANN: trainLoss: 0.0752 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 427 | ANN: trainLoss: 0.0673 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 427 | ANN: trainLoss: 0.0770 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 427 | ANN: trainLoss: 0.0924 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 427 | ANN: trainLoss: 0.0952 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 427 | ANN: trainLoss: 0.0948 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 427 | ANN: trainLoss: 0.0891 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 427 | ANN: trainLoss: 0.0890 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 427 | ANN: trainLoss: 0.0831 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 427 | ANN: trainLoss: 0.0838 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 427 | ANN: trainLoss: 0.2542 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 427 | ANN: testLoss: 0.9047 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 427 | ANN: testLoss: 0.8666 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 427 | ANN: testLoss: 0.9787 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 427 | ANN: testLoss: 0.7340 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 428 | ANN: trainLoss: 0.2500 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 428 | ANN: trainLoss: 0.1509 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 428 | ANN: trainLoss: 0.1180 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 428 | ANN: trainLoss: 0.1204 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 428 | ANN: trainLoss: 0.1178 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 428 | ANN: trainLoss: 0.1113 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 428 | ANN: trainLoss: 0.1115 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 428 | ANN: trainLoss: 0.1099 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 428 | ANN: trainLoss: 0.1018 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 428 | ANN: trainLoss: 0.1089 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 428 | ANN: trainLoss: 0.1102 | trainAcc: 95.8807% (675/704)\n",
            "11 13 Epoch: 428 | ANN: trainLoss: 0.1070 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 428 | ANN: trainLoss: 0.1350 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 428 | ANN: testLoss: 1.2382 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 428 | ANN: testLoss: 0.8980 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 428 | ANN: testLoss: 0.9528 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 428 | ANN: testLoss: 0.8008 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 429 | ANN: trainLoss: 0.0850 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 429 | ANN: trainLoss: 0.1051 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 429 | ANN: trainLoss: 0.0976 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 429 | ANN: trainLoss: 0.0910 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 429 | ANN: trainLoss: 0.0902 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 429 | ANN: trainLoss: 0.1008 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 429 | ANN: trainLoss: 0.0959 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 429 | ANN: trainLoss: 0.0945 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 429 | ANN: trainLoss: 0.0957 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 429 | ANN: trainLoss: 0.0947 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 429 | ANN: trainLoss: 0.0914 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 429 | ANN: trainLoss: 0.0919 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 429 | ANN: trainLoss: 0.1070 | trainAcc: 96.8872% (747/771)\n",
            "0 4 Epoch: 429 | ANN: testLoss: 1.0031 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 429 | ANN: testLoss: 0.9407 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 429 | ANN: testLoss: 0.9469 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 429 | ANN: testLoss: 0.7103 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 430 | ANN: trainLoss: 0.0369 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 430 | ANN: trainLoss: 0.0591 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 430 | ANN: trainLoss: 0.0915 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 430 | ANN: trainLoss: 0.0879 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 430 | ANN: trainLoss: 0.0937 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 430 | ANN: trainLoss: 0.0920 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 430 | ANN: trainLoss: 0.0914 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 430 | ANN: trainLoss: 0.0894 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 430 | ANN: trainLoss: 0.0844 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 430 | ANN: trainLoss: 0.0832 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 430 | ANN: trainLoss: 0.0844 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 430 | ANN: trainLoss: 0.0844 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 430 | ANN: trainLoss: 0.0783 | trainAcc: 97.7951% (754/771)\n",
            "0 4 Epoch: 430 | ANN: testLoss: 1.0814 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 430 | ANN: testLoss: 0.9687 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 430 | ANN: testLoss: 0.9558 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 430 | ANN: testLoss: 0.7191 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 431 | ANN: trainLoss: 0.0855 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 431 | ANN: trainLoss: 0.0767 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 431 | ANN: trainLoss: 0.0684 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 431 | ANN: trainLoss: 0.0809 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 431 | ANN: trainLoss: 0.0802 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 431 | ANN: trainLoss: 0.0902 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 431 | ANN: trainLoss: 0.0897 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 431 | ANN: trainLoss: 0.0868 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 431 | ANN: trainLoss: 0.0835 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 431 | ANN: trainLoss: 0.0829 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 431 | ANN: trainLoss: 0.0822 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 431 | ANN: trainLoss: 0.0822 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 431 | ANN: trainLoss: 0.1388 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 431 | ANN: testLoss: 1.0460 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 431 | ANN: testLoss: 1.0138 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 431 | ANN: testLoss: 0.9358 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 431 | ANN: testLoss: 0.7019 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 432 | ANN: trainLoss: 0.1068 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 432 | ANN: trainLoss: 0.0888 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 432 | ANN: trainLoss: 0.0957 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 432 | ANN: trainLoss: 0.1050 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 432 | ANN: trainLoss: 0.1081 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 432 | ANN: trainLoss: 0.1048 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 432 | ANN: trainLoss: 0.0961 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 432 | ANN: trainLoss: 0.0987 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 432 | ANN: trainLoss: 0.0962 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 432 | ANN: trainLoss: 0.0973 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 432 | ANN: trainLoss: 0.0953 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 432 | ANN: trainLoss: 0.0996 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 432 | ANN: trainLoss: 0.1926 | trainAcc: 95.7198% (738/771)\n",
            "0 4 Epoch: 432 | ANN: testLoss: 0.9716 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 432 | ANN: testLoss: 0.9840 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 432 | ANN: testLoss: 0.9028 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 432 | ANN: testLoss: 1.0653 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 433 | ANN: trainLoss: 0.0883 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 433 | ANN: trainLoss: 0.0770 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 433 | ANN: trainLoss: 0.0770 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 433 | ANN: trainLoss: 0.0710 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 433 | ANN: trainLoss: 0.0684 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 433 | ANN: trainLoss: 0.0631 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 433 | ANN: trainLoss: 0.0798 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 433 | ANN: trainLoss: 0.0832 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 433 | ANN: trainLoss: 0.0927 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 433 | ANN: trainLoss: 0.0923 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 433 | ANN: trainLoss: 0.0919 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 433 | ANN: trainLoss: 0.0919 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 433 | ANN: trainLoss: 0.2245 | trainAcc: 96.1089% (741/771)\n",
            "0 4 Epoch: 433 | ANN: testLoss: 0.9456 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 433 | ANN: testLoss: 1.0263 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 433 | ANN: testLoss: 0.9336 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 433 | ANN: testLoss: 1.0851 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 434 | ANN: trainLoss: 0.1526 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 434 | ANN: trainLoss: 0.1319 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 434 | ANN: trainLoss: 0.1090 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 434 | ANN: trainLoss: 0.1012 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 434 | ANN: trainLoss: 0.1020 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 434 | ANN: trainLoss: 0.0963 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 434 | ANN: trainLoss: 0.1064 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 434 | ANN: trainLoss: 0.1011 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 434 | ANN: trainLoss: 0.0934 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 434 | ANN: trainLoss: 0.0900 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 434 | ANN: trainLoss: 0.0903 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 434 | ANN: trainLoss: 0.0948 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 434 | ANN: trainLoss: 0.0879 | trainAcc: 96.2387% (742/771)\n",
            "0 4 Epoch: 434 | ANN: testLoss: 0.9925 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 434 | ANN: testLoss: 0.9854 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 434 | ANN: testLoss: 0.9677 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 434 | ANN: testLoss: 0.7258 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 435 | ANN: trainLoss: 0.1265 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 435 | ANN: trainLoss: 0.1238 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 435 | ANN: trainLoss: 0.1194 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 435 | ANN: trainLoss: 0.1052 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 435 | ANN: trainLoss: 0.0950 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 435 | ANN: trainLoss: 0.1014 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 435 | ANN: trainLoss: 0.0944 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 435 | ANN: trainLoss: 0.0944 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 435 | ANN: trainLoss: 0.0917 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 435 | ANN: trainLoss: 0.0900 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 435 | ANN: trainLoss: 0.0911 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 435 | ANN: trainLoss: 0.0961 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 435 | ANN: trainLoss: 0.3718 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 435 | ANN: testLoss: 0.9315 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 435 | ANN: testLoss: 0.9757 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 435 | ANN: testLoss: 0.9877 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 435 | ANN: testLoss: 0.8724 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 436 | ANN: trainLoss: 0.0962 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 436 | ANN: trainLoss: 0.0793 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 436 | ANN: trainLoss: 0.0672 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 436 | ANN: trainLoss: 0.0739 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 436 | ANN: trainLoss: 0.0708 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 436 | ANN: trainLoss: 0.0662 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 436 | ANN: trainLoss: 0.0702 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 436 | ANN: trainLoss: 0.0684 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 436 | ANN: trainLoss: 0.0758 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 436 | ANN: trainLoss: 0.0728 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 436 | ANN: trainLoss: 0.0755 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 436 | ANN: trainLoss: 0.0796 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 436 | ANN: trainLoss: 0.1174 | trainAcc: 97.5357% (752/771)\n",
            "0 4 Epoch: 436 | ANN: testLoss: 0.8795 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 436 | ANN: testLoss: 0.9910 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 436 | ANN: testLoss: 0.9838 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 436 | ANN: testLoss: 0.9025 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 437 | ANN: trainLoss: 0.1102 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 437 | ANN: trainLoss: 0.1127 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 437 | ANN: trainLoss: 0.0923 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 437 | ANN: trainLoss: 0.0838 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 437 | ANN: trainLoss: 0.0885 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 437 | ANN: trainLoss: 0.0873 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 437 | ANN: trainLoss: 0.0837 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 437 | ANN: trainLoss: 0.0899 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 437 | ANN: trainLoss: 0.0895 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 437 | ANN: trainLoss: 0.0865 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 437 | ANN: trainLoss: 0.0852 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 437 | ANN: trainLoss: 0.0819 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 437 | ANN: trainLoss: 0.1162 | trainAcc: 96.3684% (743/771)\n",
            "0 4 Epoch: 437 | ANN: testLoss: 0.7966 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 437 | ANN: testLoss: 0.6951 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 437 | ANN: testLoss: 0.9369 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 437 | ANN: testLoss: 0.7028 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 438 | ANN: trainLoss: 0.0961 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 438 | ANN: trainLoss: 0.0955 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 438 | ANN: trainLoss: 0.0900 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 438 | ANN: trainLoss: 0.0935 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 438 | ANN: trainLoss: 0.0946 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 438 | ANN: trainLoss: 0.0929 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 438 | ANN: trainLoss: 0.0974 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 438 | ANN: trainLoss: 0.0955 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 438 | ANN: trainLoss: 0.0938 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 438 | ANN: trainLoss: 0.0905 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 438 | ANN: trainLoss: 0.0879 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 438 | ANN: trainLoss: 0.0866 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 438 | ANN: trainLoss: 0.0803 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 438 | ANN: testLoss: 1.0741 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 438 | ANN: testLoss: 1.0969 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 438 | ANN: testLoss: 0.9676 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 438 | ANN: testLoss: 0.7257 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 439 | ANN: trainLoss: 0.0539 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 439 | ANN: trainLoss: 0.0620 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 439 | ANN: trainLoss: 0.0620 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 439 | ANN: trainLoss: 0.0862 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 439 | ANN: trainLoss: 0.0967 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 439 | ANN: trainLoss: 0.0933 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 439 | ANN: trainLoss: 0.0972 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 439 | ANN: trainLoss: 0.0950 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 439 | ANN: trainLoss: 0.0914 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 439 | ANN: trainLoss: 0.0921 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 439 | ANN: trainLoss: 0.0884 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 439 | ANN: trainLoss: 0.0878 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 439 | ANN: trainLoss: 0.0819 | trainAcc: 97.0169% (748/771)\n",
            "0 4 Epoch: 439 | ANN: testLoss: 0.9374 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 439 | ANN: testLoss: 0.8792 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 439 | ANN: testLoss: 0.9182 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 439 | ANN: testLoss: 0.6934 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 440 | ANN: trainLoss: 0.1209 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 440 | ANN: trainLoss: 0.0905 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 440 | ANN: trainLoss: 0.0990 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 440 | ANN: trainLoss: 0.1023 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 440 | ANN: trainLoss: 0.0893 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 440 | ANN: trainLoss: 0.0868 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 440 | ANN: trainLoss: 0.0918 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 440 | ANN: trainLoss: 0.0900 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 440 | ANN: trainLoss: 0.0863 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 440 | ANN: trainLoss: 0.0925 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 440 | ANN: trainLoss: 0.0934 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 440 | ANN: trainLoss: 0.0947 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 440 | ANN: trainLoss: 0.1025 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 440 | ANN: testLoss: 1.1499 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 440 | ANN: testLoss: 1.0101 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 440 | ANN: testLoss: 0.9626 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 440 | ANN: testLoss: 0.7220 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 441 | ANN: trainLoss: 0.1060 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 441 | ANN: trainLoss: 0.0867 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 441 | ANN: trainLoss: 0.0826 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 441 | ANN: trainLoss: 0.0918 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 441 | ANN: trainLoss: 0.0901 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 441 | ANN: trainLoss: 0.0880 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 441 | ANN: trainLoss: 0.0891 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 441 | ANN: trainLoss: 0.0880 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 441 | ANN: trainLoss: 0.0829 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 441 | ANN: trainLoss: 0.0817 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 441 | ANN: trainLoss: 0.0796 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 441 | ANN: trainLoss: 0.0821 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 441 | ANN: trainLoss: 0.1119 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 441 | ANN: testLoss: 1.3485 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 441 | ANN: testLoss: 1.0590 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 441 | ANN: testLoss: 0.9640 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 441 | ANN: testLoss: 0.7452 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 442 | ANN: trainLoss: 0.1219 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 442 | ANN: trainLoss: 0.0772 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 442 | ANN: trainLoss: 0.0682 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 442 | ANN: trainLoss: 0.0908 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 442 | ANN: trainLoss: 0.0867 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 442 | ANN: trainLoss: 0.0780 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 442 | ANN: trainLoss: 0.0810 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 442 | ANN: trainLoss: 0.0847 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 442 | ANN: trainLoss: 0.0867 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 442 | ANN: trainLoss: 0.0932 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 442 | ANN: trainLoss: 0.0961 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 442 | ANN: trainLoss: 0.0953 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 442 | ANN: trainLoss: 0.0897 | trainAcc: 96.1089% (741/771)\n",
            "0 4 Epoch: 442 | ANN: testLoss: 0.9702 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 442 | ANN: testLoss: 0.9815 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 442 | ANN: testLoss: 0.9538 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 442 | ANN: testLoss: 0.7216 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 443 | ANN: trainLoss: 0.0728 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 443 | ANN: trainLoss: 0.0706 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 443 | ANN: trainLoss: 0.0825 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 443 | ANN: trainLoss: 0.0785 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 443 | ANN: trainLoss: 0.0718 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 443 | ANN: trainLoss: 0.0741 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 443 | ANN: trainLoss: 0.0957 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 443 | ANN: trainLoss: 0.0905 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 443 | ANN: trainLoss: 0.0876 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 443 | ANN: trainLoss: 0.1013 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 443 | ANN: trainLoss: 0.1085 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 443 | ANN: trainLoss: 0.1061 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 443 | ANN: trainLoss: 0.0985 | trainAcc: 96.3684% (743/771)\n",
            "0 4 Epoch: 443 | ANN: testLoss: 1.1044 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 443 | ANN: testLoss: 0.9964 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 443 | ANN: testLoss: 0.9415 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 443 | ANN: testLoss: 1.8457 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 444 | ANN: trainLoss: 0.0916 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 444 | ANN: trainLoss: 0.0887 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 444 | ANN: trainLoss: 0.0704 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 444 | ANN: trainLoss: 0.0693 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 444 | ANN: trainLoss: 0.0666 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 444 | ANN: trainLoss: 0.0626 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 444 | ANN: trainLoss: 0.0657 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 444 | ANN: trainLoss: 0.0713 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 444 | ANN: trainLoss: 0.0753 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 444 | ANN: trainLoss: 0.0721 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 444 | ANN: trainLoss: 0.0766 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 444 | ANN: trainLoss: 0.0821 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 444 | ANN: trainLoss: 0.1418 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 444 | ANN: testLoss: 0.6932 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 444 | ANN: testLoss: 0.7749 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 444 | ANN: testLoss: 0.9272 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 444 | ANN: testLoss: 2.7208 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 445 | ANN: trainLoss: 0.0646 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 445 | ANN: trainLoss: 0.0634 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 445 | ANN: trainLoss: 0.0550 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 445 | ANN: trainLoss: 0.0616 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 445 | ANN: trainLoss: 0.0705 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 445 | ANN: trainLoss: 0.0747 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 445 | ANN: trainLoss: 0.0761 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 445 | ANN: trainLoss: 0.0731 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 445 | ANN: trainLoss: 0.0766 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 445 | ANN: trainLoss: 0.0907 | trainAcc: 95.6250% (612/640)\n",
            "10 13 Epoch: 445 | ANN: trainLoss: 0.0901 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 445 | ANN: trainLoss: 0.0900 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 445 | ANN: trainLoss: 0.1084 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 445 | ANN: testLoss: 1.2459 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 445 | ANN: testLoss: 1.0322 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 445 | ANN: testLoss: 0.9341 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 445 | ANN: testLoss: 0.7006 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 446 | ANN: trainLoss: 0.1621 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 446 | ANN: trainLoss: 0.1350 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 446 | ANN: trainLoss: 0.1145 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 446 | ANN: trainLoss: 0.1018 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 446 | ANN: trainLoss: 0.0976 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 446 | ANN: trainLoss: 0.0930 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 446 | ANN: trainLoss: 0.1040 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 446 | ANN: trainLoss: 0.1006 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 446 | ANN: trainLoss: 0.0982 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 446 | ANN: trainLoss: 0.1077 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 446 | ANN: trainLoss: 0.1070 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 446 | ANN: trainLoss: 0.1027 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 446 | ANN: trainLoss: 0.1059 | trainAcc: 96.8872% (747/771)\n",
            "0 4 Epoch: 446 | ANN: testLoss: 1.0927 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 446 | ANN: testLoss: 1.0101 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 446 | ANN: testLoss: 0.9772 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 446 | ANN: testLoss: 1.3897 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 447 | ANN: trainLoss: 0.0621 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 447 | ANN: trainLoss: 0.0878 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 447 | ANN: trainLoss: 0.0777 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 447 | ANN: trainLoss: 0.0976 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 447 | ANN: trainLoss: 0.0856 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 447 | ANN: trainLoss: 0.0880 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 447 | ANN: trainLoss: 0.0877 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 447 | ANN: trainLoss: 0.0875 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 447 | ANN: trainLoss: 0.0911 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 447 | ANN: trainLoss: 0.0858 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 447 | ANN: trainLoss: 0.0869 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 447 | ANN: trainLoss: 0.0862 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 447 | ANN: trainLoss: 0.0831 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 447 | ANN: testLoss: 0.7395 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 447 | ANN: testLoss: 0.8273 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 447 | ANN: testLoss: 0.9712 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 447 | ANN: testLoss: 1.4151 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 448 | ANN: trainLoss: 0.0304 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 448 | ANN: trainLoss: 0.0511 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 448 | ANN: trainLoss: 0.0488 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 448 | ANN: trainLoss: 0.0650 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 448 | ANN: trainLoss: 0.0686 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 448 | ANN: trainLoss: 0.0759 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 448 | ANN: trainLoss: 0.0723 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 448 | ANN: trainLoss: 0.0711 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 448 | ANN: trainLoss: 0.0689 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 448 | ANN: trainLoss: 0.0688 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 448 | ANN: trainLoss: 0.0722 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 448 | ANN: trainLoss: 0.0734 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 448 | ANN: trainLoss: 0.0939 | trainAcc: 97.5357% (752/771)\n",
            "0 4 Epoch: 448 | ANN: testLoss: 0.6882 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 448 | ANN: testLoss: 0.8844 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 448 | ANN: testLoss: 1.0253 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 448 | ANN: testLoss: 0.7690 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 449 | ANN: trainLoss: 0.0481 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 449 | ANN: trainLoss: 0.0879 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 449 | ANN: trainLoss: 0.0932 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 449 | ANN: trainLoss: 0.0816 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 449 | ANN: trainLoss: 0.0925 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 449 | ANN: trainLoss: 0.0847 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 449 | ANN: trainLoss: 0.0878 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 449 | ANN: trainLoss: 0.0885 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 449 | ANN: trainLoss: 0.0869 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 449 | ANN: trainLoss: 0.0928 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 449 | ANN: trainLoss: 0.0893 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 449 | ANN: trainLoss: 0.0906 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 449 | ANN: trainLoss: 0.1287 | trainAcc: 97.0169% (748/771)\n",
            "0 4 Epoch: 449 | ANN: testLoss: 0.6176 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 449 | ANN: testLoss: 0.8923 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 449 | ANN: testLoss: 0.9448 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 449 | ANN: testLoss: 0.7218 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 450 | ANN: trainLoss: 0.1333 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 450 | ANN: trainLoss: 0.1194 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 450 | ANN: trainLoss: 0.1000 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 450 | ANN: trainLoss: 0.0959 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 450 | ANN: trainLoss: 0.0886 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 450 | ANN: trainLoss: 0.0884 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 450 | ANN: trainLoss: 0.0828 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 450 | ANN: trainLoss: 0.0827 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 450 | ANN: trainLoss: 0.0770 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 450 | ANN: trainLoss: 0.0798 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 450 | ANN: trainLoss: 0.0880 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 450 | ANN: trainLoss: 0.0896 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 450 | ANN: trainLoss: 0.3273 | trainAcc: 96.6278% (745/771)\n",
            "0 4 Epoch: 450 | ANN: testLoss: 1.3195 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 450 | ANN: testLoss: 1.2650 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 450 | ANN: testLoss: 0.9834 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 450 | ANN: testLoss: 1.1687 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 451 | ANN: trainLoss: 0.0419 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 451 | ANN: trainLoss: 0.0431 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 451 | ANN: trainLoss: 0.0562 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 451 | ANN: trainLoss: 0.0642 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 451 | ANN: trainLoss: 0.0637 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 451 | ANN: trainLoss: 0.0735 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 451 | ANN: trainLoss: 0.0706 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 451 | ANN: trainLoss: 0.0677 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 451 | ANN: trainLoss: 0.0697 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 451 | ANN: trainLoss: 0.0737 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 451 | ANN: trainLoss: 0.0698 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 451 | ANN: trainLoss: 0.0700 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 451 | ANN: trainLoss: 0.0667 | trainAcc: 98.3139% (758/771)\n",
            "0 4 Epoch: 451 | ANN: testLoss: 1.0026 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 451 | ANN: testLoss: 1.0495 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 451 | ANN: testLoss: 0.9636 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 451 | ANN: testLoss: 0.7227 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 452 | ANN: trainLoss: 0.0894 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 452 | ANN: trainLoss: 0.0844 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 452 | ANN: trainLoss: 0.0855 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 452 | ANN: trainLoss: 0.0773 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 452 | ANN: trainLoss: 0.0730 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 452 | ANN: trainLoss: 0.0935 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 452 | ANN: trainLoss: 0.0899 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 452 | ANN: trainLoss: 0.0862 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 452 | ANN: trainLoss: 0.0823 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 452 | ANN: trainLoss: 0.0848 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 452 | ANN: trainLoss: 0.0860 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 452 | ANN: trainLoss: 0.0858 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 452 | ANN: trainLoss: 0.0826 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 452 | ANN: testLoss: 1.0483 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 452 | ANN: testLoss: 1.0494 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 452 | ANN: testLoss: 0.9769 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 452 | ANN: testLoss: 0.7719 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 453 | ANN: trainLoss: 0.0710 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 453 | ANN: trainLoss: 0.1177 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 453 | ANN: trainLoss: 0.1346 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 453 | ANN: trainLoss: 0.1226 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 453 | ANN: trainLoss: 0.1162 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 453 | ANN: trainLoss: 0.1225 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 453 | ANN: trainLoss: 0.1158 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 453 | ANN: trainLoss: 0.1162 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 453 | ANN: trainLoss: 0.1082 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 453 | ANN: trainLoss: 0.1151 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 453 | ANN: trainLoss: 0.1130 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 453 | ANN: trainLoss: 0.1082 | trainAcc: 95.5729% (734/768)\n",
            "12 13 Epoch: 453 | ANN: trainLoss: 0.1021 | trainAcc: 95.5901% (737/771)\n",
            "0 4 Epoch: 453 | ANN: testLoss: 0.8465 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 453 | ANN: testLoss: 1.0608 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 453 | ANN: testLoss: 0.9974 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 453 | ANN: testLoss: 0.7482 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 454 | ANN: trainLoss: 0.0678 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 454 | ANN: trainLoss: 0.1025 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 454 | ANN: trainLoss: 0.0880 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 454 | ANN: trainLoss: 0.0930 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 454 | ANN: trainLoss: 0.0922 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 454 | ANN: trainLoss: 0.0952 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 454 | ANN: trainLoss: 0.0930 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 454 | ANN: trainLoss: 0.0961 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 454 | ANN: trainLoss: 0.0942 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 454 | ANN: trainLoss: 0.1056 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 454 | ANN: trainLoss: 0.1017 | trainAcc: 95.8807% (675/704)\n",
            "11 13 Epoch: 454 | ANN: trainLoss: 0.1039 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 454 | ANN: trainLoss: 0.1707 | trainAcc: 95.7198% (738/771)\n",
            "0 4 Epoch: 454 | ANN: testLoss: 0.7026 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 454 | ANN: testLoss: 0.8754 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 454 | ANN: testLoss: 0.9807 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 454 | ANN: testLoss: 0.7357 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 455 | ANN: trainLoss: 0.0619 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 455 | ANN: trainLoss: 0.0612 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 455 | ANN: trainLoss: 0.0604 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 455 | ANN: trainLoss: 0.0713 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 455 | ANN: trainLoss: 0.0662 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 455 | ANN: trainLoss: 0.0640 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 455 | ANN: trainLoss: 0.0693 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 455 | ANN: trainLoss: 0.0701 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 455 | ANN: trainLoss: 0.0791 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 455 | ANN: trainLoss: 0.0800 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 455 | ANN: trainLoss: 0.0869 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 455 | ANN: trainLoss: 0.0837 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 455 | ANN: trainLoss: 0.0794 | trainAcc: 97.2763% (750/771)\n",
            "0 4 Epoch: 455 | ANN: testLoss: 1.0199 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 455 | ANN: testLoss: 0.9492 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 455 | ANN: testLoss: 0.9858 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 455 | ANN: testLoss: 0.7394 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 456 | ANN: trainLoss: 0.1127 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 456 | ANN: trainLoss: 0.1200 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 456 | ANN: trainLoss: 0.0991 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 456 | ANN: trainLoss: 0.0951 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 456 | ANN: trainLoss: 0.0954 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 456 | ANN: trainLoss: 0.0940 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 456 | ANN: trainLoss: 0.0917 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 456 | ANN: trainLoss: 0.0906 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 456 | ANN: trainLoss: 0.0965 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 456 | ANN: trainLoss: 0.0967 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 456 | ANN: trainLoss: 0.0926 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 456 | ANN: trainLoss: 0.0908 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 456 | ANN: trainLoss: 0.0853 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 456 | ANN: testLoss: 0.8118 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 456 | ANN: testLoss: 1.0036 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 456 | ANN: testLoss: 0.9281 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 456 | ANN: testLoss: 0.6961 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 457 | ANN: trainLoss: 0.1676 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 457 | ANN: trainLoss: 0.1222 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 457 | ANN: trainLoss: 0.1086 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 457 | ANN: trainLoss: 0.1172 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 457 | ANN: trainLoss: 0.1219 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 457 | ANN: trainLoss: 0.1153 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 457 | ANN: trainLoss: 0.1199 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 457 | ANN: trainLoss: 0.1249 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 457 | ANN: trainLoss: 0.1227 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 457 | ANN: trainLoss: 0.1273 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 457 | ANN: trainLoss: 0.1246 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 457 | ANN: trainLoss: 0.1207 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 457 | ANN: trainLoss: 0.1170 | trainAcc: 95.4604% (736/771)\n",
            "0 4 Epoch: 457 | ANN: testLoss: 0.7972 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 457 | ANN: testLoss: 1.0954 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 457 | ANN: testLoss: 0.9674 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 457 | ANN: testLoss: 0.7309 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 458 | ANN: trainLoss: 0.1131 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 458 | ANN: trainLoss: 0.0876 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 458 | ANN: trainLoss: 0.0969 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 458 | ANN: trainLoss: 0.0909 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 458 | ANN: trainLoss: 0.0797 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 458 | ANN: trainLoss: 0.0799 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 458 | ANN: trainLoss: 0.0769 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 458 | ANN: trainLoss: 0.0761 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 458 | ANN: trainLoss: 0.0762 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 458 | ANN: trainLoss: 0.0785 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 458 | ANN: trainLoss: 0.0793 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 458 | ANN: trainLoss: 0.0776 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 458 | ANN: trainLoss: 0.0728 | trainAcc: 97.9248% (755/771)\n",
            "0 4 Epoch: 458 | ANN: testLoss: 1.1113 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 458 | ANN: testLoss: 0.9912 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 458 | ANN: testLoss: 0.9722 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 458 | ANN: testLoss: 0.7292 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 459 | ANN: trainLoss: 0.1165 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 459 | ANN: trainLoss: 0.1142 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 459 | ANN: trainLoss: 0.1043 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 459 | ANN: trainLoss: 0.1161 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 459 | ANN: trainLoss: 0.1092 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 459 | ANN: trainLoss: 0.1096 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 459 | ANN: trainLoss: 0.0977 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 459 | ANN: trainLoss: 0.0951 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 459 | ANN: trainLoss: 0.0920 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 459 | ANN: trainLoss: 0.0917 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 459 | ANN: trainLoss: 0.0920 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 459 | ANN: trainLoss: 0.0905 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 459 | ANN: trainLoss: 0.0857 | trainAcc: 97.2763% (750/771)\n",
            "0 4 Epoch: 459 | ANN: testLoss: 0.8669 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 459 | ANN: testLoss: 0.9028 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 459 | ANN: testLoss: 0.9936 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 459 | ANN: testLoss: 0.9353 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 460 | ANN: trainLoss: 0.0579 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 460 | ANN: trainLoss: 0.0671 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 460 | ANN: trainLoss: 0.0765 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 460 | ANN: trainLoss: 0.0840 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 460 | ANN: trainLoss: 0.0866 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 460 | ANN: trainLoss: 0.0759 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 460 | ANN: trainLoss: 0.0784 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 460 | ANN: trainLoss: 0.0847 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 460 | ANN: trainLoss: 0.0812 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 460 | ANN: trainLoss: 0.0789 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 460 | ANN: trainLoss: 0.0784 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 460 | ANN: trainLoss: 0.0740 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 460 | ANN: trainLoss: 0.0687 | trainAcc: 97.4060% (751/771)\n",
            "0 4 Epoch: 460 | ANN: testLoss: 1.2499 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 460 | ANN: testLoss: 0.9654 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 460 | ANN: testLoss: 0.9422 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 460 | ANN: testLoss: 0.7067 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 461 | ANN: trainLoss: 0.0621 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 461 | ANN: trainLoss: 0.0921 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 461 | ANN: trainLoss: 0.0767 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 461 | ANN: trainLoss: 0.0676 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 461 | ANN: trainLoss: 0.0706 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 461 | ANN: trainLoss: 0.0703 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 461 | ANN: trainLoss: 0.0789 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 461 | ANN: trainLoss: 0.0787 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 461 | ANN: trainLoss: 0.0749 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 461 | ANN: trainLoss: 0.0738 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 461 | ANN: trainLoss: 0.0802 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 461 | ANN: trainLoss: 0.0804 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 461 | ANN: trainLoss: 0.0806 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 461 | ANN: testLoss: 0.8776 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 461 | ANN: testLoss: 0.7215 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 461 | ANN: testLoss: 0.9329 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 461 | ANN: testLoss: 1.2489 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 462 | ANN: trainLoss: 0.0578 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 462 | ANN: trainLoss: 0.0664 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 462 | ANN: trainLoss: 0.0806 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 462 | ANN: trainLoss: 0.0954 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 462 | ANN: trainLoss: 0.0898 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 462 | ANN: trainLoss: 0.0831 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 462 | ANN: trainLoss: 0.0750 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 462 | ANN: trainLoss: 0.0802 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 462 | ANN: trainLoss: 0.0822 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 462 | ANN: trainLoss: 0.0784 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 462 | ANN: trainLoss: 0.0780 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 462 | ANN: trainLoss: 0.0786 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 462 | ANN: trainLoss: 0.1091 | trainAcc: 97.5357% (752/771)\n",
            "0 4 Epoch: 462 | ANN: testLoss: 1.1638 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 462 | ANN: testLoss: 0.9329 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 462 | ANN: testLoss: 0.9785 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 462 | ANN: testLoss: 0.7339 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 463 | ANN: trainLoss: 0.0787 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 463 | ANN: trainLoss: 0.0875 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 463 | ANN: trainLoss: 0.0942 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 463 | ANN: trainLoss: 0.0891 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 463 | ANN: trainLoss: 0.0900 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 463 | ANN: trainLoss: 0.0899 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 463 | ANN: trainLoss: 0.0915 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 463 | ANN: trainLoss: 0.0891 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 463 | ANN: trainLoss: 0.0853 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 463 | ANN: trainLoss: 0.0828 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 463 | ANN: trainLoss: 0.0849 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 463 | ANN: trainLoss: 0.0896 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 463 | ANN: trainLoss: 0.1063 | trainAcc: 97.2763% (750/771)\n",
            "0 4 Epoch: 463 | ANN: testLoss: 0.8501 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 463 | ANN: testLoss: 0.9555 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 463 | ANN: testLoss: 1.0064 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 463 | ANN: testLoss: 0.7634 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 464 | ANN: trainLoss: 0.0571 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 464 | ANN: trainLoss: 0.0509 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 464 | ANN: trainLoss: 0.0551 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 464 | ANN: trainLoss: 0.0637 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 464 | ANN: trainLoss: 0.0643 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 464 | ANN: trainLoss: 0.0713 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 464 | ANN: trainLoss: 0.0703 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 464 | ANN: trainLoss: 0.0725 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 464 | ANN: trainLoss: 0.0688 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 464 | ANN: trainLoss: 0.0715 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 464 | ANN: trainLoss: 0.0703 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 464 | ANN: trainLoss: 0.0707 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 464 | ANN: trainLoss: 0.2235 | trainAcc: 97.7951% (754/771)\n",
            "0 4 Epoch: 464 | ANN: testLoss: 0.9953 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 464 | ANN: testLoss: 0.9733 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 464 | ANN: testLoss: 0.9581 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 464 | ANN: testLoss: 0.7187 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 465 | ANN: trainLoss: 0.0815 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 465 | ANN: trainLoss: 0.0970 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 465 | ANN: trainLoss: 0.0892 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 465 | ANN: trainLoss: 0.0856 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 465 | ANN: trainLoss: 0.0991 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 465 | ANN: trainLoss: 0.0982 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 465 | ANN: trainLoss: 0.1068 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 465 | ANN: trainLoss: 0.1041 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 465 | ANN: trainLoss: 0.0973 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 465 | ANN: trainLoss: 0.0974 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 465 | ANN: trainLoss: 0.0980 | trainAcc: 95.8807% (675/704)\n",
            "11 13 Epoch: 465 | ANN: trainLoss: 0.0983 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 465 | ANN: trainLoss: 0.1125 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 465 | ANN: testLoss: 1.1264 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 465 | ANN: testLoss: 1.0541 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 465 | ANN: testLoss: 0.9728 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 465 | ANN: testLoss: 1.1239 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 466 | ANN: trainLoss: 0.1640 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 466 | ANN: trainLoss: 0.1247 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 466 | ANN: trainLoss: 0.1037 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 466 | ANN: trainLoss: 0.0918 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 466 | ANN: trainLoss: 0.0913 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 466 | ANN: trainLoss: 0.0838 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 466 | ANN: trainLoss: 0.0838 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 466 | ANN: trainLoss: 0.0797 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 466 | ANN: trainLoss: 0.0855 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 466 | ANN: trainLoss: 0.0869 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 466 | ANN: trainLoss: 0.0858 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 466 | ANN: trainLoss: 0.0839 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 466 | ANN: trainLoss: 0.0816 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 466 | ANN: testLoss: 1.1022 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 466 | ANN: testLoss: 1.1179 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 466 | ANN: testLoss: 0.9882 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 466 | ANN: testLoss: 0.7412 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 467 | ANN: trainLoss: 0.0862 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 467 | ANN: trainLoss: 0.0942 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 467 | ANN: trainLoss: 0.0939 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 467 | ANN: trainLoss: 0.0926 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 467 | ANN: trainLoss: 0.0972 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 467 | ANN: trainLoss: 0.0996 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 467 | ANN: trainLoss: 0.0986 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 467 | ANN: trainLoss: 0.0986 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 467 | ANN: trainLoss: 0.0997 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 467 | ANN: trainLoss: 0.0926 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 467 | ANN: trainLoss: 0.0913 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 467 | ANN: trainLoss: 0.0880 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 467 | ANN: trainLoss: 0.0861 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 467 | ANN: testLoss: 0.8022 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 467 | ANN: testLoss: 0.9443 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 467 | ANN: testLoss: 1.0042 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 467 | ANN: testLoss: 0.7531 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 468 | ANN: trainLoss: 0.0803 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 468 | ANN: trainLoss: 0.0597 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 468 | ANN: trainLoss: 0.0651 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 468 | ANN: trainLoss: 0.0584 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 468 | ANN: trainLoss: 0.0585 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 468 | ANN: trainLoss: 0.0649 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 468 | ANN: trainLoss: 0.0692 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 468 | ANN: trainLoss: 0.0715 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 468 | ANN: trainLoss: 0.0705 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 468 | ANN: trainLoss: 0.0668 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 468 | ANN: trainLoss: 0.0719 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 468 | ANN: trainLoss: 0.0736 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 468 | ANN: trainLoss: 0.1884 | trainAcc: 97.2763% (750/771)\n",
            "0 4 Epoch: 468 | ANN: testLoss: 0.7128 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 468 | ANN: testLoss: 0.7668 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 468 | ANN: testLoss: 0.9089 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 468 | ANN: testLoss: 0.6817 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 469 | ANN: trainLoss: 0.0681 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 469 | ANN: trainLoss: 0.0607 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 469 | ANN: trainLoss: 0.0820 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 469 | ANN: trainLoss: 0.0765 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 469 | ANN: trainLoss: 0.0757 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 469 | ANN: trainLoss: 0.0767 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 469 | ANN: trainLoss: 0.0741 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 469 | ANN: trainLoss: 0.0697 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 469 | ANN: trainLoss: 0.0658 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 469 | ANN: trainLoss: 0.0701 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 469 | ANN: trainLoss: 0.0677 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 469 | ANN: trainLoss: 0.0687 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 469 | ANN: trainLoss: 0.0659 | trainAcc: 97.0169% (748/771)\n",
            "0 4 Epoch: 469 | ANN: testLoss: 0.9106 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 469 | ANN: testLoss: 0.9143 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 469 | ANN: testLoss: 0.9285 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 469 | ANN: testLoss: 2.2636 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 470 | ANN: trainLoss: 0.0487 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 470 | ANN: trainLoss: 0.0741 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 470 | ANN: trainLoss: 0.0709 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 470 | ANN: trainLoss: 0.0793 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 470 | ANN: trainLoss: 0.0829 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 470 | ANN: trainLoss: 0.0816 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 470 | ANN: trainLoss: 0.0803 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 470 | ANN: trainLoss: 0.0768 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 470 | ANN: trainLoss: 0.0790 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 470 | ANN: trainLoss: 0.0829 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 470 | ANN: trainLoss: 0.0837 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 470 | ANN: trainLoss: 0.0852 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 470 | ANN: trainLoss: 0.0797 | trainAcc: 97.6654% (753/771)\n",
            "0 4 Epoch: 470 | ANN: testLoss: 1.1552 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 470 | ANN: testLoss: 0.9309 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 470 | ANN: testLoss: 0.9620 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 470 | ANN: testLoss: 0.7296 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 471 | ANN: trainLoss: 0.0580 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 471 | ANN: trainLoss: 0.0787 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 471 | ANN: trainLoss: 0.0726 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 471 | ANN: trainLoss: 0.0841 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 471 | ANN: trainLoss: 0.0930 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 471 | ANN: trainLoss: 0.0918 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 471 | ANN: trainLoss: 0.0946 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 471 | ANN: trainLoss: 0.0941 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 471 | ANN: trainLoss: 0.0952 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 471 | ANN: trainLoss: 0.0939 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 471 | ANN: trainLoss: 0.0911 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 471 | ANN: trainLoss: 0.0907 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 471 | ANN: trainLoss: 0.0875 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 471 | ANN: testLoss: 0.9697 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 471 | ANN: testLoss: 0.9571 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 471 | ANN: testLoss: 0.9562 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 471 | ANN: testLoss: 1.1099 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 472 | ANN: trainLoss: 0.0726 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 472 | ANN: trainLoss: 0.0629 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 472 | ANN: trainLoss: 0.0635 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 472 | ANN: trainLoss: 0.0692 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 472 | ANN: trainLoss: 0.0802 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 472 | ANN: trainLoss: 0.0795 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 472 | ANN: trainLoss: 0.0788 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 472 | ANN: trainLoss: 0.0756 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 472 | ANN: trainLoss: 0.0784 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 472 | ANN: trainLoss: 0.0767 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 472 | ANN: trainLoss: 0.0767 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 472 | ANN: trainLoss: 0.0771 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 472 | ANN: trainLoss: 0.0712 | trainAcc: 97.7951% (754/771)\n",
            "0 4 Epoch: 472 | ANN: testLoss: 1.0035 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 472 | ANN: testLoss: 0.8773 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 472 | ANN: testLoss: 0.9538 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 472 | ANN: testLoss: 0.9015 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 473 | ANN: trainLoss: 0.0648 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 473 | ANN: trainLoss: 0.0650 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 473 | ANN: trainLoss: 0.0695 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 473 | ANN: trainLoss: 0.0651 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 473 | ANN: trainLoss: 0.0753 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 473 | ANN: trainLoss: 0.0754 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 473 | ANN: trainLoss: 0.0789 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 473 | ANN: trainLoss: 0.0817 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 473 | ANN: trainLoss: 0.0844 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 473 | ANN: trainLoss: 0.0836 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 473 | ANN: trainLoss: 0.0888 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 473 | ANN: trainLoss: 0.0862 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 473 | ANN: trainLoss: 0.0812 | trainAcc: 95.9792% (740/771)\n",
            "0 4 Epoch: 473 | ANN: testLoss: 0.8057 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 473 | ANN: testLoss: 1.0158 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 473 | ANN: testLoss: 1.0070 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 473 | ANN: testLoss: 0.7605 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 474 | ANN: trainLoss: 0.0972 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 474 | ANN: trainLoss: 0.0737 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 474 | ANN: trainLoss: 0.0591 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 474 | ANN: trainLoss: 0.0617 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 474 | ANN: trainLoss: 0.0645 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 474 | ANN: trainLoss: 0.0578 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 474 | ANN: trainLoss: 0.0579 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 474 | ANN: trainLoss: 0.0584 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 474 | ANN: trainLoss: 0.0637 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 474 | ANN: trainLoss: 0.0646 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 474 | ANN: trainLoss: 0.0696 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 474 | ANN: trainLoss: 0.0748 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 474 | ANN: trainLoss: 0.1211 | trainAcc: 97.7951% (754/771)\n",
            "0 4 Epoch: 474 | ANN: testLoss: 0.7938 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 474 | ANN: testLoss: 0.8845 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 474 | ANN: testLoss: 0.9201 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 474 | ANN: testLoss: 2.0193 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 475 | ANN: trainLoss: 0.0786 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 475 | ANN: trainLoss: 0.0628 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 475 | ANN: trainLoss: 0.0623 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 475 | ANN: trainLoss: 0.0746 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 475 | ANN: trainLoss: 0.0734 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 475 | ANN: trainLoss: 0.0782 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 475 | ANN: trainLoss: 0.0718 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 475 | ANN: trainLoss: 0.0710 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 475 | ANN: trainLoss: 0.0730 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 475 | ANN: trainLoss: 0.0769 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 475 | ANN: trainLoss: 0.0767 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 475 | ANN: trainLoss: 0.0853 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 475 | ANN: trainLoss: 0.0791 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 475 | ANN: testLoss: 1.1445 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 475 | ANN: testLoss: 0.9752 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 475 | ANN: testLoss: 0.9537 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 475 | ANN: testLoss: 0.7153 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 476 | ANN: trainLoss: 0.0607 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 476 | ANN: trainLoss: 0.0800 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 476 | ANN: trainLoss: 0.0900 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 476 | ANN: trainLoss: 0.0860 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 476 | ANN: trainLoss: 0.0814 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 476 | ANN: trainLoss: 0.0726 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 476 | ANN: trainLoss: 0.0723 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 476 | ANN: trainLoss: 0.0785 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 476 | ANN: trainLoss: 0.0860 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 476 | ANN: trainLoss: 0.0867 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 476 | ANN: trainLoss: 0.0863 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 476 | ANN: trainLoss: 0.0832 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 476 | ANN: trainLoss: 0.1596 | trainAcc: 96.6278% (745/771)\n",
            "0 4 Epoch: 476 | ANN: testLoss: 1.1136 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 476 | ANN: testLoss: 0.9630 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 476 | ANN: testLoss: 0.9751 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 476 | ANN: testLoss: 0.7968 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 477 | ANN: trainLoss: 0.0432 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 477 | ANN: trainLoss: 0.0601 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 477 | ANN: trainLoss: 0.0728 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 477 | ANN: trainLoss: 0.0697 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 477 | ANN: trainLoss: 0.0677 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 477 | ANN: trainLoss: 0.0778 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 477 | ANN: trainLoss: 0.0774 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 477 | ANN: trainLoss: 0.0755 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 477 | ANN: trainLoss: 0.0755 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 477 | ANN: trainLoss: 0.0704 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 477 | ANN: trainLoss: 0.0776 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 477 | ANN: trainLoss: 0.0812 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 477 | ANN: trainLoss: 0.0751 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 477 | ANN: testLoss: 0.9908 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 477 | ANN: testLoss: 0.9460 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 477 | ANN: testLoss: 0.9674 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 477 | ANN: testLoss: 1.2613 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 478 | ANN: trainLoss: 0.1169 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 478 | ANN: trainLoss: 0.1548 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 478 | ANN: trainLoss: 0.1245 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 478 | ANN: trainLoss: 0.1282 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 478 | ANN: trainLoss: 0.1152 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 478 | ANN: trainLoss: 0.1214 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 478 | ANN: trainLoss: 0.1174 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 478 | ANN: trainLoss: 0.1115 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 478 | ANN: trainLoss: 0.1068 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 478 | ANN: trainLoss: 0.1045 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 478 | ANN: trainLoss: 0.1057 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 478 | ANN: trainLoss: 0.1003 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 478 | ANN: trainLoss: 0.0960 | trainAcc: 95.7198% (738/771)\n",
            "0 4 Epoch: 478 | ANN: testLoss: 0.6337 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 478 | ANN: testLoss: 0.8832 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 478 | ANN: testLoss: 1.0114 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 478 | ANN: testLoss: 0.7588 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 479 | ANN: trainLoss: 0.0740 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 479 | ANN: trainLoss: 0.0716 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 479 | ANN: trainLoss: 0.0814 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 479 | ANN: trainLoss: 0.0865 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 479 | ANN: trainLoss: 0.0906 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 479 | ANN: trainLoss: 0.0951 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 479 | ANN: trainLoss: 0.0911 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 479 | ANN: trainLoss: 0.0897 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 479 | ANN: trainLoss: 0.0920 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 479 | ANN: trainLoss: 0.0891 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 479 | ANN: trainLoss: 0.0901 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 479 | ANN: trainLoss: 0.0869 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 479 | ANN: trainLoss: 0.0840 | trainAcc: 96.8872% (747/771)\n",
            "0 4 Epoch: 479 | ANN: testLoss: 1.0202 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 479 | ANN: testLoss: 0.9390 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 479 | ANN: testLoss: 0.9806 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 479 | ANN: testLoss: 1.0647 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 480 | ANN: trainLoss: 0.1014 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 480 | ANN: trainLoss: 0.0876 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 480 | ANN: trainLoss: 0.0853 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 480 | ANN: trainLoss: 0.0889 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 480 | ANN: trainLoss: 0.0875 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 480 | ANN: trainLoss: 0.0845 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 480 | ANN: trainLoss: 0.0801 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 480 | ANN: trainLoss: 0.0784 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 480 | ANN: trainLoss: 0.0759 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 480 | ANN: trainLoss: 0.0729 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 480 | ANN: trainLoss: 0.0772 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 480 | ANN: trainLoss: 0.0799 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 480 | ANN: trainLoss: 0.3880 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 480 | ANN: testLoss: 0.9017 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 480 | ANN: testLoss: 0.8984 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 480 | ANN: testLoss: 0.9781 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 480 | ANN: testLoss: 1.3440 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 481 | ANN: trainLoss: 0.0419 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 481 | ANN: trainLoss: 0.0420 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 481 | ANN: trainLoss: 0.0769 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 481 | ANN: trainLoss: 0.0828 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 481 | ANN: trainLoss: 0.0796 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 481 | ANN: trainLoss: 0.0815 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 481 | ANN: trainLoss: 0.0886 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 481 | ANN: trainLoss: 0.0924 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 481 | ANN: trainLoss: 0.0901 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 481 | ANN: trainLoss: 0.0877 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 481 | ANN: trainLoss: 0.0927 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 481 | ANN: trainLoss: 0.0893 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 481 | ANN: trainLoss: 0.1146 | trainAcc: 96.8872% (747/771)\n",
            "0 4 Epoch: 481 | ANN: testLoss: 0.7700 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 481 | ANN: testLoss: 0.9070 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 481 | ANN: testLoss: 0.9983 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 481 | ANN: testLoss: 0.7487 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 482 | ANN: trainLoss: 0.0732 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 482 | ANN: trainLoss: 0.0841 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 482 | ANN: trainLoss: 0.0810 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 482 | ANN: trainLoss: 0.0776 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 482 | ANN: trainLoss: 0.0841 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 482 | ANN: trainLoss: 0.0822 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 482 | ANN: trainLoss: 0.0854 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 482 | ANN: trainLoss: 0.0823 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 482 | ANN: trainLoss: 0.0842 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 482 | ANN: trainLoss: 0.0870 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 482 | ANN: trainLoss: 0.0894 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 482 | ANN: trainLoss: 0.0978 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 482 | ANN: trainLoss: 0.1423 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 482 | ANN: testLoss: 0.9231 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 482 | ANN: testLoss: 0.8489 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 482 | ANN: testLoss: 0.9825 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 482 | ANN: testLoss: 0.7371 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 483 | ANN: trainLoss: 0.0456 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 483 | ANN: trainLoss: 0.0523 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 483 | ANN: trainLoss: 0.0619 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 483 | ANN: trainLoss: 0.0853 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 483 | ANN: trainLoss: 0.0841 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 483 | ANN: trainLoss: 0.0842 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 483 | ANN: trainLoss: 0.0864 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 483 | ANN: trainLoss: 0.0806 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 483 | ANN: trainLoss: 0.0792 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 483 | ANN: trainLoss: 0.0802 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 483 | ANN: trainLoss: 0.0768 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 483 | ANN: trainLoss: 0.0797 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 483 | ANN: trainLoss: 0.0873 | trainAcc: 97.4060% (751/771)\n",
            "0 4 Epoch: 483 | ANN: testLoss: 1.3300 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 483 | ANN: testLoss: 1.0929 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 483 | ANN: testLoss: 0.9535 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 483 | ANN: testLoss: 0.7151 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 484 | ANN: trainLoss: 0.0803 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 484 | ANN: trainLoss: 0.1020 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 484 | ANN: trainLoss: 0.0924 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 484 | ANN: trainLoss: 0.0847 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 484 | ANN: trainLoss: 0.0792 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 484 | ANN: trainLoss: 0.0787 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 484 | ANN: trainLoss: 0.0736 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 484 | ANN: trainLoss: 0.0715 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 484 | ANN: trainLoss: 0.0684 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 484 | ANN: trainLoss: 0.0651 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 484 | ANN: trainLoss: 0.0689 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 484 | ANN: trainLoss: 0.0741 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 484 | ANN: trainLoss: 0.0721 | trainAcc: 97.7951% (754/771)\n",
            "0 4 Epoch: 484 | ANN: testLoss: 0.9284 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 484 | ANN: testLoss: 0.9522 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 484 | ANN: testLoss: 0.9982 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 484 | ANN: testLoss: 0.7554 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 485 | ANN: trainLoss: 0.0643 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 485 | ANN: trainLoss: 0.0801 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 485 | ANN: trainLoss: 0.0798 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 485 | ANN: trainLoss: 0.1050 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 485 | ANN: trainLoss: 0.1089 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 485 | ANN: trainLoss: 0.1069 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 485 | ANN: trainLoss: 0.1003 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 485 | ANN: trainLoss: 0.0951 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 485 | ANN: trainLoss: 0.0951 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 485 | ANN: trainLoss: 0.0947 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 485 | ANN: trainLoss: 0.0904 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 485 | ANN: trainLoss: 0.0901 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 485 | ANN: trainLoss: 0.0847 | trainAcc: 96.4981% (744/771)\n",
            "0 4 Epoch: 485 | ANN: testLoss: 0.7211 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 485 | ANN: testLoss: 0.9581 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 485 | ANN: testLoss: 0.9949 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 485 | ANN: testLoss: 0.7497 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 486 | ANN: trainLoss: 0.0597 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 486 | ANN: trainLoss: 0.0759 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 486 | ANN: trainLoss: 0.0748 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 486 | ANN: trainLoss: 0.0781 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 486 | ANN: trainLoss: 0.0770 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 486 | ANN: trainLoss: 0.0708 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 486 | ANN: trainLoss: 0.0742 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 486 | ANN: trainLoss: 0.0765 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 486 | ANN: trainLoss: 0.0716 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 486 | ANN: trainLoss: 0.0741 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 486 | ANN: trainLoss: 0.0801 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 486 | ANN: trainLoss: 0.0783 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 486 | ANN: trainLoss: 0.0986 | trainAcc: 97.6654% (753/771)\n",
            "0 4 Epoch: 486 | ANN: testLoss: 0.9157 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 486 | ANN: testLoss: 0.8302 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 486 | ANN: testLoss: 0.9756 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 486 | ANN: testLoss: 0.7606 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 487 | ANN: trainLoss: 0.0758 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 487 | ANN: trainLoss: 0.1002 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 487 | ANN: trainLoss: 0.1057 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 487 | ANN: trainLoss: 0.0911 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 487 | ANN: trainLoss: 0.0828 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 487 | ANN: trainLoss: 0.0805 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 487 | ANN: trainLoss: 0.0843 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 487 | ANN: trainLoss: 0.0851 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 487 | ANN: trainLoss: 0.0868 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 487 | ANN: trainLoss: 0.0887 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 487 | ANN: trainLoss: 0.0856 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 487 | ANN: trainLoss: 0.0862 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 487 | ANN: trainLoss: 0.2698 | trainAcc: 96.8872% (747/771)\n",
            "0 4 Epoch: 487 | ANN: testLoss: 1.2904 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 487 | ANN: testLoss: 1.0571 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 487 | ANN: testLoss: 0.9946 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 487 | ANN: testLoss: 0.7461 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 488 | ANN: trainLoss: 0.0426 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 488 | ANN: trainLoss: 0.0705 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 488 | ANN: trainLoss: 0.0866 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 488 | ANN: trainLoss: 0.0862 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 488 | ANN: trainLoss: 0.0916 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 488 | ANN: trainLoss: 0.0922 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 488 | ANN: trainLoss: 0.0932 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 488 | ANN: trainLoss: 0.0916 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 488 | ANN: trainLoss: 0.0999 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 488 | ANN: trainLoss: 0.1040 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 488 | ANN: trainLoss: 0.1039 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 488 | ANN: trainLoss: 0.0994 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 488 | ANN: trainLoss: 0.1025 | trainAcc: 95.8495% (739/771)\n",
            "0 4 Epoch: 488 | ANN: testLoss: 0.9161 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 488 | ANN: testLoss: 0.7737 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 488 | ANN: testLoss: 0.9507 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 488 | ANN: testLoss: 0.7130 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 489 | ANN: trainLoss: 0.0870 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 489 | ANN: trainLoss: 0.0805 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 489 | ANN: trainLoss: 0.0966 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 489 | ANN: trainLoss: 0.0896 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 489 | ANN: trainLoss: 0.0922 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 489 | ANN: trainLoss: 0.0864 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 489 | ANN: trainLoss: 0.0866 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 489 | ANN: trainLoss: 0.0877 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 489 | ANN: trainLoss: 0.0869 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 489 | ANN: trainLoss: 0.0893 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 489 | ANN: trainLoss: 0.0875 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 489 | ANN: trainLoss: 0.0845 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 489 | ANN: trainLoss: 0.0867 | trainAcc: 97.2763% (750/771)\n",
            "0 4 Epoch: 489 | ANN: testLoss: 0.6898 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 489 | ANN: testLoss: 0.8299 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 489 | ANN: testLoss: 0.9985 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 489 | ANN: testLoss: 0.8304 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 490 | ANN: trainLoss: 0.0470 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 490 | ANN: trainLoss: 0.0519 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 490 | ANN: trainLoss: 0.0560 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 490 | ANN: trainLoss: 0.0771 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 490 | ANN: trainLoss: 0.0769 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 490 | ANN: trainLoss: 0.0772 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 490 | ANN: trainLoss: 0.0705 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 490 | ANN: trainLoss: 0.0681 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 490 | ANN: trainLoss: 0.0695 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 490 | ANN: trainLoss: 0.0675 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 490 | ANN: trainLoss: 0.0735 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 490 | ANN: trainLoss: 0.0735 | trainAcc: 97.5260% (749/768)\n",
            "12 13 Epoch: 490 | ANN: trainLoss: 0.0727 | trainAcc: 97.5357% (752/771)\n",
            "0 4 Epoch: 490 | ANN: testLoss: 0.6826 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 490 | ANN: testLoss: 1.0552 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 490 | ANN: testLoss: 1.0239 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 490 | ANN: testLoss: 0.7681 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 491 | ANN: trainLoss: 0.0449 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 491 | ANN: trainLoss: 0.0683 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 491 | ANN: trainLoss: 0.0770 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 491 | ANN: trainLoss: 0.0849 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 491 | ANN: trainLoss: 0.0763 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 491 | ANN: trainLoss: 0.0767 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 491 | ANN: trainLoss: 0.0754 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 491 | ANN: trainLoss: 0.0775 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 491 | ANN: trainLoss: 0.0764 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 491 | ANN: trainLoss: 0.0760 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 491 | ANN: trainLoss: 0.0783 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 491 | ANN: trainLoss: 0.0761 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 491 | ANN: trainLoss: 0.3644 | trainAcc: 96.7575% (746/771)\n",
            "0 4 Epoch: 491 | ANN: testLoss: 0.8049 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 491 | ANN: testLoss: 1.0184 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 491 | ANN: testLoss: 1.0336 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 491 | ANN: testLoss: 0.7752 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 492 | ANN: trainLoss: 0.1587 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 492 | ANN: trainLoss: 0.1090 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 492 | ANN: trainLoss: 0.1372 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 492 | ANN: trainLoss: 0.1185 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 492 | ANN: trainLoss: 0.1161 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 492 | ANN: trainLoss: 0.1129 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 492 | ANN: trainLoss: 0.1097 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 492 | ANN: trainLoss: 0.1026 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 492 | ANN: trainLoss: 0.1003 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 492 | ANN: trainLoss: 0.1013 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 492 | ANN: trainLoss: 0.0977 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 492 | ANN: trainLoss: 0.1005 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 492 | ANN: trainLoss: 0.0986 | trainAcc: 96.2387% (742/771)\n",
            "0 4 Epoch: 492 | ANN: testLoss: 0.7699 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 492 | ANN: testLoss: 0.9642 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 492 | ANN: testLoss: 0.9427 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 492 | ANN: testLoss: 2.3428 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 493 | ANN: trainLoss: 0.0971 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 493 | ANN: trainLoss: 0.1267 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 493 | ANN: trainLoss: 0.1013 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 493 | ANN: trainLoss: 0.0819 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 493 | ANN: trainLoss: 0.0811 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 493 | ANN: trainLoss: 0.0775 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 493 | ANN: trainLoss: 0.0839 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 493 | ANN: trainLoss: 0.0846 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 493 | ANN: trainLoss: 0.0849 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 493 | ANN: trainLoss: 0.0823 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 493 | ANN: trainLoss: 0.0813 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 493 | ANN: trainLoss: 0.0822 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 493 | ANN: trainLoss: 0.0898 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 493 | ANN: testLoss: 0.9687 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 493 | ANN: testLoss: 0.9320 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 493 | ANN: testLoss: 0.9859 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 493 | ANN: testLoss: 0.7494 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 494 | ANN: trainLoss: 0.1067 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 494 | ANN: trainLoss: 0.0989 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 494 | ANN: trainLoss: 0.0975 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 494 | ANN: trainLoss: 0.0863 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 494 | ANN: trainLoss: 0.0965 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 494 | ANN: trainLoss: 0.0941 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 494 | ANN: trainLoss: 0.0908 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 494 | ANN: trainLoss: 0.0967 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 494 | ANN: trainLoss: 0.1047 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 494 | ANN: trainLoss: 0.0979 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 494 | ANN: trainLoss: 0.0923 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 494 | ANN: trainLoss: 0.0899 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 494 | ANN: trainLoss: 0.1039 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 494 | ANN: testLoss: 0.8108 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 494 | ANN: testLoss: 1.0365 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 494 | ANN: testLoss: 0.9536 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 494 | ANN: testLoss: 0.7992 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 495 | ANN: trainLoss: 0.0819 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 495 | ANN: trainLoss: 0.1008 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 495 | ANN: trainLoss: 0.0890 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 495 | ANN: trainLoss: 0.0902 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 495 | ANN: trainLoss: 0.0823 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 495 | ANN: trainLoss: 0.0838 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 495 | ANN: trainLoss: 0.0823 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 495 | ANN: trainLoss: 0.0864 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 495 | ANN: trainLoss: 0.0824 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 495 | ANN: trainLoss: 0.0809 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 495 | ANN: trainLoss: 0.0809 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 495 | ANN: trainLoss: 0.0853 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 495 | ANN: trainLoss: 0.0972 | trainAcc: 97.2763% (750/771)\n",
            "0 4 Epoch: 495 | ANN: testLoss: 0.8923 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 495 | ANN: testLoss: 1.0358 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 495 | ANN: testLoss: 0.9917 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 495 | ANN: testLoss: 1.1080 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 496 | ANN: trainLoss: 0.1031 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 496 | ANN: trainLoss: 0.0879 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 496 | ANN: trainLoss: 0.0950 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 496 | ANN: trainLoss: 0.1083 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 496 | ANN: trainLoss: 0.1149 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 496 | ANN: trainLoss: 0.1080 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 496 | ANN: trainLoss: 0.1081 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 496 | ANN: trainLoss: 0.0999 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 496 | ANN: trainLoss: 0.0984 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 496 | ANN: trainLoss: 0.0914 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 496 | ANN: trainLoss: 0.0941 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 496 | ANN: trainLoss: 0.1014 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 496 | ANN: trainLoss: 0.1096 | trainAcc: 96.2387% (742/771)\n",
            "0 4 Epoch: 496 | ANN: testLoss: 1.0596 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 496 | ANN: testLoss: 0.9492 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 496 | ANN: testLoss: 1.0417 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 496 | ANN: testLoss: 0.7848 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 497 | ANN: trainLoss: 0.0640 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 497 | ANN: trainLoss: 0.0742 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 497 | ANN: trainLoss: 0.0629 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 497 | ANN: trainLoss: 0.0560 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 497 | ANN: trainLoss: 0.0567 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 497 | ANN: trainLoss: 0.0581 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 497 | ANN: trainLoss: 0.0610 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 497 | ANN: trainLoss: 0.0655 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 497 | ANN: trainLoss: 0.0720 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 497 | ANN: trainLoss: 0.0789 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 497 | ANN: trainLoss: 0.0743 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 497 | ANN: trainLoss: 0.0801 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 497 | ANN: trainLoss: 0.0863 | trainAcc: 97.2763% (750/771)\n",
            "0 4 Epoch: 497 | ANN: testLoss: 0.8439 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 497 | ANN: testLoss: 0.8053 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 497 | ANN: testLoss: 0.9599 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 497 | ANN: testLoss: 1.2037 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 498 | ANN: trainLoss: 0.1378 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 498 | ANN: trainLoss: 0.1053 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 498 | ANN: trainLoss: 0.1073 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 498 | ANN: trainLoss: 0.0937 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 498 | ANN: trainLoss: 0.0899 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 498 | ANN: trainLoss: 0.0865 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 498 | ANN: trainLoss: 0.0915 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 498 | ANN: trainLoss: 0.0948 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 498 | ANN: trainLoss: 0.0894 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 498 | ANN: trainLoss: 0.0891 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 498 | ANN: trainLoss: 0.0869 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 498 | ANN: trainLoss: 0.0838 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 498 | ANN: trainLoss: 0.0821 | trainAcc: 97.1466% (749/771)\n",
            "0 4 Epoch: 498 | ANN: testLoss: 1.1161 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 498 | ANN: testLoss: 1.0598 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 498 | ANN: testLoss: 0.9600 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 498 | ANN: testLoss: 0.7865 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 499 | ANN: trainLoss: 0.0412 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 499 | ANN: trainLoss: 0.0496 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 499 | ANN: trainLoss: 0.0583 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 499 | ANN: trainLoss: 0.0579 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 499 | ANN: trainLoss: 0.0667 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 499 | ANN: trainLoss: 0.0802 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 499 | ANN: trainLoss: 0.0851 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 499 | ANN: trainLoss: 0.0811 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 499 | ANN: trainLoss: 0.0802 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 499 | ANN: trainLoss: 0.0875 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 499 | ANN: trainLoss: 0.0883 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 499 | ANN: trainLoss: 0.0874 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 499 | ANN: trainLoss: 0.2304 | trainAcc: 96.3684% (743/771)\n",
            "0 4 Epoch: 499 | ANN: testLoss: 0.9383 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 499 | ANN: testLoss: 1.0102 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 499 | ANN: testLoss: 0.9648 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 499 | ANN: testLoss: 0.7256 | testAcc: 69.9482% (135/193)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 397.59it/s]\n",
            "100%|██████████| 13/13 [00:00<00:00, 295.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 69.9500%\n",
            "SNN accuracy: max_norm: 69.4301%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LENet_FCL to SNN Conversion Framework execution\n",
        "\n",
        "# Hyperparameters\n",
        "#EPOCHS = 300\n",
        "#BATCH_SIZE = 128\n",
        "#TIME_STEPS = 100  # T for SNN\n",
        "#TEST_SIZE = 0.2\n",
        "#DROP_OUT = 0.5\n",
        "\n",
        "\n",
        "# Split the data\n",
        "print(f\"{100 - (TEST_SIZE * 100)}% of the dataset is used for training and {TEST_SIZE * 100}% is used for testing.\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=TEST_SIZE, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# Initialize model\n",
        "cnn_model_lenet_fcl = LENet_FCL(classes_num=3, channel_count=channel_count, drop_out = DROP_OUT).to(device)\n",
        "cnn_model_lenet_fcl.apply(initialize_weights)\n",
        "\n",
        "# Train CNN model\n",
        "train_acc, test_acc,  cnn_model_lenet_fcl = train_ann(cnn_model_lenet_fcl, train_data, train_label, test_data, test_label,\n",
        "                                              ep=EPOCHS, batch=BATCH_SIZE)\n",
        "max_norm_acc = anntosnn( cnn_model_lenet_fcl, train_data, train_label, test_data, test_label,\n",
        "                        batch=BATCH_SIZE, T=TIME_STEPS)\n",
        "snn_model_lenet_fcl = ann2snn.Converter(mode='max', dataloader=data_loader(train_data, train_label, batch=BATCH_SIZE))(cnn_model_lenet_fcl)\n",
        "\n",
        "print('\\n')\n",
        "print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))"
      ],
      "metadata": {
        "id": "AeiiLk4TQp-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "outputId": "5e7583b2-93b5-43f1-c204-3bdf6914c146"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "3 4 Epoch: 236 | ANN: testLoss: 0.4517 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 237 | ANN: trainLoss: 0.4437 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 237 | ANN: trainLoss: 0.3444 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 237 | ANN: trainLoss: 0.3632 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 237 | ANN: trainLoss: 0.3571 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 237 | ANN: trainLoss: 0.3545 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 237 | ANN: trainLoss: 0.3637 | trainAcc: 83.0729% (319/384)\n",
            "6 13 Epoch: 237 | ANN: trainLoss: 0.3729 | trainAcc: 81.9196% (367/448)\n",
            "7 13 Epoch: 237 | ANN: trainLoss: 0.3772 | trainAcc: 81.6406% (418/512)\n",
            "8 13 Epoch: 237 | ANN: trainLoss: 0.3766 | trainAcc: 81.0764% (467/576)\n",
            "9 13 Epoch: 237 | ANN: trainLoss: 0.3915 | trainAcc: 80.0000% (512/640)\n",
            "10 13 Epoch: 237 | ANN: trainLoss: 0.3914 | trainAcc: 80.3977% (566/704)\n",
            "11 13 Epoch: 237 | ANN: trainLoss: 0.3898 | trainAcc: 80.7292% (620/768)\n",
            "12 13 Epoch: 237 | ANN: trainLoss: 0.4582 | trainAcc: 80.6744% (622/771)\n",
            "0 4 Epoch: 237 | ANN: testLoss: 0.5583 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 237 | ANN: testLoss: 0.5483 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 237 | ANN: testLoss: 0.5029 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 237 | ANN: testLoss: 0.5341 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 238 | ANN: trainLoss: 0.5065 | trainAcc: 71.8750% (46/64)\n",
            "1 13 Epoch: 238 | ANN: trainLoss: 0.4607 | trainAcc: 71.8750% (92/128)\n",
            "2 13 Epoch: 238 | ANN: trainLoss: 0.4374 | trainAcc: 77.0833% (148/192)\n",
            "3 13 Epoch: 238 | ANN: trainLoss: 0.4143 | trainAcc: 79.2969% (203/256)\n",
            "4 13 Epoch: 238 | ANN: trainLoss: 0.3783 | trainAcc: 81.5625% (261/320)\n",
            "5 13 Epoch: 238 | ANN: trainLoss: 0.3994 | trainAcc: 81.5104% (313/384)\n",
            "6 13 Epoch: 238 | ANN: trainLoss: 0.3977 | trainAcc: 81.2500% (364/448)\n",
            "7 13 Epoch: 238 | ANN: trainLoss: 0.3944 | trainAcc: 81.2500% (416/512)\n",
            "8 13 Epoch: 238 | ANN: trainLoss: 0.3975 | trainAcc: 80.7292% (465/576)\n",
            "9 13 Epoch: 238 | ANN: trainLoss: 0.4022 | trainAcc: 80.9375% (518/640)\n",
            "10 13 Epoch: 238 | ANN: trainLoss: 0.4070 | trainAcc: 80.9659% (570/704)\n",
            "11 13 Epoch: 238 | ANN: trainLoss: 0.4092 | trainAcc: 80.9896% (622/768)\n",
            "12 13 Epoch: 238 | ANN: trainLoss: 0.3879 | trainAcc: 81.0636% (625/771)\n",
            "0 4 Epoch: 238 | ANN: testLoss: 0.4593 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 238 | ANN: testLoss: 0.4606 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 238 | ANN: testLoss: 0.4763 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 238 | ANN: testLoss: 0.6163 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 239 | ANN: trainLoss: 0.3720 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 239 | ANN: trainLoss: 0.4097 | trainAcc: 80.4688% (103/128)\n",
            "2 13 Epoch: 239 | ANN: trainLoss: 0.3880 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 239 | ANN: trainLoss: 0.3897 | trainAcc: 80.4688% (206/256)\n",
            "4 13 Epoch: 239 | ANN: trainLoss: 0.3789 | trainAcc: 80.9375% (259/320)\n",
            "5 13 Epoch: 239 | ANN: trainLoss: 0.3765 | trainAcc: 81.5104% (313/384)\n",
            "6 13 Epoch: 239 | ANN: trainLoss: 0.3870 | trainAcc: 80.3571% (360/448)\n",
            "7 13 Epoch: 239 | ANN: trainLoss: 0.4061 | trainAcc: 79.1016% (405/512)\n",
            "8 13 Epoch: 239 | ANN: trainLoss: 0.4006 | trainAcc: 79.6875% (459/576)\n",
            "9 13 Epoch: 239 | ANN: trainLoss: 0.3963 | trainAcc: 79.6875% (510/640)\n",
            "10 13 Epoch: 239 | ANN: trainLoss: 0.3928 | trainAcc: 80.3977% (566/704)\n",
            "11 13 Epoch: 239 | ANN: trainLoss: 0.3980 | trainAcc: 80.4688% (618/768)\n",
            "12 13 Epoch: 239 | ANN: trainLoss: 0.3886 | trainAcc: 80.5447% (621/771)\n",
            "0 4 Epoch: 239 | ANN: testLoss: 0.4440 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 239 | ANN: testLoss: 0.4430 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 239 | ANN: testLoss: 0.4812 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 239 | ANN: testLoss: 0.3612 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 240 | ANN: trainLoss: 0.3227 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 240 | ANN: trainLoss: 0.2903 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 240 | ANN: trainLoss: 0.2990 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 240 | ANN: trainLoss: 0.3068 | trainAcc: 85.1562% (218/256)\n",
            "4 13 Epoch: 240 | ANN: trainLoss: 0.3140 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 240 | ANN: trainLoss: 0.3368 | trainAcc: 82.8125% (318/384)\n",
            "6 13 Epoch: 240 | ANN: trainLoss: 0.3375 | trainAcc: 83.7054% (375/448)\n",
            "7 13 Epoch: 240 | ANN: trainLoss: 0.3525 | trainAcc: 82.8125% (424/512)\n",
            "8 13 Epoch: 240 | ANN: trainLoss: 0.3550 | trainAcc: 82.8125% (477/576)\n",
            "9 13 Epoch: 240 | ANN: trainLoss: 0.3520 | trainAcc: 82.9688% (531/640)\n",
            "10 13 Epoch: 240 | ANN: trainLoss: 0.3499 | trainAcc: 83.0966% (585/704)\n",
            "11 13 Epoch: 240 | ANN: trainLoss: 0.3573 | trainAcc: 82.9427% (637/768)\n",
            "12 13 Epoch: 240 | ANN: trainLoss: 0.5152 | trainAcc: 82.6200% (637/771)\n",
            "0 4 Epoch: 240 | ANN: testLoss: 0.4419 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 240 | ANN: testLoss: 0.4855 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 240 | ANN: testLoss: 0.4728 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 240 | ANN: testLoss: 0.6103 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 241 | ANN: trainLoss: 0.3386 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 241 | ANN: trainLoss: 0.4217 | trainAcc: 80.4688% (103/128)\n",
            "2 13 Epoch: 241 | ANN: trainLoss: 0.4117 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 241 | ANN: trainLoss: 0.4022 | trainAcc: 81.6406% (209/256)\n",
            "4 13 Epoch: 241 | ANN: trainLoss: 0.4165 | trainAcc: 80.9375% (259/320)\n",
            "5 13 Epoch: 241 | ANN: trainLoss: 0.4043 | trainAcc: 80.9896% (311/384)\n",
            "6 13 Epoch: 241 | ANN: trainLoss: 0.4014 | trainAcc: 81.4732% (365/448)\n",
            "7 13 Epoch: 241 | ANN: trainLoss: 0.4027 | trainAcc: 81.8359% (419/512)\n",
            "8 13 Epoch: 241 | ANN: trainLoss: 0.4070 | trainAcc: 81.4236% (469/576)\n",
            "9 13 Epoch: 241 | ANN: trainLoss: 0.4117 | trainAcc: 81.5625% (522/640)\n",
            "10 13 Epoch: 241 | ANN: trainLoss: 0.4076 | trainAcc: 81.5341% (574/704)\n",
            "11 13 Epoch: 241 | ANN: trainLoss: 0.4071 | trainAcc: 81.5104% (626/768)\n",
            "12 13 Epoch: 241 | ANN: trainLoss: 0.4184 | trainAcc: 81.4527% (628/771)\n",
            "0 4 Epoch: 241 | ANN: testLoss: 0.4147 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 241 | ANN: testLoss: 0.4304 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 241 | ANN: testLoss: 0.4653 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 241 | ANN: testLoss: 0.3501 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 242 | ANN: trainLoss: 0.5261 | trainAcc: 73.4375% (47/64)\n",
            "1 13 Epoch: 242 | ANN: trainLoss: 0.4381 | trainAcc: 80.4688% (103/128)\n",
            "2 13 Epoch: 242 | ANN: trainLoss: 0.4031 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 242 | ANN: trainLoss: 0.4028 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 242 | ANN: trainLoss: 0.4077 | trainAcc: 80.6250% (258/320)\n",
            "5 13 Epoch: 242 | ANN: trainLoss: 0.3909 | trainAcc: 81.7708% (314/384)\n",
            "6 13 Epoch: 242 | ANN: trainLoss: 0.3999 | trainAcc: 80.8036% (362/448)\n",
            "7 13 Epoch: 242 | ANN: trainLoss: 0.3997 | trainAcc: 80.8594% (414/512)\n",
            "8 13 Epoch: 242 | ANN: trainLoss: 0.4045 | trainAcc: 80.2083% (462/576)\n",
            "9 13 Epoch: 242 | ANN: trainLoss: 0.4010 | trainAcc: 80.3125% (514/640)\n",
            "10 13 Epoch: 242 | ANN: trainLoss: 0.4019 | trainAcc: 80.3977% (566/704)\n",
            "11 13 Epoch: 242 | ANN: trainLoss: 0.4009 | trainAcc: 80.4688% (618/768)\n",
            "12 13 Epoch: 242 | ANN: trainLoss: 0.4031 | trainAcc: 80.4150% (620/771)\n",
            "0 4 Epoch: 242 | ANN: testLoss: 0.5031 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 242 | ANN: testLoss: 0.4596 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 242 | ANN: testLoss: 0.4636 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 242 | ANN: testLoss: 0.7317 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 243 | ANN: trainLoss: 0.3486 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 243 | ANN: trainLoss: 0.3202 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 243 | ANN: trainLoss: 0.3030 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 243 | ANN: trainLoss: 0.3362 | trainAcc: 85.1562% (218/256)\n",
            "4 13 Epoch: 243 | ANN: trainLoss: 0.3767 | trainAcc: 82.1875% (263/320)\n",
            "5 13 Epoch: 243 | ANN: trainLoss: 0.4157 | trainAcc: 80.4688% (309/384)\n",
            "6 13 Epoch: 243 | ANN: trainLoss: 0.4112 | trainAcc: 80.3571% (360/448)\n",
            "7 13 Epoch: 243 | ANN: trainLoss: 0.3990 | trainAcc: 81.2500% (416/512)\n",
            "8 13 Epoch: 243 | ANN: trainLoss: 0.4011 | trainAcc: 81.4236% (469/576)\n",
            "9 13 Epoch: 243 | ANN: trainLoss: 0.3954 | trainAcc: 81.7188% (523/640)\n",
            "10 13 Epoch: 243 | ANN: trainLoss: 0.3989 | trainAcc: 81.2500% (572/704)\n",
            "11 13 Epoch: 243 | ANN: trainLoss: 0.3976 | trainAcc: 81.1198% (623/768)\n",
            "12 13 Epoch: 243 | ANN: trainLoss: 0.3943 | trainAcc: 81.1933% (626/771)\n",
            "0 4 Epoch: 243 | ANN: testLoss: 0.4190 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 243 | ANN: testLoss: 0.4204 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 243 | ANN: testLoss: 0.4657 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 243 | ANN: testLoss: 0.8833 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 244 | ANN: trainLoss: 0.3464 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 244 | ANN: trainLoss: 0.3368 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 244 | ANN: trainLoss: 0.3309 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 244 | ANN: trainLoss: 0.3274 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 244 | ANN: trainLoss: 0.3335 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 244 | ANN: trainLoss: 0.3399 | trainAcc: 83.0729% (319/384)\n",
            "6 13 Epoch: 244 | ANN: trainLoss: 0.3464 | trainAcc: 83.0357% (372/448)\n",
            "7 13 Epoch: 244 | ANN: trainLoss: 0.3446 | trainAcc: 83.2031% (426/512)\n",
            "8 13 Epoch: 244 | ANN: trainLoss: 0.3434 | trainAcc: 83.1597% (479/576)\n",
            "9 13 Epoch: 244 | ANN: trainLoss: 0.3428 | trainAcc: 83.5938% (535/640)\n",
            "10 13 Epoch: 244 | ANN: trainLoss: 0.3452 | trainAcc: 83.5227% (588/704)\n",
            "11 13 Epoch: 244 | ANN: trainLoss: 0.3495 | trainAcc: 83.3333% (640/768)\n",
            "12 13 Epoch: 244 | ANN: trainLoss: 0.3406 | trainAcc: 83.3982% (643/771)\n",
            "0 4 Epoch: 244 | ANN: testLoss: 0.5457 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 244 | ANN: testLoss: 0.5564 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 244 | ANN: testLoss: 0.4930 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 244 | ANN: testLoss: 0.3701 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 245 | ANN: trainLoss: 0.4622 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 245 | ANN: trainLoss: 0.3865 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 245 | ANN: trainLoss: 0.3930 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 245 | ANN: trainLoss: 0.3550 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 245 | ANN: trainLoss: 0.3743 | trainAcc: 81.5625% (261/320)\n",
            "5 13 Epoch: 245 | ANN: trainLoss: 0.3747 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 245 | ANN: trainLoss: 0.3554 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 245 | ANN: trainLoss: 0.3616 | trainAcc: 82.0312% (420/512)\n",
            "8 13 Epoch: 245 | ANN: trainLoss: 0.3599 | trainAcc: 81.9444% (472/576)\n",
            "9 13 Epoch: 245 | ANN: trainLoss: 0.3511 | trainAcc: 82.5000% (528/640)\n",
            "10 13 Epoch: 245 | ANN: trainLoss: 0.3547 | trainAcc: 82.3864% (580/704)\n",
            "11 13 Epoch: 245 | ANN: trainLoss: 0.3689 | trainAcc: 82.0312% (630/768)\n",
            "12 13 Epoch: 245 | ANN: trainLoss: 0.4142 | trainAcc: 81.9715% (632/771)\n",
            "0 4 Epoch: 245 | ANN: testLoss: 0.4168 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 245 | ANN: testLoss: 0.4621 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 245 | ANN: testLoss: 0.5060 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 245 | ANN: testLoss: 0.3852 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 246 | ANN: trainLoss: 0.4486 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 246 | ANN: trainLoss: 0.3864 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 246 | ANN: trainLoss: 0.4039 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 246 | ANN: trainLoss: 0.3765 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 246 | ANN: trainLoss: 0.3639 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 246 | ANN: trainLoss: 0.3541 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 246 | ANN: trainLoss: 0.3399 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 246 | ANN: trainLoss: 0.3398 | trainAcc: 84.9609% (435/512)\n",
            "8 13 Epoch: 246 | ANN: trainLoss: 0.3387 | trainAcc: 84.7222% (488/576)\n",
            "9 13 Epoch: 246 | ANN: trainLoss: 0.3454 | trainAcc: 84.0625% (538/640)\n",
            "10 13 Epoch: 246 | ANN: trainLoss: 0.3499 | trainAcc: 83.6648% (589/704)\n",
            "11 13 Epoch: 246 | ANN: trainLoss: 0.3441 | trainAcc: 84.1146% (646/768)\n",
            "12 13 Epoch: 246 | ANN: trainLoss: 0.3204 | trainAcc: 84.1764% (649/771)\n",
            "0 4 Epoch: 246 | ANN: testLoss: 0.5617 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 246 | ANN: testLoss: 0.5430 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 246 | ANN: testLoss: 0.4921 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 246 | ANN: testLoss: 0.5464 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 247 | ANN: trainLoss: 0.3700 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 247 | ANN: trainLoss: 0.3764 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 247 | ANN: trainLoss: 0.3859 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 247 | ANN: trainLoss: 0.3829 | trainAcc: 79.6875% (204/256)\n",
            "4 13 Epoch: 247 | ANN: trainLoss: 0.3670 | trainAcc: 81.8750% (262/320)\n",
            "5 13 Epoch: 247 | ANN: trainLoss: 0.3655 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 247 | ANN: trainLoss: 0.3644 | trainAcc: 82.1429% (368/448)\n",
            "7 13 Epoch: 247 | ANN: trainLoss: 0.3618 | trainAcc: 82.2266% (421/512)\n",
            "8 13 Epoch: 247 | ANN: trainLoss: 0.3647 | trainAcc: 82.1181% (473/576)\n",
            "9 13 Epoch: 247 | ANN: trainLoss: 0.3564 | trainAcc: 82.6562% (529/640)\n",
            "10 13 Epoch: 247 | ANN: trainLoss: 0.3541 | trainAcc: 82.8125% (583/704)\n",
            "11 13 Epoch: 247 | ANN: trainLoss: 0.3530 | trainAcc: 82.9427% (637/768)\n",
            "12 13 Epoch: 247 | ANN: trainLoss: 0.5585 | trainAcc: 82.7497% (638/771)\n",
            "0 4 Epoch: 247 | ANN: testLoss: 0.5412 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 247 | ANN: testLoss: 0.5288 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 247 | ANN: testLoss: 0.4888 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 247 | ANN: testLoss: 0.5418 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 248 | ANN: trainLoss: 0.3316 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 248 | ANN: trainLoss: 0.3706 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 248 | ANN: trainLoss: 0.3140 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 248 | ANN: trainLoss: 0.3223 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 248 | ANN: trainLoss: 0.3178 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 248 | ANN: trainLoss: 0.3278 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 248 | ANN: trainLoss: 0.3307 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 248 | ANN: trainLoss: 0.3192 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 248 | ANN: trainLoss: 0.3244 | trainAcc: 85.4167% (492/576)\n",
            "9 13 Epoch: 248 | ANN: trainLoss: 0.3270 | trainAcc: 84.8438% (543/640)\n",
            "10 13 Epoch: 248 | ANN: trainLoss: 0.3229 | trainAcc: 84.8011% (597/704)\n",
            "11 13 Epoch: 248 | ANN: trainLoss: 0.3296 | trainAcc: 84.8958% (652/768)\n",
            "12 13 Epoch: 248 | ANN: trainLoss: 0.4471 | trainAcc: 84.8249% (654/771)\n",
            "0 4 Epoch: 248 | ANN: testLoss: 0.5026 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 248 | ANN: testLoss: 0.5403 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 248 | ANN: testLoss: 0.5023 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 248 | ANN: testLoss: 0.5224 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 249 | ANN: trainLoss: 0.3600 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 249 | ANN: trainLoss: 0.3272 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 249 | ANN: trainLoss: 0.3498 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 249 | ANN: trainLoss: 0.3545 | trainAcc: 84.7656% (217/256)\n",
            "4 13 Epoch: 249 | ANN: trainLoss: 0.3382 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 249 | ANN: trainLoss: 0.3409 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 249 | ANN: trainLoss: 0.3454 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 249 | ANN: trainLoss: 0.3525 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 249 | ANN: trainLoss: 0.3599 | trainAcc: 84.2014% (485/576)\n",
            "9 13 Epoch: 249 | ANN: trainLoss: 0.3676 | trainAcc: 83.2812% (533/640)\n",
            "10 13 Epoch: 249 | ANN: trainLoss: 0.3670 | trainAcc: 82.9545% (584/704)\n",
            "11 13 Epoch: 249 | ANN: trainLoss: 0.3680 | trainAcc: 82.8125% (636/768)\n",
            "12 13 Epoch: 249 | ANN: trainLoss: 0.3458 | trainAcc: 82.8794% (639/771)\n",
            "0 4 Epoch: 249 | ANN: testLoss: 0.5393 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 249 | ANN: testLoss: 0.5100 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 249 | ANN: testLoss: 0.5065 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 249 | ANN: testLoss: 0.5076 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 250 | ANN: trainLoss: 0.3728 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 250 | ANN: trainLoss: 0.3269 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 250 | ANN: trainLoss: 0.3364 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 250 | ANN: trainLoss: 0.3709 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 250 | ANN: trainLoss: 0.3661 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 250 | ANN: trainLoss: 0.3715 | trainAcc: 85.1562% (327/384)\n",
            "6 13 Epoch: 250 | ANN: trainLoss: 0.3544 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 250 | ANN: trainLoss: 0.3378 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 250 | ANN: trainLoss: 0.3337 | trainAcc: 86.9792% (501/576)\n",
            "9 13 Epoch: 250 | ANN: trainLoss: 0.3356 | trainAcc: 86.5625% (554/640)\n",
            "10 13 Epoch: 250 | ANN: trainLoss: 0.3329 | trainAcc: 86.7898% (611/704)\n",
            "11 13 Epoch: 250 | ANN: trainLoss: 0.3399 | trainAcc: 85.9375% (660/768)\n",
            "12 13 Epoch: 250 | ANN: trainLoss: 0.3203 | trainAcc: 85.9922% (663/771)\n",
            "0 4 Epoch: 250 | ANN: testLoss: 0.4855 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 250 | ANN: testLoss: 0.5308 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 250 | ANN: testLoss: 0.5142 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 250 | ANN: testLoss: 0.3857 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 251 | ANN: trainLoss: 0.3642 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 251 | ANN: trainLoss: 0.3601 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 251 | ANN: trainLoss: 0.3971 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 251 | ANN: trainLoss: 0.4131 | trainAcc: 82.0312% (210/256)\n",
            "4 13 Epoch: 251 | ANN: trainLoss: 0.4345 | trainAcc: 80.9375% (259/320)\n",
            "5 13 Epoch: 251 | ANN: trainLoss: 0.4313 | trainAcc: 80.9896% (311/384)\n",
            "6 13 Epoch: 251 | ANN: trainLoss: 0.4272 | trainAcc: 81.2500% (364/448)\n",
            "7 13 Epoch: 251 | ANN: trainLoss: 0.4202 | trainAcc: 81.4453% (417/512)\n",
            "8 13 Epoch: 251 | ANN: trainLoss: 0.4100 | trainAcc: 81.2500% (468/576)\n",
            "9 13 Epoch: 251 | ANN: trainLoss: 0.4015 | trainAcc: 81.0938% (519/640)\n",
            "10 13 Epoch: 251 | ANN: trainLoss: 0.3970 | trainAcc: 81.1080% (571/704)\n",
            "11 13 Epoch: 251 | ANN: trainLoss: 0.3934 | trainAcc: 81.1198% (623/768)\n",
            "12 13 Epoch: 251 | ANN: trainLoss: 0.3921 | trainAcc: 81.1933% (626/771)\n",
            "0 4 Epoch: 251 | ANN: testLoss: 0.4686 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 251 | ANN: testLoss: 0.4691 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 251 | ANN: testLoss: 0.5007 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 251 | ANN: testLoss: 0.4287 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 252 | ANN: trainLoss: 0.3082 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 252 | ANN: trainLoss: 0.3652 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 252 | ANN: trainLoss: 0.3626 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 252 | ANN: trainLoss: 0.3650 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 252 | ANN: trainLoss: 0.3754 | trainAcc: 80.9375% (259/320)\n",
            "5 13 Epoch: 252 | ANN: trainLoss: 0.3530 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 252 | ANN: trainLoss: 0.3621 | trainAcc: 81.4732% (365/448)\n",
            "7 13 Epoch: 252 | ANN: trainLoss: 0.3700 | trainAcc: 81.2500% (416/512)\n",
            "8 13 Epoch: 252 | ANN: trainLoss: 0.3671 | trainAcc: 81.4236% (469/576)\n",
            "9 13 Epoch: 252 | ANN: trainLoss: 0.3709 | trainAcc: 80.9375% (518/640)\n",
            "10 13 Epoch: 252 | ANN: trainLoss: 0.3714 | trainAcc: 81.3920% (573/704)\n",
            "11 13 Epoch: 252 | ANN: trainLoss: 0.3709 | trainAcc: 81.5104% (626/768)\n",
            "12 13 Epoch: 252 | ANN: trainLoss: 0.3606 | trainAcc: 81.5824% (629/771)\n",
            "0 4 Epoch: 252 | ANN: testLoss: 0.4660 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 252 | ANN: testLoss: 0.4588 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 252 | ANN: testLoss: 0.5003 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 252 | ANN: testLoss: 0.6895 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 253 | ANN: trainLoss: 0.3871 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 253 | ANN: trainLoss: 0.3585 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 253 | ANN: trainLoss: 0.3589 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 253 | ANN: trainLoss: 0.3697 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 253 | ANN: trainLoss: 0.3844 | trainAcc: 80.6250% (258/320)\n",
            "5 13 Epoch: 253 | ANN: trainLoss: 0.3813 | trainAcc: 81.5104% (313/384)\n",
            "6 13 Epoch: 253 | ANN: trainLoss: 0.3662 | trainAcc: 82.1429% (368/448)\n",
            "7 13 Epoch: 253 | ANN: trainLoss: 0.3674 | trainAcc: 82.2266% (421/512)\n",
            "8 13 Epoch: 253 | ANN: trainLoss: 0.3628 | trainAcc: 82.2917% (474/576)\n",
            "9 13 Epoch: 253 | ANN: trainLoss: 0.3569 | trainAcc: 82.3438% (527/640)\n",
            "10 13 Epoch: 253 | ANN: trainLoss: 0.3533 | trainAcc: 82.3864% (580/704)\n",
            "11 13 Epoch: 253 | ANN: trainLoss: 0.3486 | trainAcc: 82.2917% (632/768)\n",
            "12 13 Epoch: 253 | ANN: trainLoss: 0.5341 | trainAcc: 82.1012% (633/771)\n",
            "0 4 Epoch: 253 | ANN: testLoss: 0.4748 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 253 | ANN: testLoss: 0.4722 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 253 | ANN: testLoss: 0.4998 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 253 | ANN: testLoss: 0.4848 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 254 | ANN: trainLoss: 0.3279 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 254 | ANN: trainLoss: 0.3331 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 254 | ANN: trainLoss: 0.3508 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 254 | ANN: trainLoss: 0.3818 | trainAcc: 82.0312% (210/256)\n",
            "4 13 Epoch: 254 | ANN: trainLoss: 0.3767 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 254 | ANN: trainLoss: 0.3651 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 254 | ANN: trainLoss: 0.3563 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 254 | ANN: trainLoss: 0.3586 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 254 | ANN: trainLoss: 0.3572 | trainAcc: 84.2014% (485/576)\n",
            "9 13 Epoch: 254 | ANN: trainLoss: 0.3472 | trainAcc: 84.6875% (542/640)\n",
            "10 13 Epoch: 254 | ANN: trainLoss: 0.3553 | trainAcc: 83.9489% (591/704)\n",
            "11 13 Epoch: 254 | ANN: trainLoss: 0.3575 | trainAcc: 83.7240% (643/768)\n",
            "12 13 Epoch: 254 | ANN: trainLoss: 0.3514 | trainAcc: 83.7873% (646/771)\n",
            "0 4 Epoch: 254 | ANN: testLoss: 0.4761 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 254 | ANN: testLoss: 0.4753 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 254 | ANN: testLoss: 0.4950 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 254 | ANN: testLoss: 0.4319 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 255 | ANN: trainLoss: 0.2995 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 255 | ANN: trainLoss: 0.3239 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 255 | ANN: trainLoss: 0.3652 | trainAcc: 82.8125% (159/192)\n",
            "3 13 Epoch: 255 | ANN: trainLoss: 0.3679 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 255 | ANN: trainLoss: 0.3780 | trainAcc: 82.5000% (264/320)\n",
            "5 13 Epoch: 255 | ANN: trainLoss: 0.3757 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 255 | ANN: trainLoss: 0.3752 | trainAcc: 81.4732% (365/448)\n",
            "7 13 Epoch: 255 | ANN: trainLoss: 0.3768 | trainAcc: 81.2500% (416/512)\n",
            "8 13 Epoch: 255 | ANN: trainLoss: 0.3786 | trainAcc: 81.2500% (468/576)\n",
            "9 13 Epoch: 255 | ANN: trainLoss: 0.3780 | trainAcc: 81.2500% (520/640)\n",
            "10 13 Epoch: 255 | ANN: trainLoss: 0.3816 | trainAcc: 81.5341% (574/704)\n",
            "11 13 Epoch: 255 | ANN: trainLoss: 0.3884 | trainAcc: 81.5104% (626/768)\n",
            "12 13 Epoch: 255 | ANN: trainLoss: 0.3705 | trainAcc: 81.5824% (629/771)\n",
            "0 4 Epoch: 255 | ANN: testLoss: 0.4325 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 255 | ANN: testLoss: 0.4916 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 255 | ANN: testLoss: 0.5011 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 255 | ANN: testLoss: 0.7602 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 256 | ANN: trainLoss: 0.3159 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 256 | ANN: trainLoss: 0.3458 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 256 | ANN: trainLoss: 0.3298 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 256 | ANN: trainLoss: 0.3278 | trainAcc: 84.7656% (217/256)\n",
            "4 13 Epoch: 256 | ANN: trainLoss: 0.3476 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 256 | ANN: trainLoss: 0.3427 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 256 | ANN: trainLoss: 0.3524 | trainAcc: 83.4821% (374/448)\n",
            "7 13 Epoch: 256 | ANN: trainLoss: 0.3566 | trainAcc: 83.3984% (427/512)\n",
            "8 13 Epoch: 256 | ANN: trainLoss: 0.3461 | trainAcc: 83.8542% (483/576)\n",
            "9 13 Epoch: 256 | ANN: trainLoss: 0.3493 | trainAcc: 83.7500% (536/640)\n",
            "10 13 Epoch: 256 | ANN: trainLoss: 0.3470 | trainAcc: 83.5227% (588/704)\n",
            "11 13 Epoch: 256 | ANN: trainLoss: 0.3553 | trainAcc: 82.8125% (636/768)\n",
            "12 13 Epoch: 256 | ANN: trainLoss: 0.3463 | trainAcc: 82.8794% (639/771)\n",
            "0 4 Epoch: 256 | ANN: testLoss: 0.4967 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 256 | ANN: testLoss: 0.5176 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 256 | ANN: testLoss: 0.5014 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 256 | ANN: testLoss: 0.4472 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 257 | ANN: trainLoss: 0.5011 | trainAcc: 71.8750% (46/64)\n",
            "1 13 Epoch: 257 | ANN: trainLoss: 0.4099 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 257 | ANN: trainLoss: 0.3826 | trainAcc: 78.6458% (151/192)\n",
            "3 13 Epoch: 257 | ANN: trainLoss: 0.3612 | trainAcc: 80.4688% (206/256)\n",
            "4 13 Epoch: 257 | ANN: trainLoss: 0.3504 | trainAcc: 81.2500% (260/320)\n",
            "5 13 Epoch: 257 | ANN: trainLoss: 0.3418 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 257 | ANN: trainLoss: 0.3339 | trainAcc: 83.2589% (373/448)\n",
            "7 13 Epoch: 257 | ANN: trainLoss: 0.3404 | trainAcc: 82.4219% (422/512)\n",
            "8 13 Epoch: 257 | ANN: trainLoss: 0.3403 | trainAcc: 82.4653% (475/576)\n",
            "9 13 Epoch: 257 | ANN: trainLoss: 0.3320 | trainAcc: 83.2812% (533/640)\n",
            "10 13 Epoch: 257 | ANN: trainLoss: 0.3347 | trainAcc: 83.3807% (587/704)\n",
            "11 13 Epoch: 257 | ANN: trainLoss: 0.3335 | trainAcc: 83.2031% (639/768)\n",
            "12 13 Epoch: 257 | ANN: trainLoss: 0.3514 | trainAcc: 83.1388% (641/771)\n",
            "0 4 Epoch: 257 | ANN: testLoss: 0.3820 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 257 | ANN: testLoss: 0.4922 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 257 | ANN: testLoss: 0.4949 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 257 | ANN: testLoss: 0.3711 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 258 | ANN: trainLoss: 0.2910 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 258 | ANN: trainLoss: 0.2674 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 258 | ANN: trainLoss: 0.3163 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 258 | ANN: trainLoss: 0.3179 | trainAcc: 85.1562% (218/256)\n",
            "4 13 Epoch: 258 | ANN: trainLoss: 0.3179 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 258 | ANN: trainLoss: 0.3201 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 258 | ANN: trainLoss: 0.3403 | trainAcc: 83.7054% (375/448)\n",
            "7 13 Epoch: 258 | ANN: trainLoss: 0.3461 | trainAcc: 83.0078% (425/512)\n",
            "8 13 Epoch: 258 | ANN: trainLoss: 0.3459 | trainAcc: 83.1597% (479/576)\n",
            "9 13 Epoch: 258 | ANN: trainLoss: 0.3485 | trainAcc: 83.1250% (532/640)\n",
            "10 13 Epoch: 258 | ANN: trainLoss: 0.3407 | trainAcc: 83.6648% (589/704)\n",
            "11 13 Epoch: 258 | ANN: trainLoss: 0.3387 | trainAcc: 83.7240% (643/768)\n",
            "12 13 Epoch: 258 | ANN: trainLoss: 0.3569 | trainAcc: 83.6576% (645/771)\n",
            "0 4 Epoch: 258 | ANN: testLoss: 0.6327 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 258 | ANN: testLoss: 0.5136 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 258 | ANN: testLoss: 0.5124 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 258 | ANN: testLoss: 0.4549 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 259 | ANN: trainLoss: 0.5478 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 259 | ANN: trainLoss: 0.3990 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 259 | ANN: trainLoss: 0.3837 | trainAcc: 82.8125% (159/192)\n",
            "3 13 Epoch: 259 | ANN: trainLoss: 0.3749 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 259 | ANN: trainLoss: 0.3663 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 259 | ANN: trainLoss: 0.3527 | trainAcc: 84.6354% (325/384)\n",
            "6 13 Epoch: 259 | ANN: trainLoss: 0.3422 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 259 | ANN: trainLoss: 0.3403 | trainAcc: 84.3750% (432/512)\n",
            "8 13 Epoch: 259 | ANN: trainLoss: 0.3544 | trainAcc: 83.5069% (481/576)\n",
            "9 13 Epoch: 259 | ANN: trainLoss: 0.3455 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 259 | ANN: trainLoss: 0.3403 | trainAcc: 83.9489% (591/704)\n",
            "11 13 Epoch: 259 | ANN: trainLoss: 0.3458 | trainAcc: 83.7240% (643/768)\n",
            "12 13 Epoch: 259 | ANN: trainLoss: 0.3467 | trainAcc: 83.7873% (646/771)\n",
            "0 4 Epoch: 259 | ANN: testLoss: 0.6083 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 259 | ANN: testLoss: 0.5067 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 259 | ANN: testLoss: 0.4838 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 259 | ANN: testLoss: 0.3629 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 260 | ANN: trainLoss: 0.3366 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 260 | ANN: trainLoss: 0.3820 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 260 | ANN: trainLoss: 0.3682 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 260 | ANN: trainLoss: 0.3444 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 260 | ANN: trainLoss: 0.3437 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 260 | ANN: trainLoss: 0.3529 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 260 | ANN: trainLoss: 0.3489 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 260 | ANN: trainLoss: 0.3510 | trainAcc: 85.1562% (436/512)\n",
            "8 13 Epoch: 260 | ANN: trainLoss: 0.3498 | trainAcc: 85.0694% (490/576)\n",
            "9 13 Epoch: 260 | ANN: trainLoss: 0.3421 | trainAcc: 85.3125% (546/640)\n",
            "10 13 Epoch: 260 | ANN: trainLoss: 0.3289 | trainAcc: 86.0795% (606/704)\n",
            "11 13 Epoch: 260 | ANN: trainLoss: 0.3267 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 260 | ANN: trainLoss: 0.3259 | trainAcc: 86.3813% (666/771)\n",
            "0 4 Epoch: 260 | ANN: testLoss: 0.5253 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 260 | ANN: testLoss: 0.5479 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 260 | ANN: testLoss: 0.4770 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 260 | ANN: testLoss: 0.6193 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 261 | ANN: trainLoss: 0.3764 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 261 | ANN: trainLoss: 0.3641 | trainAcc: 78.9062% (101/128)\n",
            "2 13 Epoch: 261 | ANN: trainLoss: 0.3450 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 261 | ANN: trainLoss: 0.3383 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 261 | ANN: trainLoss: 0.3439 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 261 | ANN: trainLoss: 0.3561 | trainAcc: 82.5521% (317/384)\n",
            "6 13 Epoch: 261 | ANN: trainLoss: 0.3425 | trainAcc: 83.2589% (373/448)\n",
            "7 13 Epoch: 261 | ANN: trainLoss: 0.3444 | trainAcc: 83.0078% (425/512)\n",
            "8 13 Epoch: 261 | ANN: trainLoss: 0.3351 | trainAcc: 83.3333% (480/576)\n",
            "9 13 Epoch: 261 | ANN: trainLoss: 0.3304 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 261 | ANN: trainLoss: 0.3411 | trainAcc: 83.3807% (587/704)\n",
            "11 13 Epoch: 261 | ANN: trainLoss: 0.3461 | trainAcc: 82.8125% (636/768)\n",
            "12 13 Epoch: 261 | ANN: trainLoss: 0.3632 | trainAcc: 82.7497% (638/771)\n",
            "0 4 Epoch: 261 | ANN: testLoss: 0.3841 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 261 | ANN: testLoss: 0.4116 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 261 | ANN: testLoss: 0.4774 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 261 | ANN: testLoss: 0.5146 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 262 | ANN: trainLoss: 0.2929 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 262 | ANN: trainLoss: 0.3168 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 262 | ANN: trainLoss: 0.3217 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 262 | ANN: trainLoss: 0.3259 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 262 | ANN: trainLoss: 0.3455 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 262 | ANN: trainLoss: 0.3430 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 262 | ANN: trainLoss: 0.3450 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 262 | ANN: trainLoss: 0.3475 | trainAcc: 83.9844% (430/512)\n",
            "8 13 Epoch: 262 | ANN: trainLoss: 0.3483 | trainAcc: 83.8542% (483/576)\n",
            "9 13 Epoch: 262 | ANN: trainLoss: 0.3480 | trainAcc: 84.0625% (538/640)\n",
            "10 13 Epoch: 262 | ANN: trainLoss: 0.3477 | trainAcc: 83.9489% (591/704)\n",
            "11 13 Epoch: 262 | ANN: trainLoss: 0.3471 | trainAcc: 83.7240% (643/768)\n",
            "12 13 Epoch: 262 | ANN: trainLoss: 0.3563 | trainAcc: 83.6576% (645/771)\n",
            "0 4 Epoch: 262 | ANN: testLoss: 0.5224 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 262 | ANN: testLoss: 0.4624 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 262 | ANN: testLoss: 0.4882 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 262 | ANN: testLoss: 0.3662 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 263 | ANN: trainLoss: 0.4255 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 263 | ANN: trainLoss: 0.4239 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 263 | ANN: trainLoss: 0.3954 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 263 | ANN: trainLoss: 0.3755 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 263 | ANN: trainLoss: 0.3670 | trainAcc: 83.7500% (268/320)\n",
            "5 13 Epoch: 263 | ANN: trainLoss: 0.3573 | trainAcc: 83.5938% (321/384)\n",
            "6 13 Epoch: 263 | ANN: trainLoss: 0.3681 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 263 | ANN: trainLoss: 0.3565 | trainAcc: 83.5938% (428/512)\n",
            "8 13 Epoch: 263 | ANN: trainLoss: 0.3602 | trainAcc: 83.3333% (480/576)\n",
            "9 13 Epoch: 263 | ANN: trainLoss: 0.3703 | trainAcc: 82.5000% (528/640)\n",
            "10 13 Epoch: 263 | ANN: trainLoss: 0.3657 | trainAcc: 82.9545% (584/704)\n",
            "11 13 Epoch: 263 | ANN: trainLoss: 0.3611 | trainAcc: 83.3333% (640/768)\n",
            "12 13 Epoch: 263 | ANN: trainLoss: 0.3833 | trainAcc: 83.2685% (642/771)\n",
            "0 4 Epoch: 263 | ANN: testLoss: 0.4385 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 263 | ANN: testLoss: 0.5134 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 263 | ANN: testLoss: 0.4864 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 263 | ANN: testLoss: 0.5199 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 264 | ANN: trainLoss: 0.3375 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 264 | ANN: trainLoss: 0.3251 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 264 | ANN: trainLoss: 0.3225 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 264 | ANN: trainLoss: 0.3636 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 264 | ANN: trainLoss: 0.3651 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 264 | ANN: trainLoss: 0.3616 | trainAcc: 83.5938% (321/384)\n",
            "6 13 Epoch: 264 | ANN: trainLoss: 0.3811 | trainAcc: 83.0357% (372/448)\n",
            "7 13 Epoch: 264 | ANN: trainLoss: 0.3807 | trainAcc: 83.0078% (425/512)\n",
            "8 13 Epoch: 264 | ANN: trainLoss: 0.3736 | trainAcc: 83.6806% (482/576)\n",
            "9 13 Epoch: 264 | ANN: trainLoss: 0.3826 | trainAcc: 83.5938% (535/640)\n",
            "10 13 Epoch: 264 | ANN: trainLoss: 0.3837 | trainAcc: 84.2330% (593/704)\n",
            "11 13 Epoch: 264 | ANN: trainLoss: 0.3819 | trainAcc: 84.3750% (648/768)\n",
            "12 13 Epoch: 264 | ANN: trainLoss: 0.3654 | trainAcc: 84.4358% (651/771)\n",
            "0 4 Epoch: 264 | ANN: testLoss: 0.5836 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 264 | ANN: testLoss: 0.4902 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 264 | ANN: testLoss: 0.5136 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 264 | ANN: testLoss: 0.3865 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 265 | ANN: trainLoss: 0.3796 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 265 | ANN: trainLoss: 0.3418 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 265 | ANN: trainLoss: 0.3616 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 265 | ANN: trainLoss: 0.3552 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 265 | ANN: trainLoss: 0.3570 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 265 | ANN: trainLoss: 0.3632 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 265 | ANN: trainLoss: 0.3697 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 265 | ANN: trainLoss: 0.3786 | trainAcc: 83.7891% (429/512)\n",
            "8 13 Epoch: 265 | ANN: trainLoss: 0.3689 | trainAcc: 83.8542% (483/576)\n",
            "9 13 Epoch: 265 | ANN: trainLoss: 0.3677 | trainAcc: 83.7500% (536/640)\n",
            "10 13 Epoch: 265 | ANN: trainLoss: 0.3679 | trainAcc: 83.5227% (588/704)\n",
            "11 13 Epoch: 265 | ANN: trainLoss: 0.3677 | trainAcc: 83.4635% (641/768)\n",
            "12 13 Epoch: 265 | ANN: trainLoss: 0.3680 | trainAcc: 83.3982% (643/771)\n",
            "0 4 Epoch: 265 | ANN: testLoss: 0.5857 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 265 | ANN: testLoss: 0.5193 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 265 | ANN: testLoss: 0.5150 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 265 | ANN: testLoss: 0.4327 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 266 | ANN: trainLoss: 0.2977 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 266 | ANN: trainLoss: 0.3550 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 266 | ANN: trainLoss: 0.3689 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 266 | ANN: trainLoss: 0.3879 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 266 | ANN: trainLoss: 0.3746 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 266 | ANN: trainLoss: 0.3834 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 266 | ANN: trainLoss: 0.3828 | trainAcc: 81.9196% (367/448)\n",
            "7 13 Epoch: 266 | ANN: trainLoss: 0.3891 | trainAcc: 81.6406% (418/512)\n",
            "8 13 Epoch: 266 | ANN: trainLoss: 0.3789 | trainAcc: 82.4653% (475/576)\n",
            "9 13 Epoch: 266 | ANN: trainLoss: 0.3799 | trainAcc: 82.1875% (526/640)\n",
            "10 13 Epoch: 266 | ANN: trainLoss: 0.3726 | trainAcc: 82.2443% (579/704)\n",
            "11 13 Epoch: 266 | ANN: trainLoss: 0.3816 | trainAcc: 82.0312% (630/768)\n",
            "12 13 Epoch: 266 | ANN: trainLoss: 0.6293 | trainAcc: 81.8418% (631/771)\n",
            "0 4 Epoch: 266 | ANN: testLoss: 0.5598 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 266 | ANN: testLoss: 0.4668 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 266 | ANN: testLoss: 0.5079 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 266 | ANN: testLoss: 0.5515 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 267 | ANN: trainLoss: 0.2242 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 267 | ANN: trainLoss: 0.2783 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 267 | ANN: trainLoss: 0.3087 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 267 | ANN: trainLoss: 0.3279 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 267 | ANN: trainLoss: 0.3469 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 267 | ANN: trainLoss: 0.3478 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 267 | ANN: trainLoss: 0.3500 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 267 | ANN: trainLoss: 0.3420 | trainAcc: 85.9375% (440/512)\n",
            "8 13 Epoch: 267 | ANN: trainLoss: 0.3355 | trainAcc: 86.1111% (496/576)\n",
            "9 13 Epoch: 267 | ANN: trainLoss: 0.3344 | trainAcc: 85.6250% (548/640)\n",
            "10 13 Epoch: 267 | ANN: trainLoss: 0.3328 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 267 | ANN: trainLoss: 0.3322 | trainAcc: 85.8073% (659/768)\n",
            "12 13 Epoch: 267 | ANN: trainLoss: 0.3365 | trainAcc: 85.8625% (662/771)\n",
            "0 4 Epoch: 267 | ANN: testLoss: 0.3881 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 267 | ANN: testLoss: 0.4564 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 267 | ANN: testLoss: 0.4941 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 267 | ANN: testLoss: 0.3707 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 268 | ANN: trainLoss: 0.3103 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 268 | ANN: trainLoss: 0.3185 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 268 | ANN: trainLoss: 0.3345 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 268 | ANN: trainLoss: 0.3209 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 268 | ANN: trainLoss: 0.3331 | trainAcc: 82.5000% (264/320)\n",
            "5 13 Epoch: 268 | ANN: trainLoss: 0.3663 | trainAcc: 80.9896% (311/384)\n",
            "6 13 Epoch: 268 | ANN: trainLoss: 0.3750 | trainAcc: 81.0268% (363/448)\n",
            "7 13 Epoch: 268 | ANN: trainLoss: 0.3711 | trainAcc: 81.8359% (419/512)\n",
            "8 13 Epoch: 268 | ANN: trainLoss: 0.3762 | trainAcc: 81.2500% (468/576)\n",
            "9 13 Epoch: 268 | ANN: trainLoss: 0.3793 | trainAcc: 81.0938% (519/640)\n",
            "10 13 Epoch: 268 | ANN: trainLoss: 0.3764 | trainAcc: 81.2500% (572/704)\n",
            "11 13 Epoch: 268 | ANN: trainLoss: 0.3735 | trainAcc: 81.3802% (625/768)\n",
            "12 13 Epoch: 268 | ANN: trainLoss: 0.3701 | trainAcc: 81.3230% (627/771)\n",
            "0 4 Epoch: 268 | ANN: testLoss: 0.5304 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 268 | ANN: testLoss: 0.5375 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 268 | ANN: testLoss: 0.4923 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 268 | ANN: testLoss: 0.4658 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 269 | ANN: trainLoss: 0.4978 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 269 | ANN: trainLoss: 0.4491 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 269 | ANN: trainLoss: 0.4048 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 269 | ANN: trainLoss: 0.3938 | trainAcc: 81.6406% (209/256)\n",
            "4 13 Epoch: 269 | ANN: trainLoss: 0.3708 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 269 | ANN: trainLoss: 0.3751 | trainAcc: 82.8125% (318/384)\n",
            "6 13 Epoch: 269 | ANN: trainLoss: 0.3630 | trainAcc: 83.4821% (374/448)\n",
            "7 13 Epoch: 269 | ANN: trainLoss: 0.3651 | trainAcc: 83.2031% (426/512)\n",
            "8 13 Epoch: 269 | ANN: trainLoss: 0.3522 | trainAcc: 83.8542% (483/576)\n",
            "9 13 Epoch: 269 | ANN: trainLoss: 0.3539 | trainAcc: 83.5938% (535/640)\n",
            "10 13 Epoch: 269 | ANN: trainLoss: 0.3548 | trainAcc: 83.6648% (589/704)\n",
            "11 13 Epoch: 269 | ANN: trainLoss: 0.3534 | trainAcc: 83.8542% (644/768)\n",
            "12 13 Epoch: 269 | ANN: trainLoss: 0.3537 | trainAcc: 83.7873% (646/771)\n",
            "0 4 Epoch: 269 | ANN: testLoss: 0.5462 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 269 | ANN: testLoss: 0.4699 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 269 | ANN: testLoss: 0.5245 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 269 | ANN: testLoss: 0.5221 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 270 | ANN: trainLoss: 0.4060 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 270 | ANN: trainLoss: 0.3648 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 270 | ANN: trainLoss: 0.3410 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 270 | ANN: trainLoss: 0.3518 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 270 | ANN: trainLoss: 0.3335 | trainAcc: 83.7500% (268/320)\n",
            "5 13 Epoch: 270 | ANN: trainLoss: 0.3249 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 270 | ANN: trainLoss: 0.3229 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 270 | ANN: trainLoss: 0.3131 | trainAcc: 84.9609% (435/512)\n",
            "8 13 Epoch: 270 | ANN: trainLoss: 0.3126 | trainAcc: 84.8958% (489/576)\n",
            "9 13 Epoch: 270 | ANN: trainLoss: 0.3160 | trainAcc: 85.0000% (544/640)\n",
            "10 13 Epoch: 270 | ANN: trainLoss: 0.3191 | trainAcc: 84.9432% (598/704)\n",
            "11 13 Epoch: 270 | ANN: trainLoss: 0.3318 | trainAcc: 84.6354% (650/768)\n",
            "12 13 Epoch: 270 | ANN: trainLoss: 0.3658 | trainAcc: 84.4358% (651/771)\n",
            "0 4 Epoch: 270 | ANN: testLoss: 0.4083 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 270 | ANN: testLoss: 0.4505 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 270 | ANN: testLoss: 0.4956 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 270 | ANN: testLoss: 0.8390 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 271 | ANN: trainLoss: 0.3363 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 271 | ANN: trainLoss: 0.3499 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 271 | ANN: trainLoss: 0.3076 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 271 | ANN: trainLoss: 0.3439 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 271 | ANN: trainLoss: 0.3463 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 271 | ANN: trainLoss: 0.3306 | trainAcc: 84.6354% (325/384)\n",
            "6 13 Epoch: 271 | ANN: trainLoss: 0.3297 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 271 | ANN: trainLoss: 0.3244 | trainAcc: 84.7656% (434/512)\n",
            "8 13 Epoch: 271 | ANN: trainLoss: 0.3226 | trainAcc: 84.7222% (488/576)\n",
            "9 13 Epoch: 271 | ANN: trainLoss: 0.3264 | trainAcc: 84.3750% (540/640)\n",
            "10 13 Epoch: 271 | ANN: trainLoss: 0.3439 | trainAcc: 83.6648% (589/704)\n",
            "11 13 Epoch: 271 | ANN: trainLoss: 0.3549 | trainAcc: 82.6823% (635/768)\n",
            "12 13 Epoch: 271 | ANN: trainLoss: 0.3411 | trainAcc: 82.7497% (638/771)\n",
            "0 4 Epoch: 271 | ANN: testLoss: 0.5176 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 271 | ANN: testLoss: 0.4980 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 271 | ANN: testLoss: 0.4966 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 271 | ANN: testLoss: 0.4991 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 272 | ANN: trainLoss: 0.3870 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 272 | ANN: trainLoss: 0.3462 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 272 | ANN: trainLoss: 0.3346 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 272 | ANN: trainLoss: 0.3221 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 272 | ANN: trainLoss: 0.3525 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 272 | ANN: trainLoss: 0.3706 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 272 | ANN: trainLoss: 0.3723 | trainAcc: 83.4821% (374/448)\n",
            "7 13 Epoch: 272 | ANN: trainLoss: 0.3598 | trainAcc: 83.7891% (429/512)\n",
            "8 13 Epoch: 272 | ANN: trainLoss: 0.3472 | trainAcc: 84.5486% (487/576)\n",
            "9 13 Epoch: 272 | ANN: trainLoss: 0.3429 | trainAcc: 84.8438% (543/640)\n",
            "10 13 Epoch: 272 | ANN: trainLoss: 0.3559 | trainAcc: 84.5170% (595/704)\n",
            "11 13 Epoch: 272 | ANN: trainLoss: 0.3559 | trainAcc: 84.1146% (646/768)\n",
            "12 13 Epoch: 272 | ANN: trainLoss: 0.4657 | trainAcc: 83.9170% (647/771)\n",
            "0 4 Epoch: 272 | ANN: testLoss: 0.5237 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 272 | ANN: testLoss: 0.5112 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 272 | ANN: testLoss: 0.5031 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 272 | ANN: testLoss: 0.3774 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 273 | ANN: trainLoss: 0.3687 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 273 | ANN: trainLoss: 0.3531 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 273 | ANN: trainLoss: 0.3725 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 273 | ANN: trainLoss: 0.3831 | trainAcc: 80.4688% (206/256)\n",
            "4 13 Epoch: 273 | ANN: trainLoss: 0.3939 | trainAcc: 80.9375% (259/320)\n",
            "5 13 Epoch: 273 | ANN: trainLoss: 0.3862 | trainAcc: 81.2500% (312/384)\n",
            "6 13 Epoch: 273 | ANN: trainLoss: 0.3792 | trainAcc: 81.9196% (367/448)\n",
            "7 13 Epoch: 273 | ANN: trainLoss: 0.3736 | trainAcc: 83.0078% (425/512)\n",
            "8 13 Epoch: 273 | ANN: trainLoss: 0.3611 | trainAcc: 83.6806% (482/576)\n",
            "9 13 Epoch: 273 | ANN: trainLoss: 0.3633 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 273 | ANN: trainLoss: 0.3585 | trainAcc: 83.6648% (589/704)\n",
            "11 13 Epoch: 273 | ANN: trainLoss: 0.3586 | trainAcc: 83.3333% (640/768)\n",
            "12 13 Epoch: 273 | ANN: trainLoss: 0.4423 | trainAcc: 83.1388% (641/771)\n",
            "0 4 Epoch: 273 | ANN: testLoss: 0.6054 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 273 | ANN: testLoss: 0.5652 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 273 | ANN: testLoss: 0.5288 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 273 | ANN: testLoss: 0.3968 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 274 | ANN: trainLoss: 0.3669 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 274 | ANN: trainLoss: 0.3939 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 274 | ANN: trainLoss: 0.3654 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 274 | ANN: trainLoss: 0.3465 | trainAcc: 85.1562% (218/256)\n",
            "4 13 Epoch: 274 | ANN: trainLoss: 0.3295 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 274 | ANN: trainLoss: 0.3250 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 274 | ANN: trainLoss: 0.3240 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 274 | ANN: trainLoss: 0.3330 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 274 | ANN: trainLoss: 0.3390 | trainAcc: 84.8958% (489/576)\n",
            "9 13 Epoch: 274 | ANN: trainLoss: 0.3362 | trainAcc: 85.1562% (545/640)\n",
            "10 13 Epoch: 274 | ANN: trainLoss: 0.3315 | trainAcc: 84.9432% (598/704)\n",
            "11 13 Epoch: 274 | ANN: trainLoss: 0.3330 | trainAcc: 84.7656% (651/768)\n",
            "12 13 Epoch: 274 | ANN: trainLoss: 0.3353 | trainAcc: 84.6952% (653/771)\n",
            "0 4 Epoch: 274 | ANN: testLoss: 0.4713 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 274 | ANN: testLoss: 0.5184 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 274 | ANN: testLoss: 0.5223 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 274 | ANN: testLoss: 0.5269 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 275 | ANN: trainLoss: 0.3152 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 275 | ANN: trainLoss: 0.3883 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 275 | ANN: trainLoss: 0.3688 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 275 | ANN: trainLoss: 0.3576 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 275 | ANN: trainLoss: 0.3655 | trainAcc: 83.7500% (268/320)\n",
            "5 13 Epoch: 275 | ANN: trainLoss: 0.3725 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 275 | ANN: trainLoss: 0.3684 | trainAcc: 83.2589% (373/448)\n",
            "7 13 Epoch: 275 | ANN: trainLoss: 0.3711 | trainAcc: 83.5938% (428/512)\n",
            "8 13 Epoch: 275 | ANN: trainLoss: 0.3603 | trainAcc: 83.8542% (483/576)\n",
            "9 13 Epoch: 275 | ANN: trainLoss: 0.3633 | trainAcc: 83.7500% (536/640)\n",
            "10 13 Epoch: 275 | ANN: trainLoss: 0.3587 | trainAcc: 83.6648% (589/704)\n",
            "11 13 Epoch: 275 | ANN: trainLoss: 0.3665 | trainAcc: 83.0729% (638/768)\n",
            "12 13 Epoch: 275 | ANN: trainLoss: 0.3886 | trainAcc: 82.8794% (639/771)\n",
            "0 4 Epoch: 275 | ANN: testLoss: 0.4650 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 275 | ANN: testLoss: 0.5165 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 275 | ANN: testLoss: 0.5247 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 275 | ANN: testLoss: 0.3938 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 276 | ANN: trainLoss: 0.4746 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 276 | ANN: trainLoss: 0.3864 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 276 | ANN: trainLoss: 0.4279 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 276 | ANN: trainLoss: 0.3743 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 276 | ANN: trainLoss: 0.3839 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 276 | ANN: trainLoss: 0.3692 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 276 | ANN: trainLoss: 0.3657 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 276 | ANN: trainLoss: 0.3576 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 276 | ANN: trainLoss: 0.3547 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 276 | ANN: trainLoss: 0.3418 | trainAcc: 86.2500% (552/640)\n",
            "10 13 Epoch: 276 | ANN: trainLoss: 0.3491 | trainAcc: 85.2273% (600/704)\n",
            "11 13 Epoch: 276 | ANN: trainLoss: 0.3435 | trainAcc: 85.6771% (658/768)\n",
            "12 13 Epoch: 276 | ANN: trainLoss: 0.3866 | trainAcc: 85.4734% (659/771)\n",
            "0 4 Epoch: 276 | ANN: testLoss: 0.4515 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 276 | ANN: testLoss: 0.5077 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 276 | ANN: testLoss: 0.5204 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 276 | ANN: testLoss: 0.5808 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 277 | ANN: trainLoss: 0.3867 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 277 | ANN: trainLoss: 0.4044 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 277 | ANN: trainLoss: 0.3756 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 277 | ANN: trainLoss: 0.3661 | trainAcc: 80.8594% (207/256)\n",
            "4 13 Epoch: 277 | ANN: trainLoss: 0.3579 | trainAcc: 80.9375% (259/320)\n",
            "5 13 Epoch: 277 | ANN: trainLoss: 0.3463 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 277 | ANN: trainLoss: 0.3451 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 277 | ANN: trainLoss: 0.3510 | trainAcc: 82.8125% (424/512)\n",
            "8 13 Epoch: 277 | ANN: trainLoss: 0.3501 | trainAcc: 83.1597% (479/576)\n",
            "9 13 Epoch: 277 | ANN: trainLoss: 0.3537 | trainAcc: 83.5938% (535/640)\n",
            "10 13 Epoch: 277 | ANN: trainLoss: 0.3588 | trainAcc: 83.5227% (588/704)\n",
            "11 13 Epoch: 277 | ANN: trainLoss: 0.3553 | trainAcc: 83.5938% (642/768)\n",
            "12 13 Epoch: 277 | ANN: trainLoss: 0.3530 | trainAcc: 83.5279% (644/771)\n",
            "0 4 Epoch: 277 | ANN: testLoss: 0.5621 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 277 | ANN: testLoss: 0.4865 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 277 | ANN: testLoss: 0.4838 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 277 | ANN: testLoss: 0.6159 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 278 | ANN: trainLoss: 0.3995 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 278 | ANN: trainLoss: 0.3820 | trainAcc: 77.3438% (99/128)\n",
            "2 13 Epoch: 278 | ANN: trainLoss: 0.4137 | trainAcc: 76.5625% (147/192)\n",
            "3 13 Epoch: 278 | ANN: trainLoss: 0.4186 | trainAcc: 76.5625% (196/256)\n",
            "4 13 Epoch: 278 | ANN: trainLoss: 0.4068 | trainAcc: 78.7500% (252/320)\n",
            "5 13 Epoch: 278 | ANN: trainLoss: 0.3777 | trainAcc: 81.5104% (313/384)\n",
            "6 13 Epoch: 278 | ANN: trainLoss: 0.3714 | trainAcc: 81.9196% (367/448)\n",
            "7 13 Epoch: 278 | ANN: trainLoss: 0.3797 | trainAcc: 81.6406% (418/512)\n",
            "8 13 Epoch: 278 | ANN: trainLoss: 0.3719 | trainAcc: 82.1181% (473/576)\n",
            "9 13 Epoch: 278 | ANN: trainLoss: 0.3727 | trainAcc: 82.1875% (526/640)\n",
            "10 13 Epoch: 278 | ANN: trainLoss: 0.3815 | trainAcc: 81.2500% (572/704)\n",
            "11 13 Epoch: 278 | ANN: trainLoss: 0.3716 | trainAcc: 81.9010% (629/768)\n",
            "12 13 Epoch: 278 | ANN: trainLoss: 0.4372 | trainAcc: 81.8418% (631/771)\n",
            "0 4 Epoch: 278 | ANN: testLoss: 0.4693 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 278 | ANN: testLoss: 0.4346 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 278 | ANN: testLoss: 0.4969 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 278 | ANN: testLoss: 0.6534 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 279 | ANN: trainLoss: 0.3163 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 279 | ANN: trainLoss: 0.3692 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 279 | ANN: trainLoss: 0.3418 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 279 | ANN: trainLoss: 0.3545 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 279 | ANN: trainLoss: 0.3486 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 279 | ANN: trainLoss: 0.3484 | trainAcc: 84.6354% (325/384)\n",
            "6 13 Epoch: 279 | ANN: trainLoss: 0.3442 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 279 | ANN: trainLoss: 0.3500 | trainAcc: 83.9844% (430/512)\n",
            "8 13 Epoch: 279 | ANN: trainLoss: 0.3460 | trainAcc: 83.8542% (483/576)\n",
            "9 13 Epoch: 279 | ANN: trainLoss: 0.3486 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 279 | ANN: trainLoss: 0.3494 | trainAcc: 83.6648% (589/704)\n",
            "11 13 Epoch: 279 | ANN: trainLoss: 0.3473 | trainAcc: 83.8542% (644/768)\n",
            "12 13 Epoch: 279 | ANN: trainLoss: 0.3323 | trainAcc: 83.9170% (647/771)\n",
            "0 4 Epoch: 279 | ANN: testLoss: 0.5292 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 279 | ANN: testLoss: 0.4994 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 279 | ANN: testLoss: 0.5048 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 279 | ANN: testLoss: 0.4678 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 280 | ANN: trainLoss: 0.3295 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 280 | ANN: trainLoss: 0.3530 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 280 | ANN: trainLoss: 0.3247 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 280 | ANN: trainLoss: 0.3113 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 280 | ANN: trainLoss: 0.3184 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 280 | ANN: trainLoss: 0.3145 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 280 | ANN: trainLoss: 0.3001 | trainAcc: 87.2768% (391/448)\n",
            "7 13 Epoch: 280 | ANN: trainLoss: 0.3081 | trainAcc: 86.7188% (444/512)\n",
            "8 13 Epoch: 280 | ANN: trainLoss: 0.3255 | trainAcc: 85.0694% (490/576)\n",
            "9 13 Epoch: 280 | ANN: trainLoss: 0.3316 | trainAcc: 85.4688% (547/640)\n",
            "10 13 Epoch: 280 | ANN: trainLoss: 0.3358 | trainAcc: 85.2273% (600/704)\n",
            "11 13 Epoch: 280 | ANN: trainLoss: 0.3481 | trainAcc: 84.7656% (651/768)\n",
            "12 13 Epoch: 280 | ANN: trainLoss: 0.4531 | trainAcc: 84.5655% (652/771)\n",
            "0 4 Epoch: 280 | ANN: testLoss: 0.5591 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 280 | ANN: testLoss: 0.5064 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 280 | ANN: testLoss: 0.5052 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 280 | ANN: testLoss: 0.6321 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 281 | ANN: trainLoss: 0.3749 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 281 | ANN: trainLoss: 0.3784 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 281 | ANN: trainLoss: 0.3895 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 281 | ANN: trainLoss: 0.3690 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 281 | ANN: trainLoss: 0.3548 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 281 | ANN: trainLoss: 0.3655 | trainAcc: 82.2917% (316/384)\n",
            "6 13 Epoch: 281 | ANN: trainLoss: 0.3476 | trainAcc: 83.2589% (373/448)\n",
            "7 13 Epoch: 281 | ANN: trainLoss: 0.3428 | trainAcc: 83.3984% (427/512)\n",
            "8 13 Epoch: 281 | ANN: trainLoss: 0.3339 | trainAcc: 84.5486% (487/576)\n",
            "9 13 Epoch: 281 | ANN: trainLoss: 0.3393 | trainAcc: 84.3750% (540/640)\n",
            "10 13 Epoch: 281 | ANN: trainLoss: 0.3480 | trainAcc: 83.8068% (590/704)\n",
            "11 13 Epoch: 281 | ANN: trainLoss: 0.3503 | trainAcc: 83.5938% (642/768)\n",
            "12 13 Epoch: 281 | ANN: trainLoss: 0.3306 | trainAcc: 83.6576% (645/771)\n",
            "0 4 Epoch: 281 | ANN: testLoss: 0.6046 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 281 | ANN: testLoss: 0.5168 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 281 | ANN: testLoss: 0.5046 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 281 | ANN: testLoss: 0.6258 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 282 | ANN: trainLoss: 0.4443 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 282 | ANN: trainLoss: 0.3973 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 282 | ANN: trainLoss: 0.3576 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 282 | ANN: trainLoss: 0.3561 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 282 | ANN: trainLoss: 0.3696 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 282 | ANN: trainLoss: 0.3551 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 282 | ANN: trainLoss: 0.3539 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 282 | ANN: trainLoss: 0.3547 | trainAcc: 84.5703% (433/512)\n",
            "8 13 Epoch: 282 | ANN: trainLoss: 0.3481 | trainAcc: 84.7222% (488/576)\n",
            "9 13 Epoch: 282 | ANN: trainLoss: 0.3540 | trainAcc: 84.2188% (539/640)\n",
            "10 13 Epoch: 282 | ANN: trainLoss: 0.3540 | trainAcc: 83.9489% (591/704)\n",
            "11 13 Epoch: 282 | ANN: trainLoss: 0.3594 | trainAcc: 83.7240% (643/768)\n",
            "12 13 Epoch: 282 | ANN: trainLoss: 0.4338 | trainAcc: 83.5279% (644/771)\n",
            "0 4 Epoch: 282 | ANN: testLoss: 0.4208 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 282 | ANN: testLoss: 0.4262 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 282 | ANN: testLoss: 0.4837 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 282 | ANN: testLoss: 0.5934 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 283 | ANN: trainLoss: 0.2422 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 283 | ANN: trainLoss: 0.3046 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 283 | ANN: trainLoss: 0.3164 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 283 | ANN: trainLoss: 0.3070 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 283 | ANN: trainLoss: 0.3126 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 283 | ANN: trainLoss: 0.3554 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 283 | ANN: trainLoss: 0.3543 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 283 | ANN: trainLoss: 0.3690 | trainAcc: 83.0078% (425/512)\n",
            "8 13 Epoch: 283 | ANN: trainLoss: 0.3706 | trainAcc: 83.5069% (481/576)\n",
            "9 13 Epoch: 283 | ANN: trainLoss: 0.3627 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 283 | ANN: trainLoss: 0.3708 | trainAcc: 83.5227% (588/704)\n",
            "11 13 Epoch: 283 | ANN: trainLoss: 0.3754 | trainAcc: 82.8125% (636/768)\n",
            "12 13 Epoch: 283 | ANN: trainLoss: 0.4786 | trainAcc: 82.6200% (637/771)\n",
            "0 4 Epoch: 283 | ANN: testLoss: 0.5543 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 283 | ANN: testLoss: 0.4861 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 283 | ANN: testLoss: 0.4858 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 283 | ANN: testLoss: 0.8063 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 284 | ANN: trainLoss: 0.3451 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 284 | ANN: trainLoss: 0.3436 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 284 | ANN: trainLoss: 0.3466 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 284 | ANN: trainLoss: 0.3438 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 284 | ANN: trainLoss: 0.3611 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 284 | ANN: trainLoss: 0.3709 | trainAcc: 82.8125% (318/384)\n",
            "6 13 Epoch: 284 | ANN: trainLoss: 0.3642 | trainAcc: 83.0357% (372/448)\n",
            "7 13 Epoch: 284 | ANN: trainLoss: 0.3837 | trainAcc: 81.4453% (417/512)\n",
            "8 13 Epoch: 284 | ANN: trainLoss: 0.3960 | trainAcc: 81.0764% (467/576)\n",
            "9 13 Epoch: 284 | ANN: trainLoss: 0.3995 | trainAcc: 80.9375% (518/640)\n",
            "10 13 Epoch: 284 | ANN: trainLoss: 0.3975 | trainAcc: 80.8239% (569/704)\n",
            "11 13 Epoch: 284 | ANN: trainLoss: 0.4023 | trainAcc: 80.9896% (622/768)\n",
            "12 13 Epoch: 284 | ANN: trainLoss: 0.4655 | trainAcc: 80.9339% (624/771)\n",
            "0 4 Epoch: 284 | ANN: testLoss: 0.4316 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 284 | ANN: testLoss: 0.4681 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 284 | ANN: testLoss: 0.4870 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 284 | ANN: testLoss: 0.6288 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 285 | ANN: trainLoss: 0.3913 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 285 | ANN: trainLoss: 0.4366 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 285 | ANN: trainLoss: 0.4505 | trainAcc: 78.1250% (150/192)\n",
            "3 13 Epoch: 285 | ANN: trainLoss: 0.4523 | trainAcc: 79.2969% (203/256)\n",
            "4 13 Epoch: 285 | ANN: trainLoss: 0.4388 | trainAcc: 79.3750% (254/320)\n",
            "5 13 Epoch: 285 | ANN: trainLoss: 0.4302 | trainAcc: 79.9479% (307/384)\n",
            "6 13 Epoch: 285 | ANN: trainLoss: 0.4301 | trainAcc: 79.6875% (357/448)\n",
            "7 13 Epoch: 285 | ANN: trainLoss: 0.4122 | trainAcc: 80.6641% (413/512)\n",
            "8 13 Epoch: 285 | ANN: trainLoss: 0.4068 | trainAcc: 80.7292% (465/576)\n",
            "9 13 Epoch: 285 | ANN: trainLoss: 0.3970 | trainAcc: 80.7812% (517/640)\n",
            "10 13 Epoch: 285 | ANN: trainLoss: 0.3940 | trainAcc: 80.6818% (568/704)\n",
            "11 13 Epoch: 285 | ANN: trainLoss: 0.3892 | trainAcc: 81.2500% (624/768)\n",
            "12 13 Epoch: 285 | ANN: trainLoss: 0.4823 | trainAcc: 80.9339% (624/771)\n",
            "0 4 Epoch: 285 | ANN: testLoss: 0.4715 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 285 | ANN: testLoss: 0.4940 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 285 | ANN: testLoss: 0.4966 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 285 | ANN: testLoss: 0.4503 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 286 | ANN: trainLoss: 0.3874 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 286 | ANN: trainLoss: 0.3881 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 286 | ANN: trainLoss: 0.3581 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 286 | ANN: trainLoss: 0.3882 | trainAcc: 82.0312% (210/256)\n",
            "4 13 Epoch: 286 | ANN: trainLoss: 0.3658 | trainAcc: 84.3750% (270/320)\n",
            "5 13 Epoch: 286 | ANN: trainLoss: 0.3707 | trainAcc: 83.5938% (321/384)\n",
            "6 13 Epoch: 286 | ANN: trainLoss: 0.3632 | trainAcc: 83.4821% (374/448)\n",
            "7 13 Epoch: 286 | ANN: trainLoss: 0.3596 | trainAcc: 83.5938% (428/512)\n",
            "8 13 Epoch: 286 | ANN: trainLoss: 0.3681 | trainAcc: 82.8125% (477/576)\n",
            "9 13 Epoch: 286 | ANN: trainLoss: 0.3587 | trainAcc: 83.7500% (536/640)\n",
            "10 13 Epoch: 286 | ANN: trainLoss: 0.3713 | trainAcc: 82.6705% (582/704)\n",
            "11 13 Epoch: 286 | ANN: trainLoss: 0.3723 | trainAcc: 82.8125% (636/768)\n",
            "12 13 Epoch: 286 | ANN: trainLoss: 0.3583 | trainAcc: 82.8794% (639/771)\n",
            "0 4 Epoch: 286 | ANN: testLoss: 0.4808 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 286 | ANN: testLoss: 0.5368 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 286 | ANN: testLoss: 0.5134 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 286 | ANN: testLoss: 0.4669 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 287 | ANN: trainLoss: 0.3021 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 287 | ANN: trainLoss: 0.3739 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 287 | ANN: trainLoss: 0.4102 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 287 | ANN: trainLoss: 0.3827 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 287 | ANN: trainLoss: 0.3945 | trainAcc: 83.7500% (268/320)\n",
            "5 13 Epoch: 287 | ANN: trainLoss: 0.3825 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 287 | ANN: trainLoss: 0.3779 | trainAcc: 83.4821% (374/448)\n",
            "7 13 Epoch: 287 | ANN: trainLoss: 0.3717 | trainAcc: 83.7891% (429/512)\n",
            "8 13 Epoch: 287 | ANN: trainLoss: 0.3660 | trainAcc: 84.0278% (484/576)\n",
            "9 13 Epoch: 287 | ANN: trainLoss: 0.3686 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 287 | ANN: trainLoss: 0.3702 | trainAcc: 83.3807% (587/704)\n",
            "11 13 Epoch: 287 | ANN: trainLoss: 0.3726 | trainAcc: 83.0729% (638/768)\n",
            "12 13 Epoch: 287 | ANN: trainLoss: 0.4251 | trainAcc: 82.8794% (639/771)\n",
            "0 4 Epoch: 287 | ANN: testLoss: 0.5583 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 287 | ANN: testLoss: 0.5301 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 287 | ANN: testLoss: 0.5084 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 287 | ANN: testLoss: 0.4604 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 288 | ANN: trainLoss: 0.3642 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 288 | ANN: trainLoss: 0.3897 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 288 | ANN: trainLoss: 0.3665 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 288 | ANN: trainLoss: 0.3647 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 288 | ANN: trainLoss: 0.3445 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 288 | ANN: trainLoss: 0.3350 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 288 | ANN: trainLoss: 0.3306 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 288 | ANN: trainLoss: 0.3199 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 288 | ANN: trainLoss: 0.3296 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 288 | ANN: trainLoss: 0.3356 | trainAcc: 85.9375% (550/640)\n",
            "10 13 Epoch: 288 | ANN: trainLoss: 0.3361 | trainAcc: 85.9375% (605/704)\n",
            "11 13 Epoch: 288 | ANN: trainLoss: 0.3435 | trainAcc: 85.4167% (656/768)\n",
            "12 13 Epoch: 288 | ANN: trainLoss: 0.3276 | trainAcc: 85.4734% (659/771)\n",
            "0 4 Epoch: 288 | ANN: testLoss: 0.6204 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 288 | ANN: testLoss: 0.5856 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 288 | ANN: testLoss: 0.5372 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 288 | ANN: testLoss: 0.7443 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 289 | ANN: trainLoss: 0.4798 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 289 | ANN: trainLoss: 0.4599 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 289 | ANN: trainLoss: 0.4116 | trainAcc: 78.6458% (151/192)\n",
            "3 13 Epoch: 289 | ANN: trainLoss: 0.3816 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 289 | ANN: trainLoss: 0.3975 | trainAcc: 80.9375% (259/320)\n",
            "5 13 Epoch: 289 | ANN: trainLoss: 0.3820 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 289 | ANN: trainLoss: 0.3786 | trainAcc: 81.9196% (367/448)\n",
            "7 13 Epoch: 289 | ANN: trainLoss: 0.3731 | trainAcc: 82.2266% (421/512)\n",
            "8 13 Epoch: 289 | ANN: trainLoss: 0.3712 | trainAcc: 81.9444% (472/576)\n",
            "9 13 Epoch: 289 | ANN: trainLoss: 0.3748 | trainAcc: 82.0312% (525/640)\n",
            "10 13 Epoch: 289 | ANN: trainLoss: 0.3808 | trainAcc: 82.3864% (580/704)\n",
            "11 13 Epoch: 289 | ANN: trainLoss: 0.3816 | trainAcc: 82.6823% (635/768)\n",
            "12 13 Epoch: 289 | ANN: trainLoss: 0.4023 | trainAcc: 82.6200% (637/771)\n",
            "0 4 Epoch: 289 | ANN: testLoss: 0.5848 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 289 | ANN: testLoss: 0.5351 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 289 | ANN: testLoss: 0.5323 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 289 | ANN: testLoss: 0.6848 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 290 | ANN: trainLoss: 0.3122 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 290 | ANN: trainLoss: 0.3411 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 290 | ANN: trainLoss: 0.3508 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 290 | ANN: trainLoss: 0.3479 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 290 | ANN: trainLoss: 0.3670 | trainAcc: 82.5000% (264/320)\n",
            "5 13 Epoch: 290 | ANN: trainLoss: 0.3638 | trainAcc: 83.0729% (319/384)\n",
            "6 13 Epoch: 290 | ANN: trainLoss: 0.3584 | trainAcc: 82.8125% (371/448)\n",
            "7 13 Epoch: 290 | ANN: trainLoss: 0.3422 | trainAcc: 83.7891% (429/512)\n",
            "8 13 Epoch: 290 | ANN: trainLoss: 0.3394 | trainAcc: 83.6806% (482/576)\n",
            "9 13 Epoch: 290 | ANN: trainLoss: 0.3353 | trainAcc: 83.5938% (535/640)\n",
            "10 13 Epoch: 290 | ANN: trainLoss: 0.3439 | trainAcc: 83.2386% (586/704)\n",
            "11 13 Epoch: 290 | ANN: trainLoss: 0.3481 | trainAcc: 83.0729% (638/768)\n",
            "12 13 Epoch: 290 | ANN: trainLoss: 0.3914 | trainAcc: 82.8794% (639/771)\n",
            "0 4 Epoch: 290 | ANN: testLoss: 0.4106 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 290 | ANN: testLoss: 0.5074 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 290 | ANN: testLoss: 0.5125 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 290 | ANN: testLoss: 0.3844 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 291 | ANN: trainLoss: 0.3448 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 291 | ANN: trainLoss: 0.3601 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 291 | ANN: trainLoss: 0.4128 | trainAcc: 80.2083% (154/192)\n",
            "3 13 Epoch: 291 | ANN: trainLoss: 0.3831 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 291 | ANN: trainLoss: 0.3534 | trainAcc: 84.3750% (270/320)\n",
            "5 13 Epoch: 291 | ANN: trainLoss: 0.3472 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 291 | ANN: trainLoss: 0.3708 | trainAcc: 83.2589% (373/448)\n",
            "7 13 Epoch: 291 | ANN: trainLoss: 0.3710 | trainAcc: 82.8125% (424/512)\n",
            "8 13 Epoch: 291 | ANN: trainLoss: 0.3840 | trainAcc: 82.2917% (474/576)\n",
            "9 13 Epoch: 291 | ANN: trainLoss: 0.3818 | trainAcc: 82.1875% (526/640)\n",
            "10 13 Epoch: 291 | ANN: trainLoss: 0.3776 | trainAcc: 82.5284% (581/704)\n",
            "11 13 Epoch: 291 | ANN: trainLoss: 0.3839 | trainAcc: 82.0312% (630/768)\n",
            "12 13 Epoch: 291 | ANN: trainLoss: 0.3749 | trainAcc: 82.1012% (633/771)\n",
            "0 4 Epoch: 291 | ANN: testLoss: 0.5526 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 291 | ANN: testLoss: 0.5102 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 291 | ANN: testLoss: 0.4986 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 291 | ANN: testLoss: 0.3740 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 292 | ANN: trainLoss: 0.3446 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 292 | ANN: trainLoss: 0.3597 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 292 | ANN: trainLoss: 0.3790 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 292 | ANN: trainLoss: 0.3742 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 292 | ANN: trainLoss: 0.3702 | trainAcc: 81.5625% (261/320)\n",
            "5 13 Epoch: 292 | ANN: trainLoss: 0.3703 | trainAcc: 80.2083% (308/384)\n",
            "6 13 Epoch: 292 | ANN: trainLoss: 0.3622 | trainAcc: 81.0268% (363/448)\n",
            "7 13 Epoch: 292 | ANN: trainLoss: 0.3745 | trainAcc: 80.8594% (414/512)\n",
            "8 13 Epoch: 292 | ANN: trainLoss: 0.3744 | trainAcc: 81.4236% (469/576)\n",
            "9 13 Epoch: 292 | ANN: trainLoss: 0.3692 | trainAcc: 81.5625% (522/640)\n",
            "10 13 Epoch: 292 | ANN: trainLoss: 0.3624 | trainAcc: 81.9602% (577/704)\n",
            "11 13 Epoch: 292 | ANN: trainLoss: 0.3639 | trainAcc: 82.1615% (631/768)\n",
            "12 13 Epoch: 292 | ANN: trainLoss: 0.3460 | trainAcc: 82.2309% (634/771)\n",
            "0 4 Epoch: 292 | ANN: testLoss: 0.5305 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 292 | ANN: testLoss: 0.4918 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 292 | ANN: testLoss: 0.5017 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 292 | ANN: testLoss: 0.6734 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 293 | ANN: trainLoss: 0.2620 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 293 | ANN: trainLoss: 0.3090 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 293 | ANN: trainLoss: 0.3085 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 293 | ANN: trainLoss: 0.3271 | trainAcc: 82.0312% (210/256)\n",
            "4 13 Epoch: 293 | ANN: trainLoss: 0.3292 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 293 | ANN: trainLoss: 0.3176 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 293 | ANN: trainLoss: 0.3341 | trainAcc: 83.0357% (372/448)\n",
            "7 13 Epoch: 293 | ANN: trainLoss: 0.3300 | trainAcc: 83.5938% (428/512)\n",
            "8 13 Epoch: 293 | ANN: trainLoss: 0.3351 | trainAcc: 83.8542% (483/576)\n",
            "9 13 Epoch: 293 | ANN: trainLoss: 0.3437 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 293 | ANN: trainLoss: 0.3514 | trainAcc: 83.8068% (590/704)\n",
            "11 13 Epoch: 293 | ANN: trainLoss: 0.3468 | trainAcc: 84.2448% (647/768)\n",
            "12 13 Epoch: 293 | ANN: trainLoss: 0.3488 | trainAcc: 84.1764% (649/771)\n",
            "0 4 Epoch: 293 | ANN: testLoss: 0.5180 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 293 | ANN: testLoss: 0.4848 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 293 | ANN: testLoss: 0.4981 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 293 | ANN: testLoss: 0.3920 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 294 | ANN: trainLoss: 0.3843 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 294 | ANN: trainLoss: 0.3458 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 294 | ANN: trainLoss: 0.3222 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 294 | ANN: trainLoss: 0.3310 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 294 | ANN: trainLoss: 0.3425 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 294 | ANN: trainLoss: 0.3417 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 294 | ANN: trainLoss: 0.3378 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 294 | ANN: trainLoss: 0.3421 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 294 | ANN: trainLoss: 0.3403 | trainAcc: 84.5486% (487/576)\n",
            "9 13 Epoch: 294 | ANN: trainLoss: 0.3423 | trainAcc: 84.5312% (541/640)\n",
            "10 13 Epoch: 294 | ANN: trainLoss: 0.3435 | trainAcc: 84.6591% (596/704)\n",
            "11 13 Epoch: 294 | ANN: trainLoss: 0.3500 | trainAcc: 84.1146% (646/768)\n",
            "12 13 Epoch: 294 | ANN: trainLoss: 0.3476 | trainAcc: 84.0467% (648/771)\n",
            "0 4 Epoch: 294 | ANN: testLoss: 0.6052 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 294 | ANN: testLoss: 0.4705 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 294 | ANN: testLoss: 0.4907 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 294 | ANN: testLoss: 0.3682 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 295 | ANN: trainLoss: 0.3444 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 295 | ANN: trainLoss: 0.3141 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 295 | ANN: trainLoss: 0.3265 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 295 | ANN: trainLoss: 0.3092 | trainAcc: 84.7656% (217/256)\n",
            "4 13 Epoch: 295 | ANN: trainLoss: 0.3095 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 295 | ANN: trainLoss: 0.2944 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 295 | ANN: trainLoss: 0.3013 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 295 | ANN: trainLoss: 0.3153 | trainAcc: 84.5703% (433/512)\n",
            "8 13 Epoch: 295 | ANN: trainLoss: 0.3247 | trainAcc: 84.2014% (485/576)\n",
            "9 13 Epoch: 295 | ANN: trainLoss: 0.3227 | trainAcc: 84.6875% (542/640)\n",
            "10 13 Epoch: 295 | ANN: trainLoss: 0.3206 | trainAcc: 84.8011% (597/704)\n",
            "11 13 Epoch: 295 | ANN: trainLoss: 0.3190 | trainAcc: 85.2865% (655/768)\n",
            "12 13 Epoch: 295 | ANN: trainLoss: 0.3222 | trainAcc: 85.3437% (658/771)\n",
            "0 4 Epoch: 295 | ANN: testLoss: 0.4873 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 295 | ANN: testLoss: 0.4710 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 295 | ANN: testLoss: 0.5008 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 295 | ANN: testLoss: 0.3757 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 296 | ANN: trainLoss: 0.4791 | trainAcc: 73.4375% (47/64)\n",
            "1 13 Epoch: 296 | ANN: trainLoss: 0.3910 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 296 | ANN: trainLoss: 0.3400 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 296 | ANN: trainLoss: 0.3436 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 296 | ANN: trainLoss: 0.3222 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 296 | ANN: trainLoss: 0.3166 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 296 | ANN: trainLoss: 0.3115 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 296 | ANN: trainLoss: 0.3307 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 296 | ANN: trainLoss: 0.3205 | trainAcc: 87.1528% (502/576)\n",
            "9 13 Epoch: 296 | ANN: trainLoss: 0.3135 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 296 | ANN: trainLoss: 0.3256 | trainAcc: 86.5057% (609/704)\n",
            "11 13 Epoch: 296 | ANN: trainLoss: 0.3190 | trainAcc: 86.8490% (667/768)\n",
            "12 13 Epoch: 296 | ANN: trainLoss: 0.3677 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 296 | ANN: testLoss: 0.4767 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 296 | ANN: testLoss: 0.4687 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 296 | ANN: testLoss: 0.4945 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 296 | ANN: testLoss: 0.3710 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 297 | ANN: trainLoss: 0.4129 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 297 | ANN: trainLoss: 0.3121 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 297 | ANN: trainLoss: 0.3021 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 297 | ANN: trainLoss: 0.3410 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 297 | ANN: trainLoss: 0.3390 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 297 | ANN: trainLoss: 0.3398 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 297 | ANN: trainLoss: 0.3482 | trainAcc: 83.7054% (375/448)\n",
            "7 13 Epoch: 297 | ANN: trainLoss: 0.3437 | trainAcc: 83.9844% (430/512)\n",
            "8 13 Epoch: 297 | ANN: trainLoss: 0.3502 | trainAcc: 82.9861% (478/576)\n",
            "9 13 Epoch: 297 | ANN: trainLoss: 0.3581 | trainAcc: 82.3438% (527/640)\n",
            "10 13 Epoch: 297 | ANN: trainLoss: 0.3537 | trainAcc: 82.8125% (583/704)\n",
            "11 13 Epoch: 297 | ANN: trainLoss: 0.3483 | trainAcc: 83.3333% (640/768)\n",
            "12 13 Epoch: 297 | ANN: trainLoss: 0.3704 | trainAcc: 83.2685% (642/771)\n",
            "0 4 Epoch: 297 | ANN: testLoss: 0.4786 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 297 | ANN: testLoss: 0.4879 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 297 | ANN: testLoss: 0.4838 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 297 | ANN: testLoss: 0.5853 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 298 | ANN: trainLoss: 0.3533 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 298 | ANN: trainLoss: 0.2914 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 298 | ANN: trainLoss: 0.3470 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 298 | ANN: trainLoss: 0.3683 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 298 | ANN: trainLoss: 0.3726 | trainAcc: 82.1875% (263/320)\n",
            "5 13 Epoch: 298 | ANN: trainLoss: 0.3627 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 298 | ANN: trainLoss: 0.3623 | trainAcc: 82.8125% (371/448)\n",
            "7 13 Epoch: 298 | ANN: trainLoss: 0.3563 | trainAcc: 82.8125% (424/512)\n",
            "8 13 Epoch: 298 | ANN: trainLoss: 0.3596 | trainAcc: 82.6389% (476/576)\n",
            "9 13 Epoch: 298 | ANN: trainLoss: 0.3676 | trainAcc: 82.5000% (528/640)\n",
            "10 13 Epoch: 298 | ANN: trainLoss: 0.3610 | trainAcc: 82.9545% (584/704)\n",
            "11 13 Epoch: 298 | ANN: trainLoss: 0.3533 | trainAcc: 83.0729% (638/768)\n",
            "12 13 Epoch: 298 | ANN: trainLoss: 0.3402 | trainAcc: 83.1388% (641/771)\n",
            "0 4 Epoch: 298 | ANN: testLoss: 0.5439 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 298 | ANN: testLoss: 0.5244 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 298 | ANN: testLoss: 0.4916 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 298 | ANN: testLoss: 0.3689 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 299 | ANN: trainLoss: 0.2122 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 299 | ANN: trainLoss: 0.3053 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 299 | ANN: trainLoss: 0.3222 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 299 | ANN: trainLoss: 0.3410 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 299 | ANN: trainLoss: 0.3528 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 299 | ANN: trainLoss: 0.3343 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 299 | ANN: trainLoss: 0.3358 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 299 | ANN: trainLoss: 0.3404 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 299 | ANN: trainLoss: 0.3342 | trainAcc: 84.0278% (484/576)\n",
            "9 13 Epoch: 299 | ANN: trainLoss: 0.3282 | trainAcc: 84.5312% (541/640)\n",
            "10 13 Epoch: 299 | ANN: trainLoss: 0.3287 | trainAcc: 84.8011% (597/704)\n",
            "11 13 Epoch: 299 | ANN: trainLoss: 0.3420 | trainAcc: 84.1146% (646/768)\n",
            "12 13 Epoch: 299 | ANN: trainLoss: 0.3538 | trainAcc: 84.0467% (648/771)\n",
            "0 4 Epoch: 299 | ANN: testLoss: 0.4658 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 299 | ANN: testLoss: 0.4749 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 299 | ANN: testLoss: 0.4885 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 299 | ANN: testLoss: 0.5714 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 300 | ANN: trainLoss: 0.2900 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 300 | ANN: trainLoss: 0.3053 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 300 | ANN: trainLoss: 0.3314 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 300 | ANN: trainLoss: 0.3229 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 300 | ANN: trainLoss: 0.3025 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 300 | ANN: trainLoss: 0.3268 | trainAcc: 83.5938% (321/384)\n",
            "6 13 Epoch: 300 | ANN: trainLoss: 0.3148 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 300 | ANN: trainLoss: 0.3143 | trainAcc: 84.9609% (435/512)\n",
            "8 13 Epoch: 300 | ANN: trainLoss: 0.3105 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 300 | ANN: trainLoss: 0.3159 | trainAcc: 84.8438% (543/640)\n",
            "10 13 Epoch: 300 | ANN: trainLoss: 0.3148 | trainAcc: 84.9432% (598/704)\n",
            "11 13 Epoch: 300 | ANN: trainLoss: 0.3260 | trainAcc: 84.6354% (650/768)\n",
            "12 13 Epoch: 300 | ANN: trainLoss: 0.3235 | trainAcc: 84.5655% (652/771)\n",
            "0 4 Epoch: 300 | ANN: testLoss: 0.5078 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 300 | ANN: testLoss: 0.5219 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 300 | ANN: testLoss: 0.4934 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 300 | ANN: testLoss: 0.5429 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 301 | ANN: trainLoss: 0.4299 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 301 | ANN: trainLoss: 0.3948 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 301 | ANN: trainLoss: 0.3413 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 301 | ANN: trainLoss: 0.3522 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 301 | ANN: trainLoss: 0.3704 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 301 | ANN: trainLoss: 0.3626 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 301 | ANN: trainLoss: 0.3684 | trainAcc: 83.0357% (372/448)\n",
            "7 13 Epoch: 301 | ANN: trainLoss: 0.3491 | trainAcc: 83.3984% (427/512)\n",
            "8 13 Epoch: 301 | ANN: trainLoss: 0.3512 | trainAcc: 82.6389% (476/576)\n",
            "9 13 Epoch: 301 | ANN: trainLoss: 0.3462 | trainAcc: 82.9688% (531/640)\n",
            "10 13 Epoch: 301 | ANN: trainLoss: 0.3389 | trainAcc: 83.8068% (590/704)\n",
            "11 13 Epoch: 301 | ANN: trainLoss: 0.3353 | trainAcc: 84.1146% (646/768)\n",
            "12 13 Epoch: 301 | ANN: trainLoss: 0.3283 | trainAcc: 84.1764% (649/771)\n",
            "0 4 Epoch: 301 | ANN: testLoss: 0.4801 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 301 | ANN: testLoss: 0.5285 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 301 | ANN: testLoss: 0.5046 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 301 | ANN: testLoss: 0.3785 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 302 | ANN: trainLoss: 0.4812 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 302 | ANN: trainLoss: 0.4176 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 302 | ANN: trainLoss: 0.4025 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 302 | ANN: trainLoss: 0.3635 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 302 | ANN: trainLoss: 0.3693 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 302 | ANN: trainLoss: 0.3659 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 302 | ANN: trainLoss: 0.3764 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 302 | ANN: trainLoss: 0.3657 | trainAcc: 83.3984% (427/512)\n",
            "8 13 Epoch: 302 | ANN: trainLoss: 0.3595 | trainAcc: 84.0278% (484/576)\n",
            "9 13 Epoch: 302 | ANN: trainLoss: 0.3524 | trainAcc: 84.0625% (538/640)\n",
            "10 13 Epoch: 302 | ANN: trainLoss: 0.3455 | trainAcc: 84.5170% (595/704)\n",
            "11 13 Epoch: 302 | ANN: trainLoss: 0.3449 | trainAcc: 84.3750% (648/768)\n",
            "12 13 Epoch: 302 | ANN: trainLoss: 0.3660 | trainAcc: 84.3061% (650/771)\n",
            "0 4 Epoch: 302 | ANN: testLoss: 0.4244 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 302 | ANN: testLoss: 0.5111 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 302 | ANN: testLoss: 0.5007 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 302 | ANN: testLoss: 0.5778 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 303 | ANN: trainLoss: 0.4095 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 303 | ANN: trainLoss: 0.3640 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 303 | ANN: trainLoss: 0.3698 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 303 | ANN: trainLoss: 0.3332 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 303 | ANN: trainLoss: 0.3275 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 303 | ANN: trainLoss: 0.3255 | trainAcc: 82.5521% (317/384)\n",
            "6 13 Epoch: 303 | ANN: trainLoss: 0.3167 | trainAcc: 83.0357% (372/448)\n",
            "7 13 Epoch: 303 | ANN: trainLoss: 0.3315 | trainAcc: 82.2266% (421/512)\n",
            "8 13 Epoch: 303 | ANN: trainLoss: 0.3287 | trainAcc: 82.4653% (475/576)\n",
            "9 13 Epoch: 303 | ANN: trainLoss: 0.3295 | trainAcc: 82.9688% (531/640)\n",
            "10 13 Epoch: 303 | ANN: trainLoss: 0.3200 | trainAcc: 83.5227% (588/704)\n",
            "11 13 Epoch: 303 | ANN: trainLoss: 0.3171 | trainAcc: 83.5938% (642/768)\n",
            "12 13 Epoch: 303 | ANN: trainLoss: 0.3300 | trainAcc: 83.5279% (644/771)\n",
            "0 4 Epoch: 303 | ANN: testLoss: 0.5266 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 303 | ANN: testLoss: 0.5153 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 303 | ANN: testLoss: 0.5059 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 303 | ANN: testLoss: 0.5277 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 304 | ANN: trainLoss: 0.3143 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 304 | ANN: trainLoss: 0.3643 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 304 | ANN: trainLoss: 0.3088 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 304 | ANN: trainLoss: 0.3004 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 304 | ANN: trainLoss: 0.3251 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 304 | ANN: trainLoss: 0.3213 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 304 | ANN: trainLoss: 0.3063 | trainAcc: 86.3839% (387/448)\n",
            "7 13 Epoch: 304 | ANN: trainLoss: 0.3167 | trainAcc: 85.1562% (436/512)\n",
            "8 13 Epoch: 304 | ANN: trainLoss: 0.3115 | trainAcc: 85.5903% (493/576)\n",
            "9 13 Epoch: 304 | ANN: trainLoss: 0.3052 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 304 | ANN: trainLoss: 0.3108 | trainAcc: 86.0795% (606/704)\n",
            "11 13 Epoch: 304 | ANN: trainLoss: 0.3187 | trainAcc: 85.6771% (658/768)\n",
            "12 13 Epoch: 304 | ANN: trainLoss: 0.3105 | trainAcc: 85.7328% (661/771)\n",
            "0 4 Epoch: 304 | ANN: testLoss: 0.4568 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 304 | ANN: testLoss: 0.5395 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 304 | ANN: testLoss: 0.4992 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 304 | ANN: testLoss: 0.5641 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 305 | ANN: trainLoss: 0.3230 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 305 | ANN: trainLoss: 0.2714 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 305 | ANN: trainLoss: 0.2662 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 305 | ANN: trainLoss: 0.2801 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 305 | ANN: trainLoss: 0.2864 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 305 | ANN: trainLoss: 0.3011 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 305 | ANN: trainLoss: 0.3039 | trainAcc: 86.6071% (388/448)\n",
            "7 13 Epoch: 305 | ANN: trainLoss: 0.3051 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 305 | ANN: trainLoss: 0.3173 | trainAcc: 85.4167% (492/576)\n",
            "9 13 Epoch: 305 | ANN: trainLoss: 0.3130 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 305 | ANN: trainLoss: 0.3114 | trainAcc: 85.9375% (605/704)\n",
            "11 13 Epoch: 305 | ANN: trainLoss: 0.3161 | trainAcc: 85.6771% (658/768)\n",
            "12 13 Epoch: 305 | ANN: trainLoss: 0.2955 | trainAcc: 85.7328% (661/771)\n",
            "0 4 Epoch: 305 | ANN: testLoss: 0.5805 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 305 | ANN: testLoss: 0.5100 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 305 | ANN: testLoss: 0.5091 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 305 | ANN: testLoss: 0.5586 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 306 | ANN: trainLoss: 0.2503 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 306 | ANN: trainLoss: 0.2954 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 306 | ANN: trainLoss: 0.3167 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 306 | ANN: trainLoss: 0.3270 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 306 | ANN: trainLoss: 0.3222 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 306 | ANN: trainLoss: 0.3237 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 306 | ANN: trainLoss: 0.3203 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 306 | ANN: trainLoss: 0.3271 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 306 | ANN: trainLoss: 0.3363 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 306 | ANN: trainLoss: 0.3337 | trainAcc: 86.2500% (552/640)\n",
            "10 13 Epoch: 306 | ANN: trainLoss: 0.3323 | trainAcc: 86.2216% (607/704)\n",
            "11 13 Epoch: 306 | ANN: trainLoss: 0.3307 | trainAcc: 86.3281% (663/768)\n",
            "12 13 Epoch: 306 | ANN: trainLoss: 0.3297 | trainAcc: 86.2516% (665/771)\n",
            "0 4 Epoch: 306 | ANN: testLoss: 0.5645 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 306 | ANN: testLoss: 0.5271 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 306 | ANN: testLoss: 0.5100 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 306 | ANN: testLoss: 0.5816 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 307 | ANN: trainLoss: 0.4114 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 307 | ANN: trainLoss: 0.4022 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 307 | ANN: trainLoss: 0.3605 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 307 | ANN: trainLoss: 0.3178 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 307 | ANN: trainLoss: 0.3078 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 307 | ANN: trainLoss: 0.3285 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 307 | ANN: trainLoss: 0.3268 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 307 | ANN: trainLoss: 0.3157 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 307 | ANN: trainLoss: 0.3119 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 307 | ANN: trainLoss: 0.3164 | trainAcc: 87.5000% (560/640)\n",
            "10 13 Epoch: 307 | ANN: trainLoss: 0.3041 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 307 | ANN: trainLoss: 0.3114 | trainAcc: 87.2396% (670/768)\n",
            "12 13 Epoch: 307 | ANN: trainLoss: 0.3168 | trainAcc: 87.1595% (672/771)\n",
            "0 4 Epoch: 307 | ANN: testLoss: 0.4304 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 307 | ANN: testLoss: 0.4719 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 307 | ANN: testLoss: 0.5088 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 307 | ANN: testLoss: 0.3816 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 308 | ANN: trainLoss: 0.5681 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 308 | ANN: trainLoss: 0.4566 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 308 | ANN: trainLoss: 0.3989 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 308 | ANN: trainLoss: 0.3714 | trainAcc: 85.1562% (218/256)\n",
            "4 13 Epoch: 308 | ANN: trainLoss: 0.3458 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 308 | ANN: trainLoss: 0.3579 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 308 | ANN: trainLoss: 0.3450 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 308 | ANN: trainLoss: 0.3348 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 308 | ANN: trainLoss: 0.3232 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 308 | ANN: trainLoss: 0.3243 | trainAcc: 85.3125% (546/640)\n",
            "10 13 Epoch: 308 | ANN: trainLoss: 0.3218 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 308 | ANN: trainLoss: 0.3184 | trainAcc: 85.5469% (657/768)\n",
            "12 13 Epoch: 308 | ANN: trainLoss: 0.3054 | trainAcc: 85.6031% (660/771)\n",
            "0 4 Epoch: 308 | ANN: testLoss: 0.5235 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 308 | ANN: testLoss: 0.5122 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 308 | ANN: testLoss: 0.5113 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 308 | ANN: testLoss: 0.4072 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 309 | ANN: trainLoss: 0.3186 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 309 | ANN: trainLoss: 0.2825 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 309 | ANN: trainLoss: 0.2827 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 309 | ANN: trainLoss: 0.2938 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 309 | ANN: trainLoss: 0.2950 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 309 | ANN: trainLoss: 0.3041 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 309 | ANN: trainLoss: 0.3041 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 309 | ANN: trainLoss: 0.2978 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 309 | ANN: trainLoss: 0.2895 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 309 | ANN: trainLoss: 0.2976 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 309 | ANN: trainLoss: 0.3033 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 309 | ANN: trainLoss: 0.3113 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 309 | ANN: trainLoss: 0.3275 | trainAcc: 86.3813% (666/771)\n",
            "0 4 Epoch: 309 | ANN: testLoss: 0.4477 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 309 | ANN: testLoss: 0.4542 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 309 | ANN: testLoss: 0.5214 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 309 | ANN: testLoss: 0.3910 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 310 | ANN: trainLoss: 0.3784 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 310 | ANN: trainLoss: 0.3584 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 310 | ANN: trainLoss: 0.3416 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 310 | ANN: trainLoss: 0.3383 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 310 | ANN: trainLoss: 0.3643 | trainAcc: 83.7500% (268/320)\n",
            "5 13 Epoch: 310 | ANN: trainLoss: 0.3527 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 310 | ANN: trainLoss: 0.3349 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 310 | ANN: trainLoss: 0.3135 | trainAcc: 85.7422% (439/512)\n",
            "8 13 Epoch: 310 | ANN: trainLoss: 0.3289 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 310 | ANN: trainLoss: 0.3271 | trainAcc: 85.0000% (544/640)\n",
            "10 13 Epoch: 310 | ANN: trainLoss: 0.3154 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 310 | ANN: trainLoss: 0.3193 | trainAcc: 84.8958% (652/768)\n",
            "12 13 Epoch: 310 | ANN: trainLoss: 0.4221 | trainAcc: 84.6952% (653/771)\n",
            "0 4 Epoch: 310 | ANN: testLoss: 0.5331 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 310 | ANN: testLoss: 0.5430 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 310 | ANN: testLoss: 0.5108 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 310 | ANN: testLoss: 0.6977 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 311 | ANN: trainLoss: 0.2818 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 311 | ANN: trainLoss: 0.2828 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 311 | ANN: trainLoss: 0.3114 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 311 | ANN: trainLoss: 0.2933 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 311 | ANN: trainLoss: 0.2899 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 311 | ANN: trainLoss: 0.3048 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 311 | ANN: trainLoss: 0.3035 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 311 | ANN: trainLoss: 0.2997 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 311 | ANN: trainLoss: 0.3106 | trainAcc: 85.0694% (490/576)\n",
            "9 13 Epoch: 311 | ANN: trainLoss: 0.3215 | trainAcc: 84.3750% (540/640)\n",
            "10 13 Epoch: 311 | ANN: trainLoss: 0.3309 | trainAcc: 84.2330% (593/704)\n",
            "11 13 Epoch: 311 | ANN: trainLoss: 0.3248 | trainAcc: 84.6354% (650/768)\n",
            "12 13 Epoch: 311 | ANN: trainLoss: 0.3029 | trainAcc: 84.6952% (653/771)\n",
            "0 4 Epoch: 311 | ANN: testLoss: 0.5151 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 311 | ANN: testLoss: 0.5105 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 311 | ANN: testLoss: 0.5089 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 311 | ANN: testLoss: 0.6107 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 312 | ANN: trainLoss: 0.3109 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 312 | ANN: trainLoss: 0.3262 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 312 | ANN: trainLoss: 0.3133 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 312 | ANN: trainLoss: 0.3149 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 312 | ANN: trainLoss: 0.3233 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 312 | ANN: trainLoss: 0.3373 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 312 | ANN: trainLoss: 0.3436 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 312 | ANN: trainLoss: 0.3366 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 312 | ANN: trainLoss: 0.3430 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 312 | ANN: trainLoss: 0.3323 | trainAcc: 86.2500% (552/640)\n",
            "10 13 Epoch: 312 | ANN: trainLoss: 0.3291 | trainAcc: 86.5057% (609/704)\n",
            "11 13 Epoch: 312 | ANN: trainLoss: 0.3319 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 312 | ANN: trainLoss: 0.3433 | trainAcc: 86.3813% (666/771)\n",
            "0 4 Epoch: 312 | ANN: testLoss: 0.6350 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 312 | ANN: testLoss: 0.5560 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 312 | ANN: testLoss: 0.5089 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 312 | ANN: testLoss: 0.4946 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 313 | ANN: trainLoss: 0.3236 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 313 | ANN: trainLoss: 0.3330 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 313 | ANN: trainLoss: 0.3213 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 313 | ANN: trainLoss: 0.3275 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 313 | ANN: trainLoss: 0.3255 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 313 | ANN: trainLoss: 0.3111 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 313 | ANN: trainLoss: 0.2962 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 313 | ANN: trainLoss: 0.2887 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 313 | ANN: trainLoss: 0.2986 | trainAcc: 87.1528% (502/576)\n",
            "9 13 Epoch: 313 | ANN: trainLoss: 0.2959 | trainAcc: 87.1875% (558/640)\n",
            "10 13 Epoch: 313 | ANN: trainLoss: 0.2880 | trainAcc: 87.5000% (616/704)\n",
            "11 13 Epoch: 313 | ANN: trainLoss: 0.2929 | trainAcc: 87.2396% (670/768)\n",
            "12 13 Epoch: 313 | ANN: trainLoss: 0.4590 | trainAcc: 86.9001% (670/771)\n",
            "0 4 Epoch: 313 | ANN: testLoss: 0.5721 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 313 | ANN: testLoss: 0.5569 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 313 | ANN: testLoss: 0.5247 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 313 | ANN: testLoss: 0.5911 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 314 | ANN: trainLoss: 0.3599 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 314 | ANN: trainLoss: 0.3689 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 314 | ANN: trainLoss: 0.3641 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 314 | ANN: trainLoss: 0.3789 | trainAcc: 79.6875% (204/256)\n",
            "4 13 Epoch: 314 | ANN: trainLoss: 0.3672 | trainAcc: 81.2500% (260/320)\n",
            "5 13 Epoch: 314 | ANN: trainLoss: 0.3651 | trainAcc: 81.2500% (312/384)\n",
            "6 13 Epoch: 314 | ANN: trainLoss: 0.3310 | trainAcc: 83.7054% (375/448)\n",
            "7 13 Epoch: 314 | ANN: trainLoss: 0.3207 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 314 | ANN: trainLoss: 0.3210 | trainAcc: 83.6806% (482/576)\n",
            "9 13 Epoch: 314 | ANN: trainLoss: 0.3216 | trainAcc: 83.5938% (535/640)\n",
            "10 13 Epoch: 314 | ANN: trainLoss: 0.3221 | trainAcc: 84.0909% (592/704)\n",
            "11 13 Epoch: 314 | ANN: trainLoss: 0.3171 | trainAcc: 84.7656% (651/768)\n",
            "12 13 Epoch: 314 | ANN: trainLoss: 0.3529 | trainAcc: 84.6952% (653/771)\n",
            "0 4 Epoch: 314 | ANN: testLoss: 0.3818 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 314 | ANN: testLoss: 0.4910 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 314 | ANN: testLoss: 0.5130 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 314 | ANN: testLoss: 0.6049 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 315 | ANN: trainLoss: 0.3842 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 315 | ANN: trainLoss: 0.3402 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 315 | ANN: trainLoss: 0.3516 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 315 | ANN: trainLoss: 0.3295 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 315 | ANN: trainLoss: 0.3317 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 315 | ANN: trainLoss: 0.3217 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 315 | ANN: trainLoss: 0.3425 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 315 | ANN: trainLoss: 0.3441 | trainAcc: 84.5703% (433/512)\n",
            "8 13 Epoch: 315 | ANN: trainLoss: 0.3356 | trainAcc: 84.8958% (489/576)\n",
            "9 13 Epoch: 315 | ANN: trainLoss: 0.3328 | trainAcc: 85.1562% (545/640)\n",
            "10 13 Epoch: 315 | ANN: trainLoss: 0.3283 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 315 | ANN: trainLoss: 0.3224 | trainAcc: 85.4167% (656/768)\n",
            "12 13 Epoch: 315 | ANN: trainLoss: 0.3271 | trainAcc: 85.3437% (658/771)\n",
            "0 4 Epoch: 315 | ANN: testLoss: 0.5925 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 315 | ANN: testLoss: 0.5261 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 315 | ANN: testLoss: 0.5133 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 315 | ANN: testLoss: 0.5362 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 316 | ANN: trainLoss: 0.3001 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 316 | ANN: trainLoss: 0.2644 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 316 | ANN: trainLoss: 0.2782 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 316 | ANN: trainLoss: 0.3163 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 316 | ANN: trainLoss: 0.3225 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 316 | ANN: trainLoss: 0.3291 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 316 | ANN: trainLoss: 0.3430 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 316 | ANN: trainLoss: 0.3414 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 316 | ANN: trainLoss: 0.3352 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 316 | ANN: trainLoss: 0.3322 | trainAcc: 85.3125% (546/640)\n",
            "10 13 Epoch: 316 | ANN: trainLoss: 0.3271 | trainAcc: 85.5114% (602/704)\n",
            "11 13 Epoch: 316 | ANN: trainLoss: 0.3237 | trainAcc: 85.2865% (655/768)\n",
            "12 13 Epoch: 316 | ANN: trainLoss: 0.3002 | trainAcc: 85.3437% (658/771)\n",
            "0 4 Epoch: 316 | ANN: testLoss: 0.4687 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 316 | ANN: testLoss: 0.5382 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 316 | ANN: testLoss: 0.5269 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 316 | ANN: testLoss: 0.5740 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 317 | ANN: trainLoss: 0.2229 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 317 | ANN: trainLoss: 0.2755 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 317 | ANN: trainLoss: 0.2551 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 317 | ANN: trainLoss: 0.2801 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 317 | ANN: trainLoss: 0.2726 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 317 | ANN: trainLoss: 0.2837 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 317 | ANN: trainLoss: 0.2996 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 317 | ANN: trainLoss: 0.2943 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 317 | ANN: trainLoss: 0.3005 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 317 | ANN: trainLoss: 0.3025 | trainAcc: 85.7812% (549/640)\n",
            "10 13 Epoch: 317 | ANN: trainLoss: 0.3000 | trainAcc: 85.7955% (604/704)\n",
            "11 13 Epoch: 317 | ANN: trainLoss: 0.2990 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 317 | ANN: trainLoss: 0.3075 | trainAcc: 85.9922% (663/771)\n",
            "0 4 Epoch: 317 | ANN: testLoss: 0.4496 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 317 | ANN: testLoss: 0.5146 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 317 | ANN: testLoss: 0.5195 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 317 | ANN: testLoss: 0.6473 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 318 | ANN: trainLoss: 0.3014 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 318 | ANN: trainLoss: 0.3024 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 318 | ANN: trainLoss: 0.2801 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 318 | ANN: trainLoss: 0.3187 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 318 | ANN: trainLoss: 0.3170 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 318 | ANN: trainLoss: 0.3337 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 318 | ANN: trainLoss: 0.3472 | trainAcc: 83.7054% (375/448)\n",
            "7 13 Epoch: 318 | ANN: trainLoss: 0.3519 | trainAcc: 83.3984% (427/512)\n",
            "8 13 Epoch: 318 | ANN: trainLoss: 0.3496 | trainAcc: 83.3333% (480/576)\n",
            "9 13 Epoch: 318 | ANN: trainLoss: 0.3350 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 318 | ANN: trainLoss: 0.3273 | trainAcc: 84.3750% (594/704)\n",
            "11 13 Epoch: 318 | ANN: trainLoss: 0.3241 | trainAcc: 84.6354% (650/768)\n",
            "12 13 Epoch: 318 | ANN: trainLoss: 0.3000 | trainAcc: 84.6952% (653/771)\n",
            "0 4 Epoch: 318 | ANN: testLoss: 0.5270 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 318 | ANN: testLoss: 0.4775 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 318 | ANN: testLoss: 0.5307 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 318 | ANN: testLoss: 0.3980 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 319 | ANN: trainLoss: 0.3093 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 319 | ANN: trainLoss: 0.3734 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 319 | ANN: trainLoss: 0.3514 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 319 | ANN: trainLoss: 0.3455 | trainAcc: 84.7656% (217/256)\n",
            "4 13 Epoch: 319 | ANN: trainLoss: 0.3535 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 319 | ANN: trainLoss: 0.3619 | trainAcc: 84.3750% (324/384)\n",
            "6 13 Epoch: 319 | ANN: trainLoss: 0.3596 | trainAcc: 83.9286% (376/448)\n",
            "7 13 Epoch: 319 | ANN: trainLoss: 0.3516 | trainAcc: 83.9844% (430/512)\n",
            "8 13 Epoch: 319 | ANN: trainLoss: 0.3416 | trainAcc: 84.3750% (486/576)\n",
            "9 13 Epoch: 319 | ANN: trainLoss: 0.3403 | trainAcc: 84.5312% (541/640)\n",
            "10 13 Epoch: 319 | ANN: trainLoss: 0.3337 | trainAcc: 84.6591% (596/704)\n",
            "11 13 Epoch: 319 | ANN: trainLoss: 0.3302 | trainAcc: 84.7656% (651/768)\n",
            "12 13 Epoch: 319 | ANN: trainLoss: 0.3715 | trainAcc: 84.6952% (653/771)\n",
            "0 4 Epoch: 319 | ANN: testLoss: 0.6248 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 319 | ANN: testLoss: 0.5642 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 319 | ANN: testLoss: 0.5149 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 319 | ANN: testLoss: 0.6884 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 320 | ANN: trainLoss: 0.2203 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 320 | ANN: trainLoss: 0.2508 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 320 | ANN: trainLoss: 0.2944 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 320 | ANN: trainLoss: 0.3227 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 320 | ANN: trainLoss: 0.3271 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 320 | ANN: trainLoss: 0.3234 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 320 | ANN: trainLoss: 0.3054 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 320 | ANN: trainLoss: 0.2942 | trainAcc: 86.7188% (444/512)\n",
            "8 13 Epoch: 320 | ANN: trainLoss: 0.2897 | trainAcc: 87.1528% (502/576)\n",
            "9 13 Epoch: 320 | ANN: trainLoss: 0.2953 | trainAcc: 86.7188% (555/640)\n",
            "10 13 Epoch: 320 | ANN: trainLoss: 0.3047 | trainAcc: 85.9375% (605/704)\n",
            "11 13 Epoch: 320 | ANN: trainLoss: 0.2974 | trainAcc: 85.9375% (660/768)\n",
            "12 13 Epoch: 320 | ANN: trainLoss: 0.3706 | trainAcc: 85.8625% (662/771)\n",
            "0 4 Epoch: 320 | ANN: testLoss: 0.4439 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 320 | ANN: testLoss: 0.5414 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 320 | ANN: testLoss: 0.5001 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 320 | ANN: testLoss: 0.3751 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 321 | ANN: trainLoss: 0.2615 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 321 | ANN: trainLoss: 0.2822 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 321 | ANN: trainLoss: 0.2854 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 321 | ANN: trainLoss: 0.3107 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 321 | ANN: trainLoss: 0.3108 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 321 | ANN: trainLoss: 0.3243 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 321 | ANN: trainLoss: 0.3298 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 321 | ANN: trainLoss: 0.3080 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 321 | ANN: trainLoss: 0.3168 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 321 | ANN: trainLoss: 0.3142 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 321 | ANN: trainLoss: 0.3246 | trainAcc: 85.6534% (603/704)\n",
            "11 13 Epoch: 321 | ANN: trainLoss: 0.3255 | trainAcc: 85.4167% (656/768)\n",
            "12 13 Epoch: 321 | ANN: trainLoss: 0.3374 | trainAcc: 85.3437% (658/771)\n",
            "0 4 Epoch: 321 | ANN: testLoss: 0.5317 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 321 | ANN: testLoss: 0.5018 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 321 | ANN: testLoss: 0.5003 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 321 | ANN: testLoss: 0.6116 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 322 | ANN: trainLoss: 0.1937 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 322 | ANN: trainLoss: 0.2849 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 322 | ANN: trainLoss: 0.3095 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 322 | ANN: trainLoss: 0.3030 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 322 | ANN: trainLoss: 0.2924 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 322 | ANN: trainLoss: 0.3182 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 322 | ANN: trainLoss: 0.3316 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 322 | ANN: trainLoss: 0.3392 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 322 | ANN: trainLoss: 0.3443 | trainAcc: 84.2014% (485/576)\n",
            "9 13 Epoch: 322 | ANN: trainLoss: 0.3407 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 322 | ANN: trainLoss: 0.3323 | trainAcc: 84.3750% (594/704)\n",
            "11 13 Epoch: 322 | ANN: trainLoss: 0.3275 | trainAcc: 84.5052% (649/768)\n",
            "12 13 Epoch: 322 | ANN: trainLoss: 0.3223 | trainAcc: 84.5655% (652/771)\n",
            "0 4 Epoch: 322 | ANN: testLoss: 0.5165 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 322 | ANN: testLoss: 0.5149 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 322 | ANN: testLoss: 0.5021 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 322 | ANN: testLoss: 0.6105 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 323 | ANN: trainLoss: 0.3253 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 323 | ANN: trainLoss: 0.2988 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 323 | ANN: trainLoss: 0.3206 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 323 | ANN: trainLoss: 0.3011 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 323 | ANN: trainLoss: 0.3061 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 323 | ANN: trainLoss: 0.3038 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 323 | ANN: trainLoss: 0.2985 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 323 | ANN: trainLoss: 0.3035 | trainAcc: 86.7188% (444/512)\n",
            "8 13 Epoch: 323 | ANN: trainLoss: 0.3090 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 323 | ANN: trainLoss: 0.3149 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 323 | ANN: trainLoss: 0.3226 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 323 | ANN: trainLoss: 0.3163 | trainAcc: 85.6771% (658/768)\n",
            "12 13 Epoch: 323 | ANN: trainLoss: 0.3109 | trainAcc: 85.7328% (661/771)\n",
            "0 4 Epoch: 323 | ANN: testLoss: 0.6485 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 323 | ANN: testLoss: 0.5174 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 323 | ANN: testLoss: 0.5046 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 323 | ANN: testLoss: 0.5444 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 324 | ANN: trainLoss: 0.3094 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 324 | ANN: trainLoss: 0.3396 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 324 | ANN: trainLoss: 0.3100 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 324 | ANN: trainLoss: 0.3028 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 324 | ANN: trainLoss: 0.2904 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 324 | ANN: trainLoss: 0.2925 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 324 | ANN: trainLoss: 0.2905 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 324 | ANN: trainLoss: 0.2923 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 324 | ANN: trainLoss: 0.2878 | trainAcc: 86.4583% (498/576)\n",
            "9 13 Epoch: 324 | ANN: trainLoss: 0.2898 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 324 | ANN: trainLoss: 0.2925 | trainAcc: 86.5057% (609/704)\n",
            "11 13 Epoch: 324 | ANN: trainLoss: 0.3006 | trainAcc: 85.9375% (660/768)\n",
            "12 13 Epoch: 324 | ANN: trainLoss: 0.3070 | trainAcc: 85.8625% (662/771)\n",
            "0 4 Epoch: 324 | ANN: testLoss: 0.5998 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 324 | ANN: testLoss: 0.5003 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 324 | ANN: testLoss: 0.5204 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 324 | ANN: testLoss: 0.4380 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 325 | ANN: trainLoss: 0.2334 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 325 | ANN: trainLoss: 0.2731 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 325 | ANN: trainLoss: 0.2978 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 325 | ANN: trainLoss: 0.2891 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 325 | ANN: trainLoss: 0.3239 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 325 | ANN: trainLoss: 0.3209 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 325 | ANN: trainLoss: 0.3122 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 325 | ANN: trainLoss: 0.3143 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 325 | ANN: trainLoss: 0.3175 | trainAcc: 86.1111% (496/576)\n",
            "9 13 Epoch: 325 | ANN: trainLoss: 0.3211 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 325 | ANN: trainLoss: 0.3237 | trainAcc: 86.2216% (607/704)\n",
            "11 13 Epoch: 325 | ANN: trainLoss: 0.3150 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 325 | ANN: trainLoss: 0.2956 | trainAcc: 86.5110% (667/771)\n",
            "0 4 Epoch: 325 | ANN: testLoss: 0.5707 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 325 | ANN: testLoss: 0.5097 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 325 | ANN: testLoss: 0.5214 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 325 | ANN: testLoss: 0.6784 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 326 | ANN: trainLoss: 0.2761 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 326 | ANN: trainLoss: 0.2528 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 326 | ANN: trainLoss: 0.2870 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 326 | ANN: trainLoss: 0.3070 | trainAcc: 85.1562% (218/256)\n",
            "4 13 Epoch: 326 | ANN: trainLoss: 0.3099 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 326 | ANN: trainLoss: 0.3206 | trainAcc: 84.3750% (324/384)\n",
            "6 13 Epoch: 326 | ANN: trainLoss: 0.3194 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 326 | ANN: trainLoss: 0.3154 | trainAcc: 85.9375% (440/512)\n",
            "8 13 Epoch: 326 | ANN: trainLoss: 0.3095 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 326 | ANN: trainLoss: 0.3156 | trainAcc: 85.6250% (548/640)\n",
            "10 13 Epoch: 326 | ANN: trainLoss: 0.3092 | trainAcc: 86.0795% (606/704)\n",
            "11 13 Epoch: 326 | ANN: trainLoss: 0.3106 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 326 | ANN: trainLoss: 0.2889 | trainAcc: 86.1219% (664/771)\n",
            "0 4 Epoch: 326 | ANN: testLoss: 0.4943 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 326 | ANN: testLoss: 0.5307 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 326 | ANN: testLoss: 0.5266 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 326 | ANN: testLoss: 0.5502 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 327 | ANN: trainLoss: 0.2995 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 327 | ANN: trainLoss: 0.3103 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 327 | ANN: trainLoss: 0.2954 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 327 | ANN: trainLoss: 0.2725 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 327 | ANN: trainLoss: 0.2883 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 327 | ANN: trainLoss: 0.2930 | trainAcc: 85.1562% (327/384)\n",
            "6 13 Epoch: 327 | ANN: trainLoss: 0.2964 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 327 | ANN: trainLoss: 0.3072 | trainAcc: 84.9609% (435/512)\n",
            "8 13 Epoch: 327 | ANN: trainLoss: 0.2988 | trainAcc: 85.4167% (492/576)\n",
            "9 13 Epoch: 327 | ANN: trainLoss: 0.3013 | trainAcc: 85.3125% (546/640)\n",
            "10 13 Epoch: 327 | ANN: trainLoss: 0.3003 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 327 | ANN: trainLoss: 0.2977 | trainAcc: 85.5469% (657/768)\n",
            "12 13 Epoch: 327 | ANN: trainLoss: 0.4878 | trainAcc: 85.4734% (659/771)\n",
            "0 4 Epoch: 327 | ANN: testLoss: 0.4814 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 327 | ANN: testLoss: 0.5521 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 327 | ANN: testLoss: 0.5329 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 327 | ANN: testLoss: 0.5373 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 328 | ANN: trainLoss: 0.2192 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 328 | ANN: trainLoss: 0.3766 | trainAcc: 80.4688% (103/128)\n",
            "2 13 Epoch: 328 | ANN: trainLoss: 0.3686 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 328 | ANN: trainLoss: 0.3314 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 328 | ANN: trainLoss: 0.3502 | trainAcc: 82.1875% (263/320)\n",
            "5 13 Epoch: 328 | ANN: trainLoss: 0.3566 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 328 | ANN: trainLoss: 0.3519 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 328 | ANN: trainLoss: 0.3451 | trainAcc: 83.3984% (427/512)\n",
            "8 13 Epoch: 328 | ANN: trainLoss: 0.3382 | trainAcc: 84.0278% (484/576)\n",
            "9 13 Epoch: 328 | ANN: trainLoss: 0.3342 | trainAcc: 84.2188% (539/640)\n",
            "10 13 Epoch: 328 | ANN: trainLoss: 0.3410 | trainAcc: 83.6648% (589/704)\n",
            "11 13 Epoch: 328 | ANN: trainLoss: 0.3329 | trainAcc: 84.6354% (650/768)\n",
            "12 13 Epoch: 328 | ANN: trainLoss: 0.3211 | trainAcc: 84.6952% (653/771)\n",
            "0 4 Epoch: 328 | ANN: testLoss: 0.6225 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 328 | ANN: testLoss: 0.5552 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 328 | ANN: testLoss: 0.5368 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 328 | ANN: testLoss: 0.5966 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 329 | ANN: trainLoss: 0.2823 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 329 | ANN: trainLoss: 0.3022 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 329 | ANN: trainLoss: 0.3290 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 329 | ANN: trainLoss: 0.3321 | trainAcc: 84.7656% (217/256)\n",
            "4 13 Epoch: 329 | ANN: trainLoss: 0.3109 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 329 | ANN: trainLoss: 0.3152 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 329 | ANN: trainLoss: 0.3118 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 329 | ANN: trainLoss: 0.3002 | trainAcc: 85.9375% (440/512)\n",
            "8 13 Epoch: 329 | ANN: trainLoss: 0.3081 | trainAcc: 85.5903% (493/576)\n",
            "9 13 Epoch: 329 | ANN: trainLoss: 0.3133 | trainAcc: 85.7812% (549/640)\n",
            "10 13 Epoch: 329 | ANN: trainLoss: 0.3141 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 329 | ANN: trainLoss: 0.3058 | trainAcc: 85.6771% (658/768)\n",
            "12 13 Epoch: 329 | ANN: trainLoss: 0.2980 | trainAcc: 85.7328% (661/771)\n",
            "0 4 Epoch: 329 | ANN: testLoss: 0.6098 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 329 | ANN: testLoss: 0.5558 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 329 | ANN: testLoss: 0.5477 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 329 | ANN: testLoss: 0.4792 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 330 | ANN: trainLoss: 0.1855 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 330 | ANN: trainLoss: 0.2329 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 330 | ANN: trainLoss: 0.2771 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 330 | ANN: trainLoss: 0.2632 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 330 | ANN: trainLoss: 0.2946 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 330 | ANN: trainLoss: 0.2847 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 330 | ANN: trainLoss: 0.2894 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 330 | ANN: trainLoss: 0.2748 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 330 | ANN: trainLoss: 0.2836 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 330 | ANN: trainLoss: 0.2828 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 330 | ANN: trainLoss: 0.2824 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 330 | ANN: trainLoss: 0.2891 | trainAcc: 89.0625% (684/768)\n",
            "12 13 Epoch: 330 | ANN: trainLoss: 0.3107 | trainAcc: 88.9754% (686/771)\n",
            "0 4 Epoch: 330 | ANN: testLoss: 0.4475 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 330 | ANN: testLoss: 0.5402 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 330 | ANN: testLoss: 0.5439 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 330 | ANN: testLoss: 0.4194 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 331 | ANN: trainLoss: 0.3216 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 331 | ANN: trainLoss: 0.3797 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 331 | ANN: trainLoss: 0.3559 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 331 | ANN: trainLoss: 0.3211 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 331 | ANN: trainLoss: 0.3254 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 331 | ANN: trainLoss: 0.3192 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 331 | ANN: trainLoss: 0.3124 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 331 | ANN: trainLoss: 0.3192 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 331 | ANN: trainLoss: 0.3160 | trainAcc: 86.1111% (496/576)\n",
            "9 13 Epoch: 331 | ANN: trainLoss: 0.3203 | trainAcc: 85.7812% (549/640)\n",
            "10 13 Epoch: 331 | ANN: trainLoss: 0.3208 | trainAcc: 85.7955% (604/704)\n",
            "11 13 Epoch: 331 | ANN: trainLoss: 0.3202 | trainAcc: 85.1562% (654/768)\n",
            "12 13 Epoch: 331 | ANN: trainLoss: 0.3075 | trainAcc: 85.2140% (657/771)\n",
            "0 4 Epoch: 331 | ANN: testLoss: 0.5128 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 331 | ANN: testLoss: 0.5079 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 331 | ANN: testLoss: 0.5326 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 331 | ANN: testLoss: 0.7350 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 332 | ANN: trainLoss: 0.3425 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 332 | ANN: trainLoss: 0.2896 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 332 | ANN: trainLoss: 0.3116 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 332 | ANN: trainLoss: 0.3015 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 332 | ANN: trainLoss: 0.3199 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 332 | ANN: trainLoss: 0.3187 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 332 | ANN: trainLoss: 0.3314 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 332 | ANN: trainLoss: 0.3184 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 332 | ANN: trainLoss: 0.3187 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 332 | ANN: trainLoss: 0.3184 | trainAcc: 85.3125% (546/640)\n",
            "10 13 Epoch: 332 | ANN: trainLoss: 0.3264 | trainAcc: 84.9432% (598/704)\n",
            "11 13 Epoch: 332 | ANN: trainLoss: 0.3252 | trainAcc: 84.8958% (652/768)\n",
            "12 13 Epoch: 332 | ANN: trainLoss: 0.3541 | trainAcc: 84.8249% (654/771)\n",
            "0 4 Epoch: 332 | ANN: testLoss: 0.4762 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 332 | ANN: testLoss: 0.6056 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 332 | ANN: testLoss: 0.5335 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 332 | ANN: testLoss: 0.4002 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 333 | ANN: trainLoss: 0.3635 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 333 | ANN: trainLoss: 0.2810 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 333 | ANN: trainLoss: 0.2864 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 333 | ANN: trainLoss: 0.2806 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 333 | ANN: trainLoss: 0.2699 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 333 | ANN: trainLoss: 0.2901 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 333 | ANN: trainLoss: 0.2987 | trainAcc: 86.3839% (387/448)\n",
            "7 13 Epoch: 333 | ANN: trainLoss: 0.3145 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 333 | ANN: trainLoss: 0.3176 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 333 | ANN: trainLoss: 0.3146 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 333 | ANN: trainLoss: 0.3186 | trainAcc: 85.9375% (605/704)\n",
            "11 13 Epoch: 333 | ANN: trainLoss: 0.3202 | trainAcc: 85.5469% (657/768)\n",
            "12 13 Epoch: 333 | ANN: trainLoss: 0.5125 | trainAcc: 85.2140% (657/771)\n",
            "0 4 Epoch: 333 | ANN: testLoss: 0.4657 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 333 | ANN: testLoss: 0.5147 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 333 | ANN: testLoss: 0.5153 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 333 | ANN: testLoss: 0.5757 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 334 | ANN: trainLoss: 0.3420 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 334 | ANN: trainLoss: 0.3369 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 334 | ANN: trainLoss: 0.3212 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 334 | ANN: trainLoss: 0.3187 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 334 | ANN: trainLoss: 0.3088 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 334 | ANN: trainLoss: 0.3319 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 334 | ANN: trainLoss: 0.3400 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 334 | ANN: trainLoss: 0.3311 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 334 | ANN: trainLoss: 0.3308 | trainAcc: 84.8958% (489/576)\n",
            "9 13 Epoch: 334 | ANN: trainLoss: 0.3152 | trainAcc: 85.7812% (549/640)\n",
            "10 13 Epoch: 334 | ANN: trainLoss: 0.3089 | trainAcc: 85.7955% (604/704)\n",
            "11 13 Epoch: 334 | ANN: trainLoss: 0.3105 | trainAcc: 85.8073% (659/768)\n",
            "12 13 Epoch: 334 | ANN: trainLoss: 0.3241 | trainAcc: 85.7328% (661/771)\n",
            "0 4 Epoch: 334 | ANN: testLoss: 0.7037 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 334 | ANN: testLoss: 0.5873 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 334 | ANN: testLoss: 0.5338 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 334 | ANN: testLoss: 0.4060 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 335 | ANN: trainLoss: 0.3481 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 335 | ANN: trainLoss: 0.3338 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 335 | ANN: trainLoss: 0.3079 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 335 | ANN: trainLoss: 0.3405 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 335 | ANN: trainLoss: 0.3299 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 335 | ANN: trainLoss: 0.3428 | trainAcc: 83.5938% (321/384)\n",
            "6 13 Epoch: 335 | ANN: trainLoss: 0.3681 | trainAcc: 83.2589% (373/448)\n",
            "7 13 Epoch: 335 | ANN: trainLoss: 0.3566 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 335 | ANN: trainLoss: 0.3562 | trainAcc: 84.2014% (485/576)\n",
            "9 13 Epoch: 335 | ANN: trainLoss: 0.3553 | trainAcc: 84.2188% (539/640)\n",
            "10 13 Epoch: 335 | ANN: trainLoss: 0.3472 | trainAcc: 84.3750% (594/704)\n",
            "11 13 Epoch: 335 | ANN: trainLoss: 0.3381 | trainAcc: 84.7656% (651/768)\n",
            "12 13 Epoch: 335 | ANN: trainLoss: 0.3260 | trainAcc: 84.8249% (654/771)\n",
            "0 4 Epoch: 335 | ANN: testLoss: 0.4610 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 335 | ANN: testLoss: 0.4987 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 335 | ANN: testLoss: 0.5187 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 335 | ANN: testLoss: 0.5088 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 336 | ANN: trainLoss: 0.4234 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 336 | ANN: trainLoss: 0.3784 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 336 | ANN: trainLoss: 0.3568 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 336 | ANN: trainLoss: 0.3243 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 336 | ANN: trainLoss: 0.3212 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 336 | ANN: trainLoss: 0.3257 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 336 | ANN: trainLoss: 0.3210 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 336 | ANN: trainLoss: 0.3145 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 336 | ANN: trainLoss: 0.3259 | trainAcc: 83.6806% (482/576)\n",
            "9 13 Epoch: 336 | ANN: trainLoss: 0.3294 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 336 | ANN: trainLoss: 0.3286 | trainAcc: 83.9489% (591/704)\n",
            "11 13 Epoch: 336 | ANN: trainLoss: 0.3272 | trainAcc: 83.8542% (644/768)\n",
            "12 13 Epoch: 336 | ANN: trainLoss: 0.3163 | trainAcc: 83.9170% (647/771)\n",
            "0 4 Epoch: 336 | ANN: testLoss: 0.5848 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 336 | ANN: testLoss: 0.5669 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 336 | ANN: testLoss: 0.5156 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 336 | ANN: testLoss: 0.7025 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 337 | ANN: trainLoss: 0.3560 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 337 | ANN: trainLoss: 0.2643 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 337 | ANN: trainLoss: 0.2629 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 337 | ANN: trainLoss: 0.2792 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 337 | ANN: trainLoss: 0.2759 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 337 | ANN: trainLoss: 0.2918 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 337 | ANN: trainLoss: 0.2939 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 337 | ANN: trainLoss: 0.3014 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 337 | ANN: trainLoss: 0.2946 | trainAcc: 86.6319% (499/576)\n",
            "9 13 Epoch: 337 | ANN: trainLoss: 0.3010 | trainAcc: 86.7188% (555/640)\n",
            "10 13 Epoch: 337 | ANN: trainLoss: 0.2985 | trainAcc: 86.3636% (608/704)\n",
            "11 13 Epoch: 337 | ANN: trainLoss: 0.3005 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 337 | ANN: trainLoss: 0.2885 | trainAcc: 86.5110% (667/771)\n",
            "0 4 Epoch: 337 | ANN: testLoss: 0.4982 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 337 | ANN: testLoss: 0.4996 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 337 | ANN: testLoss: 0.5008 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 337 | ANN: testLoss: 1.6629 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 338 | ANN: trainLoss: 0.3038 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 338 | ANN: trainLoss: 0.3242 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 338 | ANN: trainLoss: 0.3085 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 338 | ANN: trainLoss: 0.2821 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 338 | ANN: trainLoss: 0.2978 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 338 | ANN: trainLoss: 0.3083 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 338 | ANN: trainLoss: 0.3094 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 338 | ANN: trainLoss: 0.2993 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 338 | ANN: trainLoss: 0.2948 | trainAcc: 87.1528% (502/576)\n",
            "9 13 Epoch: 338 | ANN: trainLoss: 0.2883 | trainAcc: 87.0312% (557/640)\n",
            "10 13 Epoch: 338 | ANN: trainLoss: 0.2904 | trainAcc: 86.7898% (611/704)\n",
            "11 13 Epoch: 338 | ANN: trainLoss: 0.2861 | trainAcc: 86.8490% (667/768)\n",
            "12 13 Epoch: 338 | ANN: trainLoss: 0.2892 | trainAcc: 86.9001% (670/771)\n",
            "0 4 Epoch: 338 | ANN: testLoss: 0.5256 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 338 | ANN: testLoss: 0.5408 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 338 | ANN: testLoss: 0.5326 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 338 | ANN: testLoss: 0.4850 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 339 | ANN: trainLoss: 0.3309 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 339 | ANN: trainLoss: 0.2977 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 339 | ANN: trainLoss: 0.2571 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 339 | ANN: trainLoss: 0.2748 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 339 | ANN: trainLoss: 0.2897 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 339 | ANN: trainLoss: 0.2863 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 339 | ANN: trainLoss: 0.2887 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 339 | ANN: trainLoss: 0.2876 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 339 | ANN: trainLoss: 0.2853 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 339 | ANN: trainLoss: 0.2881 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 339 | ANN: trainLoss: 0.2921 | trainAcc: 87.0739% (613/704)\n",
            "11 13 Epoch: 339 | ANN: trainLoss: 0.2959 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 339 | ANN: trainLoss: 0.3208 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 339 | ANN: testLoss: 0.5198 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 339 | ANN: testLoss: 0.5552 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 339 | ANN: testLoss: 0.5240 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 339 | ANN: testLoss: 0.5824 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 340 | ANN: trainLoss: 0.3052 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 340 | ANN: trainLoss: 0.3159 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 340 | ANN: trainLoss: 0.3426 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 340 | ANN: trainLoss: 0.3507 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 340 | ANN: trainLoss: 0.3350 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 340 | ANN: trainLoss: 0.3235 | trainAcc: 85.1562% (327/384)\n",
            "6 13 Epoch: 340 | ANN: trainLoss: 0.3151 | trainAcc: 86.1607% (386/448)\n",
            "7 13 Epoch: 340 | ANN: trainLoss: 0.3210 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 340 | ANN: trainLoss: 0.3057 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 340 | ANN: trainLoss: 0.3087 | trainAcc: 85.9375% (550/640)\n",
            "10 13 Epoch: 340 | ANN: trainLoss: 0.2967 | trainAcc: 86.5057% (609/704)\n",
            "11 13 Epoch: 340 | ANN: trainLoss: 0.2966 | trainAcc: 86.5885% (665/768)\n",
            "12 13 Epoch: 340 | ANN: trainLoss: 0.2821 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 340 | ANN: testLoss: 0.4715 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 340 | ANN: testLoss: 0.5131 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 340 | ANN: testLoss: 0.5274 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 340 | ANN: testLoss: 0.5338 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 341 | ANN: trainLoss: 0.2266 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 341 | ANN: trainLoss: 0.2343 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 341 | ANN: trainLoss: 0.2838 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 341 | ANN: trainLoss: 0.2572 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 341 | ANN: trainLoss: 0.2558 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 341 | ANN: trainLoss: 0.2666 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 341 | ANN: trainLoss: 0.2696 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 341 | ANN: trainLoss: 0.2625 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 341 | ANN: trainLoss: 0.2738 | trainAcc: 88.1944% (508/576)\n",
            "9 13 Epoch: 341 | ANN: trainLoss: 0.2881 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 341 | ANN: trainLoss: 0.2886 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 341 | ANN: trainLoss: 0.2817 | trainAcc: 87.6302% (673/768)\n",
            "12 13 Epoch: 341 | ANN: trainLoss: 0.5493 | trainAcc: 87.4189% (674/771)\n",
            "0 4 Epoch: 341 | ANN: testLoss: 0.6131 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 341 | ANN: testLoss: 0.5552 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 341 | ANN: testLoss: 0.5159 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 341 | ANN: testLoss: 0.4382 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 342 | ANN: trainLoss: 0.1448 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 342 | ANN: trainLoss: 0.2350 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 342 | ANN: trainLoss: 0.2440 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 342 | ANN: trainLoss: 0.2388 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 342 | ANN: trainLoss: 0.2638 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 342 | ANN: trainLoss: 0.2816 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 342 | ANN: trainLoss: 0.2901 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 342 | ANN: trainLoss: 0.2935 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 342 | ANN: trainLoss: 0.2993 | trainAcc: 85.4167% (492/576)\n",
            "9 13 Epoch: 342 | ANN: trainLoss: 0.3031 | trainAcc: 85.6250% (548/640)\n",
            "10 13 Epoch: 342 | ANN: trainLoss: 0.2999 | trainAcc: 85.6534% (603/704)\n",
            "11 13 Epoch: 342 | ANN: trainLoss: 0.3058 | trainAcc: 85.4167% (656/768)\n",
            "12 13 Epoch: 342 | ANN: trainLoss: 0.2964 | trainAcc: 85.4734% (659/771)\n",
            "0 4 Epoch: 342 | ANN: testLoss: 0.5329 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 342 | ANN: testLoss: 0.5268 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 342 | ANN: testLoss: 0.5421 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 342 | ANN: testLoss: 0.5302 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 343 | ANN: trainLoss: 0.2245 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 343 | ANN: trainLoss: 0.2733 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 343 | ANN: trainLoss: 0.2762 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 343 | ANN: trainLoss: 0.2895 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 343 | ANN: trainLoss: 0.3017 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 343 | ANN: trainLoss: 0.3107 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 343 | ANN: trainLoss: 0.3103 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 343 | ANN: trainLoss: 0.3101 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 343 | ANN: trainLoss: 0.3146 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 343 | ANN: trainLoss: 0.3149 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 343 | ANN: trainLoss: 0.3194 | trainAcc: 86.2216% (607/704)\n",
            "11 13 Epoch: 343 | ANN: trainLoss: 0.3152 | trainAcc: 86.3281% (663/768)\n",
            "12 13 Epoch: 343 | ANN: trainLoss: 0.3053 | trainAcc: 86.3813% (666/771)\n",
            "0 4 Epoch: 343 | ANN: testLoss: 0.5274 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 343 | ANN: testLoss: 0.5346 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 343 | ANN: testLoss: 0.5223 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 343 | ANN: testLoss: 0.5806 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 344 | ANN: trainLoss: 0.2825 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 344 | ANN: trainLoss: 0.3432 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 344 | ANN: trainLoss: 0.3179 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 344 | ANN: trainLoss: 0.3615 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 344 | ANN: trainLoss: 0.3436 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 344 | ANN: trainLoss: 0.3344 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 344 | ANN: trainLoss: 0.3388 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 344 | ANN: trainLoss: 0.3632 | trainAcc: 83.7891% (429/512)\n",
            "8 13 Epoch: 344 | ANN: trainLoss: 0.3509 | trainAcc: 84.2014% (485/576)\n",
            "9 13 Epoch: 344 | ANN: trainLoss: 0.3469 | trainAcc: 84.5312% (541/640)\n",
            "10 13 Epoch: 344 | ANN: trainLoss: 0.3390 | trainAcc: 84.9432% (598/704)\n",
            "11 13 Epoch: 344 | ANN: trainLoss: 0.3405 | trainAcc: 84.8958% (652/768)\n",
            "12 13 Epoch: 344 | ANN: trainLoss: 0.3938 | trainAcc: 84.8249% (654/771)\n",
            "0 4 Epoch: 344 | ANN: testLoss: 0.5080 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 344 | ANN: testLoss: 0.4473 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 344 | ANN: testLoss: 0.5096 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 344 | ANN: testLoss: 0.4763 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 345 | ANN: trainLoss: 0.3525 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 345 | ANN: trainLoss: 0.3649 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 345 | ANN: trainLoss: 0.3477 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 345 | ANN: trainLoss: 0.3145 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 345 | ANN: trainLoss: 0.3259 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 345 | ANN: trainLoss: 0.3148 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 345 | ANN: trainLoss: 0.3240 | trainAcc: 86.6071% (388/448)\n",
            "7 13 Epoch: 345 | ANN: trainLoss: 0.3273 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 345 | ANN: trainLoss: 0.3286 | trainAcc: 86.4583% (498/576)\n",
            "9 13 Epoch: 345 | ANN: trainLoss: 0.3238 | trainAcc: 86.5625% (554/640)\n",
            "10 13 Epoch: 345 | ANN: trainLoss: 0.3205 | trainAcc: 86.7898% (611/704)\n",
            "11 13 Epoch: 345 | ANN: trainLoss: 0.3335 | trainAcc: 86.1979% (662/768)\n",
            "12 13 Epoch: 345 | ANN: trainLoss: 0.3651 | trainAcc: 85.9922% (663/771)\n",
            "0 4 Epoch: 345 | ANN: testLoss: 0.5120 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 345 | ANN: testLoss: 0.5366 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 345 | ANN: testLoss: 0.5069 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 345 | ANN: testLoss: 0.4796 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 346 | ANN: trainLoss: 0.2452 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 346 | ANN: trainLoss: 0.2195 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 346 | ANN: trainLoss: 0.2408 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 346 | ANN: trainLoss: 0.2476 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 346 | ANN: trainLoss: 0.2789 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 346 | ANN: trainLoss: 0.3055 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 346 | ANN: trainLoss: 0.3321 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 346 | ANN: trainLoss: 0.3332 | trainAcc: 84.9609% (435/512)\n",
            "8 13 Epoch: 346 | ANN: trainLoss: 0.3295 | trainAcc: 84.8958% (489/576)\n",
            "9 13 Epoch: 346 | ANN: trainLoss: 0.3287 | trainAcc: 84.3750% (540/640)\n",
            "10 13 Epoch: 346 | ANN: trainLoss: 0.3248 | trainAcc: 84.9432% (598/704)\n",
            "11 13 Epoch: 346 | ANN: trainLoss: 0.3242 | trainAcc: 85.0260% (653/768)\n",
            "12 13 Epoch: 346 | ANN: trainLoss: 0.4046 | trainAcc: 84.8249% (654/771)\n",
            "0 4 Epoch: 346 | ANN: testLoss: 0.4702 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 346 | ANN: testLoss: 0.4865 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 346 | ANN: testLoss: 0.5181 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 346 | ANN: testLoss: 0.3897 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 347 | ANN: trainLoss: 0.2975 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 347 | ANN: trainLoss: 0.2545 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 347 | ANN: trainLoss: 0.2821 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 347 | ANN: trainLoss: 0.3092 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 347 | ANN: trainLoss: 0.3262 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 347 | ANN: trainLoss: 0.3280 | trainAcc: 84.3750% (324/384)\n",
            "6 13 Epoch: 347 | ANN: trainLoss: 0.3185 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 347 | ANN: trainLoss: 0.3114 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 347 | ANN: trainLoss: 0.3051 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 347 | ANN: trainLoss: 0.3111 | trainAcc: 85.7812% (549/640)\n",
            "10 13 Epoch: 347 | ANN: trainLoss: 0.3177 | trainAcc: 85.2273% (600/704)\n",
            "11 13 Epoch: 347 | ANN: trainLoss: 0.3259 | trainAcc: 85.4167% (656/768)\n",
            "12 13 Epoch: 347 | ANN: trainLoss: 0.3072 | trainAcc: 85.4734% (659/771)\n",
            "0 4 Epoch: 347 | ANN: testLoss: 0.5903 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 347 | ANN: testLoss: 0.5714 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 347 | ANN: testLoss: 0.5349 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 347 | ANN: testLoss: 0.6259 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 348 | ANN: trainLoss: 0.2826 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 348 | ANN: trainLoss: 0.2926 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 348 | ANN: trainLoss: 0.3458 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 348 | ANN: trainLoss: 0.3457 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 348 | ANN: trainLoss: 0.3407 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 348 | ANN: trainLoss: 0.3269 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 348 | ANN: trainLoss: 0.3215 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 348 | ANN: trainLoss: 0.3166 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 348 | ANN: trainLoss: 0.3196 | trainAcc: 85.4167% (492/576)\n",
            "9 13 Epoch: 348 | ANN: trainLoss: 0.3182 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 348 | ANN: trainLoss: 0.3209 | trainAcc: 86.2216% (607/704)\n",
            "11 13 Epoch: 348 | ANN: trainLoss: 0.3142 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 348 | ANN: trainLoss: 0.3317 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 348 | ANN: testLoss: 0.5529 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 348 | ANN: testLoss: 0.6218 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 348 | ANN: testLoss: 0.5414 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 348 | ANN: testLoss: 0.4071 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 349 | ANN: trainLoss: 0.3462 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 349 | ANN: trainLoss: 0.2920 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 349 | ANN: trainLoss: 0.3277 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 349 | ANN: trainLoss: 0.3041 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 349 | ANN: trainLoss: 0.2921 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 349 | ANN: trainLoss: 0.2864 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 349 | ANN: trainLoss: 0.3020 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 349 | ANN: trainLoss: 0.3009 | trainAcc: 86.9141% (445/512)\n",
            "8 13 Epoch: 349 | ANN: trainLoss: 0.3129 | trainAcc: 86.4583% (498/576)\n",
            "9 13 Epoch: 349 | ANN: trainLoss: 0.3045 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 349 | ANN: trainLoss: 0.3128 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 349 | ANN: trainLoss: 0.3142 | trainAcc: 86.1979% (662/768)\n",
            "12 13 Epoch: 349 | ANN: trainLoss: 0.3221 | trainAcc: 86.2516% (665/771)\n",
            "0 4 Epoch: 349 | ANN: testLoss: 0.4717 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 349 | ANN: testLoss: 0.4711 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 349 | ANN: testLoss: 0.5435 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 349 | ANN: testLoss: 0.8312 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 350 | ANN: trainLoss: 0.2455 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 350 | ANN: trainLoss: 0.2733 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 350 | ANN: trainLoss: 0.2821 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 350 | ANN: trainLoss: 0.2890 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 350 | ANN: trainLoss: 0.2839 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 350 | ANN: trainLoss: 0.3070 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 350 | ANN: trainLoss: 0.3184 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 350 | ANN: trainLoss: 0.3058 | trainAcc: 86.7188% (444/512)\n",
            "8 13 Epoch: 350 | ANN: trainLoss: 0.3034 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 350 | ANN: trainLoss: 0.3049 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 350 | ANN: trainLoss: 0.2929 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 350 | ANN: trainLoss: 0.2993 | trainAcc: 86.5885% (665/768)\n",
            "12 13 Epoch: 350 | ANN: trainLoss: 0.2808 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 350 | ANN: testLoss: 0.5341 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 350 | ANN: testLoss: 0.5388 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 350 | ANN: testLoss: 0.5419 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 350 | ANN: testLoss: 0.5163 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 351 | ANN: trainLoss: 0.2550 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 351 | ANN: trainLoss: 0.2564 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 351 | ANN: trainLoss: 0.2802 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 351 | ANN: trainLoss: 0.2761 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 351 | ANN: trainLoss: 0.2697 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 351 | ANN: trainLoss: 0.2611 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 351 | ANN: trainLoss: 0.2809 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 351 | ANN: trainLoss: 0.2734 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 351 | ANN: trainLoss: 0.2755 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 351 | ANN: trainLoss: 0.2740 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 351 | ANN: trainLoss: 0.2798 | trainAcc: 87.7841% (618/704)\n",
            "11 13 Epoch: 351 | ANN: trainLoss: 0.2898 | trainAcc: 87.5000% (672/768)\n",
            "12 13 Epoch: 351 | ANN: trainLoss: 0.3216 | trainAcc: 87.4189% (674/771)\n",
            "0 4 Epoch: 351 | ANN: testLoss: 0.5778 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 351 | ANN: testLoss: 0.5623 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 351 | ANN: testLoss: 0.5452 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 351 | ANN: testLoss: 0.4103 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 352 | ANN: trainLoss: 0.3516 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 352 | ANN: trainLoss: 0.3742 | trainAcc: 78.9062% (101/128)\n",
            "2 13 Epoch: 352 | ANN: trainLoss: 0.3272 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 352 | ANN: trainLoss: 0.3077 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 352 | ANN: trainLoss: 0.3181 | trainAcc: 83.7500% (268/320)\n",
            "5 13 Epoch: 352 | ANN: trainLoss: 0.3110 | trainAcc: 84.6354% (325/384)\n",
            "6 13 Epoch: 352 | ANN: trainLoss: 0.3079 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 352 | ANN: trainLoss: 0.3100 | trainAcc: 85.1562% (436/512)\n",
            "8 13 Epoch: 352 | ANN: trainLoss: 0.3140 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 352 | ANN: trainLoss: 0.3147 | trainAcc: 85.9375% (550/640)\n",
            "10 13 Epoch: 352 | ANN: trainLoss: 0.3072 | trainAcc: 86.0795% (606/704)\n",
            "11 13 Epoch: 352 | ANN: trainLoss: 0.3124 | trainAcc: 86.3281% (663/768)\n",
            "12 13 Epoch: 352 | ANN: trainLoss: 0.3854 | trainAcc: 86.1219% (664/771)\n",
            "0 4 Epoch: 352 | ANN: testLoss: 0.5191 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 352 | ANN: testLoss: 0.5428 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 352 | ANN: testLoss: 0.5386 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 352 | ANN: testLoss: 0.4046 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 353 | ANN: trainLoss: 0.2824 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 353 | ANN: trainLoss: 0.2592 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 353 | ANN: trainLoss: 0.2739 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 353 | ANN: trainLoss: 0.2894 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 353 | ANN: trainLoss: 0.2964 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 353 | ANN: trainLoss: 0.2950 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 353 | ANN: trainLoss: 0.3028 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 353 | ANN: trainLoss: 0.3143 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 353 | ANN: trainLoss: 0.3093 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 353 | ANN: trainLoss: 0.3033 | trainAcc: 87.1875% (558/640)\n",
            "10 13 Epoch: 353 | ANN: trainLoss: 0.3066 | trainAcc: 86.9318% (612/704)\n",
            "11 13 Epoch: 353 | ANN: trainLoss: 0.2985 | trainAcc: 87.1094% (669/768)\n",
            "12 13 Epoch: 353 | ANN: trainLoss: 0.3061 | trainAcc: 87.0298% (671/771)\n",
            "0 4 Epoch: 353 | ANN: testLoss: 0.5227 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 353 | ANN: testLoss: 0.5175 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 353 | ANN: testLoss: 0.5375 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 353 | ANN: testLoss: 0.4032 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 354 | ANN: trainLoss: 0.3709 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 354 | ANN: trainLoss: 0.3441 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 354 | ANN: trainLoss: 0.3310 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 354 | ANN: trainLoss: 0.3335 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 354 | ANN: trainLoss: 0.3211 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 354 | ANN: trainLoss: 0.3146 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 354 | ANN: trainLoss: 0.3278 | trainAcc: 86.1607% (386/448)\n",
            "7 13 Epoch: 354 | ANN: trainLoss: 0.3329 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 354 | ANN: trainLoss: 0.3314 | trainAcc: 86.1111% (496/576)\n",
            "9 13 Epoch: 354 | ANN: trainLoss: 0.3357 | trainAcc: 86.2500% (552/640)\n",
            "10 13 Epoch: 354 | ANN: trainLoss: 0.3339 | trainAcc: 86.3636% (608/704)\n",
            "11 13 Epoch: 354 | ANN: trainLoss: 0.3347 | trainAcc: 86.3281% (663/768)\n",
            "12 13 Epoch: 354 | ANN: trainLoss: 0.3364 | trainAcc: 86.3813% (666/771)\n",
            "0 4 Epoch: 354 | ANN: testLoss: 0.6082 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 354 | ANN: testLoss: 0.5231 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 354 | ANN: testLoss: 0.5375 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 354 | ANN: testLoss: 0.4032 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 355 | ANN: trainLoss: 0.4110 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 355 | ANN: trainLoss: 0.3535 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 355 | ANN: trainLoss: 0.3607 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 355 | ANN: trainLoss: 0.3602 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 355 | ANN: trainLoss: 0.3450 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 355 | ANN: trainLoss: 0.3508 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 355 | ANN: trainLoss: 0.3442 | trainAcc: 84.1518% (377/448)\n",
            "7 13 Epoch: 355 | ANN: trainLoss: 0.3509 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 355 | ANN: trainLoss: 0.3330 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 355 | ANN: trainLoss: 0.3307 | trainAcc: 85.1562% (545/640)\n",
            "10 13 Epoch: 355 | ANN: trainLoss: 0.3325 | trainAcc: 84.8011% (597/704)\n",
            "11 13 Epoch: 355 | ANN: trainLoss: 0.3280 | trainAcc: 85.1562% (654/768)\n",
            "12 13 Epoch: 355 | ANN: trainLoss: 0.3592 | trainAcc: 85.0843% (656/771)\n",
            "0 4 Epoch: 355 | ANN: testLoss: 0.4230 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 355 | ANN: testLoss: 0.5696 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 355 | ANN: testLoss: 0.5347 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 355 | ANN: testLoss: 0.4011 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 356 | ANN: trainLoss: 0.2521 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 356 | ANN: trainLoss: 0.2910 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 356 | ANN: trainLoss: 0.3227 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 356 | ANN: trainLoss: 0.2931 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 356 | ANN: trainLoss: 0.2802 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 356 | ANN: trainLoss: 0.2976 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 356 | ANN: trainLoss: 0.2907 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 356 | ANN: trainLoss: 0.2936 | trainAcc: 86.9141% (445/512)\n",
            "8 13 Epoch: 356 | ANN: trainLoss: 0.2970 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 356 | ANN: trainLoss: 0.2951 | trainAcc: 87.0312% (557/640)\n",
            "10 13 Epoch: 356 | ANN: trainLoss: 0.2927 | trainAcc: 86.9318% (612/704)\n",
            "11 13 Epoch: 356 | ANN: trainLoss: 0.3019 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 356 | ANN: trainLoss: 0.3008 | trainAcc: 86.7704% (669/771)\n",
            "0 4 Epoch: 356 | ANN: testLoss: 0.5280 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 356 | ANN: testLoss: 0.5178 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 356 | ANN: testLoss: 0.5195 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 356 | ANN: testLoss: 0.3965 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 357 | ANN: trainLoss: 0.3059 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 357 | ANN: trainLoss: 0.2995 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 357 | ANN: trainLoss: 0.2674 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 357 | ANN: trainLoss: 0.2935 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 357 | ANN: trainLoss: 0.3044 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 357 | ANN: trainLoss: 0.3063 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 357 | ANN: trainLoss: 0.3106 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 357 | ANN: trainLoss: 0.3095 | trainAcc: 88.0859% (451/512)\n",
            "8 13 Epoch: 357 | ANN: trainLoss: 0.3004 | trainAcc: 88.1944% (508/576)\n",
            "9 13 Epoch: 357 | ANN: trainLoss: 0.3027 | trainAcc: 87.8125% (562/640)\n",
            "10 13 Epoch: 357 | ANN: trainLoss: 0.2987 | trainAcc: 87.6420% (617/704)\n",
            "11 13 Epoch: 357 | ANN: trainLoss: 0.2990 | trainAcc: 87.5000% (672/768)\n",
            "12 13 Epoch: 357 | ANN: trainLoss: 0.4156 | trainAcc: 87.2892% (673/771)\n",
            "0 4 Epoch: 357 | ANN: testLoss: 0.5182 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 357 | ANN: testLoss: 0.5276 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 357 | ANN: testLoss: 0.5078 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 357 | ANN: testLoss: 0.6521 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 358 | ANN: trainLoss: 0.2837 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 358 | ANN: trainLoss: 0.3083 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 358 | ANN: trainLoss: 0.2956 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 358 | ANN: trainLoss: 0.3234 | trainAcc: 85.1562% (218/256)\n",
            "4 13 Epoch: 358 | ANN: trainLoss: 0.3254 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 358 | ANN: trainLoss: 0.3169 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 358 | ANN: trainLoss: 0.3162 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 358 | ANN: trainLoss: 0.3151 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 358 | ANN: trainLoss: 0.3112 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 358 | ANN: trainLoss: 0.3153 | trainAcc: 85.6250% (548/640)\n",
            "10 13 Epoch: 358 | ANN: trainLoss: 0.3145 | trainAcc: 85.7955% (604/704)\n",
            "11 13 Epoch: 358 | ANN: trainLoss: 0.3078 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 358 | ANN: trainLoss: 0.3265 | trainAcc: 85.9922% (663/771)\n",
            "0 4 Epoch: 358 | ANN: testLoss: 0.5564 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 358 | ANN: testLoss: 0.4734 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 358 | ANN: testLoss: 0.5048 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 358 | ANN: testLoss: 0.5401 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 359 | ANN: trainLoss: 0.3894 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 359 | ANN: trainLoss: 0.3024 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 359 | ANN: trainLoss: 0.2901 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 359 | ANN: trainLoss: 0.3200 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 359 | ANN: trainLoss: 0.3294 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 359 | ANN: trainLoss: 0.3259 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 359 | ANN: trainLoss: 0.3220 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 359 | ANN: trainLoss: 0.3092 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 359 | ANN: trainLoss: 0.3149 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 359 | ANN: trainLoss: 0.3173 | trainAcc: 86.5625% (554/640)\n",
            "10 13 Epoch: 359 | ANN: trainLoss: 0.3221 | trainAcc: 86.0795% (606/704)\n",
            "11 13 Epoch: 359 | ANN: trainLoss: 0.3279 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 359 | ANN: trainLoss: 0.3823 | trainAcc: 85.9922% (663/771)\n",
            "0 4 Epoch: 359 | ANN: testLoss: 0.4650 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 359 | ANN: testLoss: 0.5144 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 359 | ANN: testLoss: 0.5120 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 359 | ANN: testLoss: 0.5747 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 360 | ANN: trainLoss: 0.2374 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 360 | ANN: trainLoss: 0.2349 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 360 | ANN: trainLoss: 0.2709 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 360 | ANN: trainLoss: 0.2910 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 360 | ANN: trainLoss: 0.2881 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 360 | ANN: trainLoss: 0.2912 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 360 | ANN: trainLoss: 0.2800 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 360 | ANN: trainLoss: 0.2811 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 360 | ANN: trainLoss: 0.2723 | trainAcc: 88.3681% (509/576)\n",
            "9 13 Epoch: 360 | ANN: trainLoss: 0.2783 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 360 | ANN: trainLoss: 0.2861 | trainAcc: 87.7841% (618/704)\n",
            "11 13 Epoch: 360 | ANN: trainLoss: 0.2846 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 360 | ANN: trainLoss: 0.2759 | trainAcc: 88.0674% (679/771)\n",
            "0 4 Epoch: 360 | ANN: testLoss: 0.4934 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 360 | ANN: testLoss: 0.4957 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 360 | ANN: testLoss: 0.5101 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 360 | ANN: testLoss: 1.0379 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 361 | ANN: trainLoss: 0.2826 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 361 | ANN: trainLoss: 0.3264 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 361 | ANN: trainLoss: 0.2804 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 361 | ANN: trainLoss: 0.2732 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 361 | ANN: trainLoss: 0.2890 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 361 | ANN: trainLoss: 0.2927 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 361 | ANN: trainLoss: 0.2855 | trainAcc: 86.6071% (388/448)\n",
            "7 13 Epoch: 361 | ANN: trainLoss: 0.2875 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 361 | ANN: trainLoss: 0.2806 | trainAcc: 86.9792% (501/576)\n",
            "9 13 Epoch: 361 | ANN: trainLoss: 0.2825 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 361 | ANN: trainLoss: 0.2861 | trainAcc: 86.7898% (611/704)\n",
            "11 13 Epoch: 361 | ANN: trainLoss: 0.2913 | trainAcc: 86.3281% (663/768)\n",
            "12 13 Epoch: 361 | ANN: trainLoss: 0.2719 | trainAcc: 86.3813% (666/771)\n",
            "0 4 Epoch: 361 | ANN: testLoss: 0.5525 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 361 | ANN: testLoss: 0.5309 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 361 | ANN: testLoss: 0.5283 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 361 | ANN: testLoss: 0.3964 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 362 | ANN: trainLoss: 0.3033 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 362 | ANN: trainLoss: 0.2857 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 362 | ANN: trainLoss: 0.3067 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 362 | ANN: trainLoss: 0.3008 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 362 | ANN: trainLoss: 0.2996 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 362 | ANN: trainLoss: 0.2939 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 362 | ANN: trainLoss: 0.2980 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 362 | ANN: trainLoss: 0.2993 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 362 | ANN: trainLoss: 0.2968 | trainAcc: 86.4583% (498/576)\n",
            "9 13 Epoch: 362 | ANN: trainLoss: 0.2994 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 362 | ANN: trainLoss: 0.3009 | trainAcc: 86.7898% (611/704)\n",
            "11 13 Epoch: 362 | ANN: trainLoss: 0.3020 | trainAcc: 86.5885% (665/768)\n",
            "12 13 Epoch: 362 | ANN: trainLoss: 0.3583 | trainAcc: 86.3813% (666/771)\n",
            "0 4 Epoch: 362 | ANN: testLoss: 0.5626 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 362 | ANN: testLoss: 0.5310 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 362 | ANN: testLoss: 0.5229 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 362 | ANN: testLoss: 0.3922 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 363 | ANN: trainLoss: 0.2468 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 363 | ANN: trainLoss: 0.3059 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 363 | ANN: trainLoss: 0.3335 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 363 | ANN: trainLoss: 0.3051 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 363 | ANN: trainLoss: 0.3124 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 363 | ANN: trainLoss: 0.3023 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 363 | ANN: trainLoss: 0.2966 | trainAcc: 87.2768% (391/448)\n",
            "7 13 Epoch: 363 | ANN: trainLoss: 0.3049 | trainAcc: 86.7188% (444/512)\n",
            "8 13 Epoch: 363 | ANN: trainLoss: 0.2979 | trainAcc: 86.9792% (501/576)\n",
            "9 13 Epoch: 363 | ANN: trainLoss: 0.3025 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 363 | ANN: trainLoss: 0.3046 | trainAcc: 86.0795% (606/704)\n",
            "11 13 Epoch: 363 | ANN: trainLoss: 0.3076 | trainAcc: 86.1979% (662/768)\n",
            "12 13 Epoch: 363 | ANN: trainLoss: 0.2906 | trainAcc: 86.2516% (665/771)\n",
            "0 4 Epoch: 363 | ANN: testLoss: 0.4442 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 363 | ANN: testLoss: 0.4975 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 363 | ANN: testLoss: 0.5209 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 363 | ANN: testLoss: 0.6169 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 364 | ANN: trainLoss: 0.3867 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 364 | ANN: trainLoss: 0.3609 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 364 | ANN: trainLoss: 0.3532 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 364 | ANN: trainLoss: 0.3264 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 364 | ANN: trainLoss: 0.3092 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 364 | ANN: trainLoss: 0.3041 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 364 | ANN: trainLoss: 0.3010 | trainAcc: 86.6071% (388/448)\n",
            "7 13 Epoch: 364 | ANN: trainLoss: 0.2898 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 364 | ANN: trainLoss: 0.2947 | trainAcc: 86.1111% (496/576)\n",
            "9 13 Epoch: 364 | ANN: trainLoss: 0.2969 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 364 | ANN: trainLoss: 0.2978 | trainAcc: 86.0795% (606/704)\n",
            "11 13 Epoch: 364 | ANN: trainLoss: 0.3022 | trainAcc: 85.8073% (659/768)\n",
            "12 13 Epoch: 364 | ANN: trainLoss: 0.3070 | trainAcc: 85.7328% (661/771)\n",
            "0 4 Epoch: 364 | ANN: testLoss: 0.3861 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 364 | ANN: testLoss: 0.4606 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 364 | ANN: testLoss: 0.5261 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 364 | ANN: testLoss: 0.3946 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 365 | ANN: trainLoss: 0.2711 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 365 | ANN: trainLoss: 0.2573 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 365 | ANN: trainLoss: 0.2882 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 365 | ANN: trainLoss: 0.2978 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 365 | ANN: trainLoss: 0.3039 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 365 | ANN: trainLoss: 0.3050 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 365 | ANN: trainLoss: 0.3032 | trainAcc: 86.6071% (388/448)\n",
            "7 13 Epoch: 365 | ANN: trainLoss: 0.2967 | trainAcc: 86.9141% (445/512)\n",
            "8 13 Epoch: 365 | ANN: trainLoss: 0.2975 | trainAcc: 86.9792% (501/576)\n",
            "9 13 Epoch: 365 | ANN: trainLoss: 0.3055 | trainAcc: 86.2500% (552/640)\n",
            "10 13 Epoch: 365 | ANN: trainLoss: 0.2982 | trainAcc: 86.9318% (612/704)\n",
            "11 13 Epoch: 365 | ANN: trainLoss: 0.3024 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 365 | ANN: trainLoss: 0.2886 | trainAcc: 86.7704% (669/771)\n",
            "0 4 Epoch: 365 | ANN: testLoss: 0.4754 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 365 | ANN: testLoss: 0.4781 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 365 | ANN: testLoss: 0.5315 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 365 | ANN: testLoss: 0.6382 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 366 | ANN: trainLoss: 0.4428 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 366 | ANN: trainLoss: 0.3709 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 366 | ANN: trainLoss: 0.3426 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 366 | ANN: trainLoss: 0.3344 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 366 | ANN: trainLoss: 0.3153 | trainAcc: 84.3750% (270/320)\n",
            "5 13 Epoch: 366 | ANN: trainLoss: 0.3209 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 366 | ANN: trainLoss: 0.3053 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 366 | ANN: trainLoss: 0.3087 | trainAcc: 85.7422% (439/512)\n",
            "8 13 Epoch: 366 | ANN: trainLoss: 0.3104 | trainAcc: 86.1111% (496/576)\n",
            "9 13 Epoch: 366 | ANN: trainLoss: 0.3053 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 366 | ANN: trainLoss: 0.3067 | trainAcc: 86.5057% (609/704)\n",
            "11 13 Epoch: 366 | ANN: trainLoss: 0.3064 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 366 | ANN: trainLoss: 0.3615 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 366 | ANN: testLoss: 0.4521 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 366 | ANN: testLoss: 0.4244 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 366 | ANN: testLoss: 0.5183 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 366 | ANN: testLoss: 0.6154 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 367 | ANN: trainLoss: 0.2532 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 367 | ANN: trainLoss: 0.2972 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 367 | ANN: trainLoss: 0.2816 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 367 | ANN: trainLoss: 0.2706 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 367 | ANN: trainLoss: 0.2604 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 367 | ANN: trainLoss: 0.2792 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 367 | ANN: trainLoss: 0.2907 | trainAcc: 87.2768% (391/448)\n",
            "7 13 Epoch: 367 | ANN: trainLoss: 0.2876 | trainAcc: 87.3047% (447/512)\n",
            "8 13 Epoch: 367 | ANN: trainLoss: 0.2768 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 367 | ANN: trainLoss: 0.2750 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 367 | ANN: trainLoss: 0.2860 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 367 | ANN: trainLoss: 0.2939 | trainAcc: 87.1094% (669/768)\n",
            "12 13 Epoch: 367 | ANN: trainLoss: 0.2941 | trainAcc: 87.1595% (672/771)\n",
            "0 4 Epoch: 367 | ANN: testLoss: 0.3608 | testAcc: 85.9375% (55/64)\n",
            "1 4 Epoch: 367 | ANN: testLoss: 0.4928 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 367 | ANN: testLoss: 0.5194 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 367 | ANN: testLoss: 0.6554 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 368 | ANN: trainLoss: 0.3396 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 368 | ANN: trainLoss: 0.3371 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 368 | ANN: trainLoss: 0.3219 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 368 | ANN: trainLoss: 0.3148 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 368 | ANN: trainLoss: 0.3020 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 368 | ANN: trainLoss: 0.3089 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 368 | ANN: trainLoss: 0.2916 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 368 | ANN: trainLoss: 0.2839 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 368 | ANN: trainLoss: 0.2875 | trainAcc: 88.3681% (509/576)\n",
            "9 13 Epoch: 368 | ANN: trainLoss: 0.2794 | trainAcc: 88.7500% (568/640)\n",
            "10 13 Epoch: 368 | ANN: trainLoss: 0.2806 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 368 | ANN: trainLoss: 0.2767 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 368 | ANN: trainLoss: 0.2633 | trainAcc: 88.8457% (685/771)\n",
            "0 4 Epoch: 368 | ANN: testLoss: 0.4735 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 368 | ANN: testLoss: 0.4489 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 368 | ANN: testLoss: 0.5213 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 368 | ANN: testLoss: 0.4927 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 369 | ANN: trainLoss: 0.2576 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 369 | ANN: trainLoss: 0.2703 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 369 | ANN: trainLoss: 0.2562 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 369 | ANN: trainLoss: 0.2399 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 369 | ANN: trainLoss: 0.2333 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 369 | ANN: trainLoss: 0.2479 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 369 | ANN: trainLoss: 0.2473 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 369 | ANN: trainLoss: 0.2497 | trainAcc: 89.0625% (456/512)\n",
            "8 13 Epoch: 369 | ANN: trainLoss: 0.2697 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 369 | ANN: trainLoss: 0.2870 | trainAcc: 86.5625% (554/640)\n",
            "10 13 Epoch: 369 | ANN: trainLoss: 0.2858 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 369 | ANN: trainLoss: 0.2932 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 369 | ANN: trainLoss: 0.2898 | trainAcc: 86.5110% (667/771)\n",
            "0 4 Epoch: 369 | ANN: testLoss: 0.4459 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 369 | ANN: testLoss: 0.4861 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 369 | ANN: testLoss: 0.5182 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 369 | ANN: testLoss: 0.7293 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 370 | ANN: trainLoss: 0.1874 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 370 | ANN: trainLoss: 0.2196 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 370 | ANN: trainLoss: 0.2209 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 370 | ANN: trainLoss: 0.2310 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 370 | ANN: trainLoss: 0.2410 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 370 | ANN: trainLoss: 0.2541 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 370 | ANN: trainLoss: 0.2557 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 370 | ANN: trainLoss: 0.2542 | trainAcc: 88.0859% (451/512)\n",
            "8 13 Epoch: 370 | ANN: trainLoss: 0.2725 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 370 | ANN: trainLoss: 0.2727 | trainAcc: 87.8125% (562/640)\n",
            "10 13 Epoch: 370 | ANN: trainLoss: 0.2828 | trainAcc: 87.5000% (616/704)\n",
            "11 13 Epoch: 370 | ANN: trainLoss: 0.2837 | trainAcc: 87.6302% (673/768)\n",
            "12 13 Epoch: 370 | ANN: trainLoss: 0.3481 | trainAcc: 87.4189% (674/771)\n",
            "0 4 Epoch: 370 | ANN: testLoss: 0.4812 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 370 | ANN: testLoss: 0.5361 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 370 | ANN: testLoss: 0.5172 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 370 | ANN: testLoss: 0.4848 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 371 | ANN: trainLoss: 0.3935 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 371 | ANN: trainLoss: 0.3543 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 371 | ANN: trainLoss: 0.3137 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 371 | ANN: trainLoss: 0.3120 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 371 | ANN: trainLoss: 0.3254 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 371 | ANN: trainLoss: 0.3142 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 371 | ANN: trainLoss: 0.3033 | trainAcc: 87.2768% (391/448)\n",
            "7 13 Epoch: 371 | ANN: trainLoss: 0.3052 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 371 | ANN: trainLoss: 0.3047 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 371 | ANN: trainLoss: 0.3059 | trainAcc: 86.5625% (554/640)\n",
            "10 13 Epoch: 371 | ANN: trainLoss: 0.3010 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 371 | ANN: trainLoss: 0.3110 | trainAcc: 85.8073% (659/768)\n",
            "12 13 Epoch: 371 | ANN: trainLoss: 0.3372 | trainAcc: 85.6031% (660/771)\n",
            "0 4 Epoch: 371 | ANN: testLoss: 0.4982 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 371 | ANN: testLoss: 0.4859 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 371 | ANN: testLoss: 0.5379 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 371 | ANN: testLoss: 0.5934 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 372 | ANN: trainLoss: 0.2787 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 372 | ANN: trainLoss: 0.2876 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 372 | ANN: trainLoss: 0.2409 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 372 | ANN: trainLoss: 0.2305 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 372 | ANN: trainLoss: 0.2573 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 372 | ANN: trainLoss: 0.2554 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 372 | ANN: trainLoss: 0.2587 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 372 | ANN: trainLoss: 0.2657 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 372 | ANN: trainLoss: 0.2706 | trainAcc: 87.5000% (504/576)\n",
            "9 13 Epoch: 372 | ANN: trainLoss: 0.2754 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 372 | ANN: trainLoss: 0.2806 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 372 | ANN: trainLoss: 0.2724 | trainAcc: 87.3698% (671/768)\n",
            "12 13 Epoch: 372 | ANN: trainLoss: 0.2934 | trainAcc: 87.2892% (673/771)\n",
            "0 4 Epoch: 372 | ANN: testLoss: 0.4260 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 372 | ANN: testLoss: 0.4443 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 372 | ANN: testLoss: 0.5231 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 372 | ANN: testLoss: 0.3923 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 373 | ANN: trainLoss: 0.1911 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 373 | ANN: trainLoss: 0.2625 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 373 | ANN: trainLoss: 0.2745 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 373 | ANN: trainLoss: 0.2543 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 373 | ANN: trainLoss: 0.2650 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 373 | ANN: trainLoss: 0.2816 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 373 | ANN: trainLoss: 0.2683 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 373 | ANN: trainLoss: 0.2730 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 373 | ANN: trainLoss: 0.2727 | trainAcc: 88.1944% (508/576)\n",
            "9 13 Epoch: 373 | ANN: trainLoss: 0.2832 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 373 | ANN: trainLoss: 0.2779 | trainAcc: 88.4943% (623/704)\n",
            "11 13 Epoch: 373 | ANN: trainLoss: 0.2690 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 373 | ANN: trainLoss: 0.2959 | trainAcc: 89.2348% (688/771)\n",
            "0 4 Epoch: 373 | ANN: testLoss: 0.5809 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 373 | ANN: testLoss: 0.5791 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 373 | ANN: testLoss: 0.5309 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 373 | ANN: testLoss: 0.5724 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 374 | ANN: trainLoss: 0.2492 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 374 | ANN: trainLoss: 0.2422 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 374 | ANN: trainLoss: 0.2892 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 374 | ANN: trainLoss: 0.2931 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 374 | ANN: trainLoss: 0.2845 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 374 | ANN: trainLoss: 0.2697 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 374 | ANN: trainLoss: 0.2598 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 374 | ANN: trainLoss: 0.2674 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 374 | ANN: trainLoss: 0.2632 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 374 | ANN: trainLoss: 0.2574 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 374 | ANN: trainLoss: 0.2756 | trainAcc: 87.9261% (619/704)\n",
            "11 13 Epoch: 374 | ANN: trainLoss: 0.2728 | trainAcc: 87.8906% (675/768)\n",
            "12 13 Epoch: 374 | ANN: trainLoss: 0.3352 | trainAcc: 87.6783% (676/771)\n",
            "0 4 Epoch: 374 | ANN: testLoss: 0.6270 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 374 | ANN: testLoss: 0.5628 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 374 | ANN: testLoss: 0.5361 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 374 | ANN: testLoss: 0.5885 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 375 | ANN: trainLoss: 0.3205 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 375 | ANN: trainLoss: 0.2795 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 375 | ANN: trainLoss: 0.3052 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 375 | ANN: trainLoss: 0.2884 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 375 | ANN: trainLoss: 0.2874 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 375 | ANN: trainLoss: 0.2731 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 375 | ANN: trainLoss: 0.2575 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 375 | ANN: trainLoss: 0.2660 | trainAcc: 86.9141% (445/512)\n",
            "8 13 Epoch: 375 | ANN: trainLoss: 0.2650 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 375 | ANN: trainLoss: 0.2726 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 375 | ANN: trainLoss: 0.2788 | trainAcc: 86.9318% (612/704)\n",
            "11 13 Epoch: 375 | ANN: trainLoss: 0.2807 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 375 | ANN: trainLoss: 0.2774 | trainAcc: 86.5110% (667/771)\n",
            "0 4 Epoch: 375 | ANN: testLoss: 0.5238 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 375 | ANN: testLoss: 0.4963 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 375 | ANN: testLoss: 0.5275 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 375 | ANN: testLoss: 0.7333 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 376 | ANN: trainLoss: 0.3363 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 376 | ANN: trainLoss: 0.3165 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 376 | ANN: trainLoss: 0.3093 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 376 | ANN: trainLoss: 0.3208 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 376 | ANN: trainLoss: 0.2941 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 376 | ANN: trainLoss: 0.2753 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 376 | ANN: trainLoss: 0.2797 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 376 | ANN: trainLoss: 0.2717 | trainAcc: 89.2578% (457/512)\n",
            "8 13 Epoch: 376 | ANN: trainLoss: 0.2769 | trainAcc: 88.3681% (509/576)\n",
            "9 13 Epoch: 376 | ANN: trainLoss: 0.2826 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 376 | ANN: trainLoss: 0.2772 | trainAcc: 88.4943% (623/704)\n",
            "11 13 Epoch: 376 | ANN: trainLoss: 0.2785 | trainAcc: 88.5417% (680/768)\n",
            "12 13 Epoch: 376 | ANN: trainLoss: 0.3048 | trainAcc: 88.4565% (682/771)\n",
            "0 4 Epoch: 376 | ANN: testLoss: 0.3526 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 376 | ANN: testLoss: 0.5453 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 376 | ANN: testLoss: 0.5398 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 376 | ANN: testLoss: 0.5240 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 377 | ANN: trainLoss: 0.3420 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 377 | ANN: trainLoss: 0.3489 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 377 | ANN: trainLoss: 0.3368 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 377 | ANN: trainLoss: 0.3099 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 377 | ANN: trainLoss: 0.3016 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 377 | ANN: trainLoss: 0.2810 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 377 | ANN: trainLoss: 0.2799 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 377 | ANN: trainLoss: 0.2745 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 377 | ANN: trainLoss: 0.2746 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 377 | ANN: trainLoss: 0.2880 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 377 | ANN: trainLoss: 0.2990 | trainAcc: 88.4943% (623/704)\n",
            "11 13 Epoch: 377 | ANN: trainLoss: 0.2969 | trainAcc: 88.1510% (677/768)\n",
            "12 13 Epoch: 377 | ANN: trainLoss: 0.3153 | trainAcc: 88.0674% (679/771)\n",
            "0 4 Epoch: 377 | ANN: testLoss: 0.6285 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 377 | ANN: testLoss: 0.5451 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 377 | ANN: testLoss: 0.5504 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 377 | ANN: testLoss: 0.5382 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 378 | ANN: trainLoss: 0.3630 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 378 | ANN: trainLoss: 0.3318 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 378 | ANN: trainLoss: 0.3190 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 378 | ANN: trainLoss: 0.3191 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 378 | ANN: trainLoss: 0.3592 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 378 | ANN: trainLoss: 0.3474 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 378 | ANN: trainLoss: 0.3254 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 378 | ANN: trainLoss: 0.3202 | trainAcc: 87.3047% (447/512)\n",
            "8 13 Epoch: 378 | ANN: trainLoss: 0.3166 | trainAcc: 87.5000% (504/576)\n",
            "9 13 Epoch: 378 | ANN: trainLoss: 0.3176 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 378 | ANN: trainLoss: 0.3214 | trainAcc: 87.5000% (616/704)\n",
            "11 13 Epoch: 378 | ANN: trainLoss: 0.3237 | trainAcc: 87.3698% (671/768)\n",
            "12 13 Epoch: 378 | ANN: trainLoss: 0.4395 | trainAcc: 87.1595% (672/771)\n",
            "0 4 Epoch: 378 | ANN: testLoss: 0.7256 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 378 | ANN: testLoss: 0.6030 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 378 | ANN: testLoss: 0.5477 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 378 | ANN: testLoss: 0.4108 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 379 | ANN: trainLoss: 0.2591 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 379 | ANN: trainLoss: 0.3239 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 379 | ANN: trainLoss: 0.3001 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 379 | ANN: trainLoss: 0.2951 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 379 | ANN: trainLoss: 0.2935 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 379 | ANN: trainLoss: 0.3186 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 379 | ANN: trainLoss: 0.3106 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 379 | ANN: trainLoss: 0.3175 | trainAcc: 85.7422% (439/512)\n",
            "8 13 Epoch: 379 | ANN: trainLoss: 0.3214 | trainAcc: 85.4167% (492/576)\n",
            "9 13 Epoch: 379 | ANN: trainLoss: 0.3262 | trainAcc: 85.1562% (545/640)\n",
            "10 13 Epoch: 379 | ANN: trainLoss: 0.3115 | trainAcc: 85.9375% (605/704)\n",
            "11 13 Epoch: 379 | ANN: trainLoss: 0.3072 | trainAcc: 86.1979% (662/768)\n",
            "12 13 Epoch: 379 | ANN: trainLoss: 0.3254 | trainAcc: 86.1219% (664/771)\n",
            "0 4 Epoch: 379 | ANN: testLoss: 0.6325 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 379 | ANN: testLoss: 0.5441 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 379 | ANN: testLoss: 0.5639 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 379 | ANN: testLoss: 0.4250 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 380 | ANN: trainLoss: 0.2936 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 380 | ANN: trainLoss: 0.2712 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 380 | ANN: trainLoss: 0.2792 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 380 | ANN: trainLoss: 0.3064 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 380 | ANN: trainLoss: 0.3175 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 380 | ANN: trainLoss: 0.3185 | trainAcc: 84.6354% (325/384)\n",
            "6 13 Epoch: 380 | ANN: trainLoss: 0.3074 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 380 | ANN: trainLoss: 0.3199 | trainAcc: 85.7422% (439/512)\n",
            "8 13 Epoch: 380 | ANN: trainLoss: 0.3055 | trainAcc: 86.6319% (499/576)\n",
            "9 13 Epoch: 380 | ANN: trainLoss: 0.2974 | trainAcc: 87.0312% (557/640)\n",
            "10 13 Epoch: 380 | ANN: trainLoss: 0.3064 | trainAcc: 86.2216% (607/704)\n",
            "11 13 Epoch: 380 | ANN: trainLoss: 0.2957 | trainAcc: 87.1094% (669/768)\n",
            "12 13 Epoch: 380 | ANN: trainLoss: 0.2808 | trainAcc: 87.1595% (672/771)\n",
            "0 4 Epoch: 380 | ANN: testLoss: 0.6367 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 380 | ANN: testLoss: 0.5697 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 380 | ANN: testLoss: 0.5595 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 380 | ANN: testLoss: 0.4202 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 381 | ANN: trainLoss: 0.3043 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 381 | ANN: trainLoss: 0.2853 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 381 | ANN: trainLoss: 0.3025 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 381 | ANN: trainLoss: 0.2703 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 381 | ANN: trainLoss: 0.2622 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 381 | ANN: trainLoss: 0.2621 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 381 | ANN: trainLoss: 0.2601 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 381 | ANN: trainLoss: 0.2631 | trainAcc: 88.6719% (454/512)\n",
            "8 13 Epoch: 381 | ANN: trainLoss: 0.2708 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 381 | ANN: trainLoss: 0.2703 | trainAcc: 88.7500% (568/640)\n",
            "10 13 Epoch: 381 | ANN: trainLoss: 0.2712 | trainAcc: 88.6364% (624/704)\n",
            "11 13 Epoch: 381 | ANN: trainLoss: 0.2640 | trainAcc: 89.1927% (685/768)\n",
            "12 13 Epoch: 381 | ANN: trainLoss: 0.2701 | trainAcc: 89.1051% (687/771)\n",
            "0 4 Epoch: 381 | ANN: testLoss: 0.5942 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 381 | ANN: testLoss: 0.6030 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 381 | ANN: testLoss: 0.5550 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 381 | ANN: testLoss: 0.4920 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 382 | ANN: trainLoss: 0.3083 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 382 | ANN: trainLoss: 0.2883 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 382 | ANN: trainLoss: 0.2990 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 382 | ANN: trainLoss: 0.2999 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 382 | ANN: trainLoss: 0.2953 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 382 | ANN: trainLoss: 0.2787 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 382 | ANN: trainLoss: 0.2820 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 382 | ANN: trainLoss: 0.2787 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 382 | ANN: trainLoss: 0.2729 | trainAcc: 88.7153% (511/576)\n",
            "9 13 Epoch: 382 | ANN: trainLoss: 0.2719 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 382 | ANN: trainLoss: 0.2738 | trainAcc: 89.0625% (627/704)\n",
            "11 13 Epoch: 382 | ANN: trainLoss: 0.2712 | trainAcc: 89.1927% (685/768)\n",
            "12 13 Epoch: 382 | ANN: trainLoss: 0.2562 | trainAcc: 89.2348% (688/771)\n",
            "0 4 Epoch: 382 | ANN: testLoss: 0.5008 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 382 | ANN: testLoss: 0.5883 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 382 | ANN: testLoss: 0.5536 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 382 | ANN: testLoss: 0.4907 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 383 | ANN: trainLoss: 0.2458 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 383 | ANN: trainLoss: 0.3157 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 383 | ANN: trainLoss: 0.3420 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 383 | ANN: trainLoss: 0.3342 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 383 | ANN: trainLoss: 0.3365 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 383 | ANN: trainLoss: 0.3506 | trainAcc: 84.6354% (325/384)\n",
            "6 13 Epoch: 383 | ANN: trainLoss: 0.3323 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 383 | ANN: trainLoss: 0.3205 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 383 | ANN: trainLoss: 0.3143 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 383 | ANN: trainLoss: 0.3165 | trainAcc: 86.5625% (554/640)\n",
            "10 13 Epoch: 383 | ANN: trainLoss: 0.3138 | trainAcc: 87.0739% (613/704)\n",
            "11 13 Epoch: 383 | ANN: trainLoss: 0.3139 | trainAcc: 86.8490% (667/768)\n",
            "12 13 Epoch: 383 | ANN: trainLoss: 0.4455 | trainAcc: 86.5110% (667/771)\n",
            "0 4 Epoch: 383 | ANN: testLoss: 0.4978 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 383 | ANN: testLoss: 0.5095 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 383 | ANN: testLoss: 0.5329 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 383 | ANN: testLoss: 0.5503 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 384 | ANN: trainLoss: 0.2297 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 384 | ANN: trainLoss: 0.2619 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 384 | ANN: trainLoss: 0.2441 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 384 | ANN: trainLoss: 0.2424 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 384 | ANN: trainLoss: 0.2484 | trainAcc: 90.0000% (288/320)\n",
            "5 13 Epoch: 384 | ANN: trainLoss: 0.2561 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 384 | ANN: trainLoss: 0.2634 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 384 | ANN: trainLoss: 0.2567 | trainAcc: 88.6719% (454/512)\n",
            "8 13 Epoch: 384 | ANN: trainLoss: 0.2570 | trainAcc: 88.3681% (509/576)\n",
            "9 13 Epoch: 384 | ANN: trainLoss: 0.2605 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 384 | ANN: trainLoss: 0.2596 | trainAcc: 87.9261% (619/704)\n",
            "11 13 Epoch: 384 | ANN: trainLoss: 0.2554 | trainAcc: 88.2812% (678/768)\n",
            "12 13 Epoch: 384 | ANN: trainLoss: 0.2377 | trainAcc: 88.3268% (681/771)\n",
            "0 4 Epoch: 384 | ANN: testLoss: 0.5025 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 384 | ANN: testLoss: 0.5209 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 384 | ANN: testLoss: 0.5197 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 384 | ANN: testLoss: 0.5937 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 385 | ANN: trainLoss: 0.2635 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 385 | ANN: trainLoss: 0.2877 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 385 | ANN: trainLoss: 0.2915 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 385 | ANN: trainLoss: 0.3075 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 385 | ANN: trainLoss: 0.3121 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 385 | ANN: trainLoss: 0.3063 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 385 | ANN: trainLoss: 0.2950 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 385 | ANN: trainLoss: 0.3049 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 385 | ANN: trainLoss: 0.3178 | trainAcc: 87.1528% (502/576)\n",
            "9 13 Epoch: 385 | ANN: trainLoss: 0.3215 | trainAcc: 86.7188% (555/640)\n",
            "10 13 Epoch: 385 | ANN: trainLoss: 0.3218 | trainAcc: 86.5057% (609/704)\n",
            "11 13 Epoch: 385 | ANN: trainLoss: 0.3142 | trainAcc: 86.9792% (668/768)\n",
            "12 13 Epoch: 385 | ANN: trainLoss: 0.3361 | trainAcc: 86.7704% (669/771)\n",
            "0 4 Epoch: 385 | ANN: testLoss: 0.4827 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 385 | ANN: testLoss: 0.5203 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 385 | ANN: testLoss: 0.5340 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 385 | ANN: testLoss: 0.4602 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 386 | ANN: trainLoss: 0.2744 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 386 | ANN: trainLoss: 0.2672 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 386 | ANN: trainLoss: 0.2546 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 386 | ANN: trainLoss: 0.2784 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 386 | ANN: trainLoss: 0.2873 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 386 | ANN: trainLoss: 0.2856 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 386 | ANN: trainLoss: 0.2812 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 386 | ANN: trainLoss: 0.2963 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 386 | ANN: trainLoss: 0.2995 | trainAcc: 86.4583% (498/576)\n",
            "9 13 Epoch: 386 | ANN: trainLoss: 0.2993 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 386 | ANN: trainLoss: 0.2925 | trainAcc: 86.9318% (612/704)\n",
            "11 13 Epoch: 386 | ANN: trainLoss: 0.2881 | trainAcc: 87.3698% (671/768)\n",
            "12 13 Epoch: 386 | ANN: trainLoss: 0.2929 | trainAcc: 87.2892% (673/771)\n",
            "0 4 Epoch: 386 | ANN: testLoss: 0.4991 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 386 | ANN: testLoss: 0.4624 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 386 | ANN: testLoss: 0.5437 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 386 | ANN: testLoss: 0.4078 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 387 | ANN: trainLoss: 0.2422 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 387 | ANN: trainLoss: 0.2562 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 387 | ANN: trainLoss: 0.2635 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 387 | ANN: trainLoss: 0.2635 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 387 | ANN: trainLoss: 0.2883 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 387 | ANN: trainLoss: 0.2902 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 387 | ANN: trainLoss: 0.2874 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 387 | ANN: trainLoss: 0.2912 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 387 | ANN: trainLoss: 0.2906 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 387 | ANN: trainLoss: 0.2894 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 387 | ANN: trainLoss: 0.2894 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 387 | ANN: trainLoss: 0.2939 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 387 | ANN: trainLoss: 0.2953 | trainAcc: 86.5110% (667/771)\n",
            "0 4 Epoch: 387 | ANN: testLoss: 0.5796 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 387 | ANN: testLoss: 0.6122 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 387 | ANN: testLoss: 0.5426 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 387 | ANN: testLoss: 0.5288 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 388 | ANN: trainLoss: 0.4016 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 388 | ANN: trainLoss: 0.3234 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 388 | ANN: trainLoss: 0.2920 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 388 | ANN: trainLoss: 0.2801 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 388 | ANN: trainLoss: 0.2770 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 388 | ANN: trainLoss: 0.2778 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 388 | ANN: trainLoss: 0.2666 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 388 | ANN: trainLoss: 0.2624 | trainAcc: 88.6719% (454/512)\n",
            "8 13 Epoch: 388 | ANN: trainLoss: 0.2533 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 388 | ANN: trainLoss: 0.2562 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 388 | ANN: trainLoss: 0.2561 | trainAcc: 89.4886% (630/704)\n",
            "11 13 Epoch: 388 | ANN: trainLoss: 0.2567 | trainAcc: 89.4531% (687/768)\n",
            "12 13 Epoch: 388 | ANN: trainLoss: 0.4168 | trainAcc: 89.2348% (688/771)\n",
            "0 4 Epoch: 388 | ANN: testLoss: 0.6147 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 388 | ANN: testLoss: 0.4954 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 388 | ANN: testLoss: 0.5362 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 388 | ANN: testLoss: 0.4352 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 389 | ANN: trainLoss: 0.2448 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 389 | ANN: trainLoss: 0.2779 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 389 | ANN: trainLoss: 0.2481 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 389 | ANN: trainLoss: 0.2456 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 389 | ANN: trainLoss: 0.2662 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 389 | ANN: trainLoss: 0.2642 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 389 | ANN: trainLoss: 0.2688 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 389 | ANN: trainLoss: 0.2786 | trainAcc: 87.3047% (447/512)\n",
            "8 13 Epoch: 389 | ANN: trainLoss: 0.2812 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 389 | ANN: trainLoss: 0.2967 | trainAcc: 86.7188% (555/640)\n",
            "10 13 Epoch: 389 | ANN: trainLoss: 0.2992 | trainAcc: 86.3636% (608/704)\n",
            "11 13 Epoch: 389 | ANN: trainLoss: 0.2995 | trainAcc: 86.8490% (667/768)\n",
            "12 13 Epoch: 389 | ANN: trainLoss: 0.4345 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 389 | ANN: testLoss: 0.5461 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 389 | ANN: testLoss: 0.4900 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 389 | ANN: testLoss: 0.5204 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 389 | ANN: testLoss: 1.0818 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 390 | ANN: trainLoss: 0.3696 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 390 | ANN: trainLoss: 0.3243 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 390 | ANN: trainLoss: 0.3291 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 390 | ANN: trainLoss: 0.3241 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 390 | ANN: trainLoss: 0.3179 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 390 | ANN: trainLoss: 0.3257 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 390 | ANN: trainLoss: 0.3205 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 390 | ANN: trainLoss: 0.3225 | trainAcc: 85.9375% (440/512)\n",
            "8 13 Epoch: 390 | ANN: trainLoss: 0.3135 | trainAcc: 86.4583% (498/576)\n",
            "9 13 Epoch: 390 | ANN: trainLoss: 0.3161 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 390 | ANN: trainLoss: 0.3119 | trainAcc: 86.5057% (609/704)\n",
            "11 13 Epoch: 390 | ANN: trainLoss: 0.3085 | trainAcc: 86.8490% (667/768)\n",
            "12 13 Epoch: 390 | ANN: trainLoss: 0.3879 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 390 | ANN: testLoss: 0.5101 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 390 | ANN: testLoss: 0.5466 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 390 | ANN: testLoss: 0.5222 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 390 | ANN: testLoss: 0.5759 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 391 | ANN: trainLoss: 0.3660 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 391 | ANN: trainLoss: 0.3515 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 391 | ANN: trainLoss: 0.3295 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 391 | ANN: trainLoss: 0.3278 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 391 | ANN: trainLoss: 0.3091 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 391 | ANN: trainLoss: 0.2973 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 391 | ANN: trainLoss: 0.2941 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 391 | ANN: trainLoss: 0.2874 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 391 | ANN: trainLoss: 0.2881 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 391 | ANN: trainLoss: 0.2890 | trainAcc: 86.2500% (552/640)\n",
            "10 13 Epoch: 391 | ANN: trainLoss: 0.2827 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 391 | ANN: trainLoss: 0.2759 | trainAcc: 86.9792% (668/768)\n",
            "12 13 Epoch: 391 | ANN: trainLoss: 0.3504 | trainAcc: 86.7704% (669/771)\n",
            "0 4 Epoch: 391 | ANN: testLoss: 0.6129 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 391 | ANN: testLoss: 0.5858 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 391 | ANN: testLoss: 0.5344 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 391 | ANN: testLoss: 0.4820 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 392 | ANN: trainLoss: 0.2599 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 392 | ANN: trainLoss: 0.2604 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 392 | ANN: trainLoss: 0.2790 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 392 | ANN: trainLoss: 0.2806 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 392 | ANN: trainLoss: 0.2888 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 392 | ANN: trainLoss: 0.3008 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 392 | ANN: trainLoss: 0.2946 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 392 | ANN: trainLoss: 0.2941 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 392 | ANN: trainLoss: 0.2866 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 392 | ANN: trainLoss: 0.2897 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 392 | ANN: trainLoss: 0.2968 | trainAcc: 87.6420% (617/704)\n",
            "11 13 Epoch: 392 | ANN: trainLoss: 0.3011 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 392 | ANN: trainLoss: 0.3807 | trainAcc: 86.5110% (667/771)\n",
            "0 4 Epoch: 392 | ANN: testLoss: 0.4319 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 392 | ANN: testLoss: 0.5854 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 392 | ANN: testLoss: 0.5386 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 392 | ANN: testLoss: 0.4040 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 393 | ANN: trainLoss: 0.3308 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 393 | ANN: trainLoss: 0.2992 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 393 | ANN: trainLoss: 0.3355 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 393 | ANN: trainLoss: 0.3195 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 393 | ANN: trainLoss: 0.3345 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 393 | ANN: trainLoss: 0.3257 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 393 | ANN: trainLoss: 0.3252 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 393 | ANN: trainLoss: 0.3200 | trainAcc: 85.7422% (439/512)\n",
            "8 13 Epoch: 393 | ANN: trainLoss: 0.3351 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 393 | ANN: trainLoss: 0.3150 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 393 | ANN: trainLoss: 0.3085 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 393 | ANN: trainLoss: 0.2998 | trainAcc: 86.9792% (668/768)\n",
            "12 13 Epoch: 393 | ANN: trainLoss: 0.2783 | trainAcc: 87.0298% (671/771)\n",
            "0 4 Epoch: 393 | ANN: testLoss: 0.4316 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 393 | ANN: testLoss: 0.4830 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 393 | ANN: testLoss: 0.5322 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 393 | ANN: testLoss: 0.5590 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 394 | ANN: trainLoss: 0.2964 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 394 | ANN: trainLoss: 0.3157 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 394 | ANN: trainLoss: 0.2940 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 394 | ANN: trainLoss: 0.2638 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 394 | ANN: trainLoss: 0.2722 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 394 | ANN: trainLoss: 0.2663 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 394 | ANN: trainLoss: 0.2764 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 394 | ANN: trainLoss: 0.2787 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 394 | ANN: trainLoss: 0.2837 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 394 | ANN: trainLoss: 0.2798 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 394 | ANN: trainLoss: 0.2887 | trainAcc: 87.9261% (619/704)\n",
            "11 13 Epoch: 394 | ANN: trainLoss: 0.2997 | trainAcc: 86.9792% (668/768)\n",
            "12 13 Epoch: 394 | ANN: trainLoss: 0.2784 | trainAcc: 87.0298% (671/771)\n",
            "0 4 Epoch: 394 | ANN: testLoss: 0.5111 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 394 | ANN: testLoss: 0.5016 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 394 | ANN: testLoss: 0.5375 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 394 | ANN: testLoss: 0.4038 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 395 | ANN: trainLoss: 0.2303 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 395 | ANN: trainLoss: 0.3001 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 395 | ANN: trainLoss: 0.2829 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 395 | ANN: trainLoss: 0.2793 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 395 | ANN: trainLoss: 0.3081 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 395 | ANN: trainLoss: 0.3095 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 395 | ANN: trainLoss: 0.2940 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 395 | ANN: trainLoss: 0.2979 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 395 | ANN: trainLoss: 0.3019 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 395 | ANN: trainLoss: 0.3050 | trainAcc: 87.6562% (561/640)\n",
            "10 13 Epoch: 395 | ANN: trainLoss: 0.3038 | trainAcc: 87.7841% (618/704)\n",
            "11 13 Epoch: 395 | ANN: trainLoss: 0.2988 | trainAcc: 88.2812% (678/768)\n",
            "12 13 Epoch: 395 | ANN: trainLoss: 0.2941 | trainAcc: 88.3268% (681/771)\n",
            "0 4 Epoch: 395 | ANN: testLoss: 0.6224 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 395 | ANN: testLoss: 0.5869 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 395 | ANN: testLoss: 0.5395 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 395 | ANN: testLoss: 0.4048 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 396 | ANN: trainLoss: 0.2946 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 396 | ANN: trainLoss: 0.3417 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 396 | ANN: trainLoss: 0.3195 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 396 | ANN: trainLoss: 0.2820 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 396 | ANN: trainLoss: 0.2693 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 396 | ANN: trainLoss: 0.2604 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 396 | ANN: trainLoss: 0.2744 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 396 | ANN: trainLoss: 0.2766 | trainAcc: 88.6719% (454/512)\n",
            "8 13 Epoch: 396 | ANN: trainLoss: 0.2691 | trainAcc: 89.2361% (514/576)\n",
            "9 13 Epoch: 396 | ANN: trainLoss: 0.2724 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 396 | ANN: trainLoss: 0.2680 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 396 | ANN: trainLoss: 0.2675 | trainAcc: 89.5833% (688/768)\n",
            "12 13 Epoch: 396 | ANN: trainLoss: 0.2488 | trainAcc: 89.6239% (691/771)\n",
            "0 4 Epoch: 396 | ANN: testLoss: 0.5269 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 396 | ANN: testLoss: 0.5388 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 396 | ANN: testLoss: 0.5435 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 396 | ANN: testLoss: 0.6154 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 397 | ANN: trainLoss: 0.2406 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 397 | ANN: trainLoss: 0.2430 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 397 | ANN: trainLoss: 0.2432 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 397 | ANN: trainLoss: 0.2280 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 397 | ANN: trainLoss: 0.2474 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 397 | ANN: trainLoss: 0.2627 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 397 | ANN: trainLoss: 0.2625 | trainAcc: 89.7321% (402/448)\n",
            "7 13 Epoch: 397 | ANN: trainLoss: 0.2580 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 397 | ANN: trainLoss: 0.2564 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 397 | ANN: trainLoss: 0.2640 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 397 | ANN: trainLoss: 0.2616 | trainAcc: 89.2045% (628/704)\n",
            "11 13 Epoch: 397 | ANN: trainLoss: 0.2646 | trainAcc: 89.0625% (684/768)\n",
            "12 13 Epoch: 397 | ANN: trainLoss: 0.4777 | trainAcc: 88.8457% (685/771)\n",
            "0 4 Epoch: 397 | ANN: testLoss: 0.5221 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 397 | ANN: testLoss: 0.6081 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 397 | ANN: testLoss: 0.5296 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 397 | ANN: testLoss: 0.6502 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 398 | ANN: trainLoss: 0.2483 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 398 | ANN: trainLoss: 0.2583 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 398 | ANN: trainLoss: 0.3057 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 398 | ANN: trainLoss: 0.2711 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 398 | ANN: trainLoss: 0.2785 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 398 | ANN: trainLoss: 0.2760 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 398 | ANN: trainLoss: 0.2854 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 398 | ANN: trainLoss: 0.2825 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 398 | ANN: trainLoss: 0.2820 | trainAcc: 87.5000% (504/576)\n",
            "9 13 Epoch: 398 | ANN: trainLoss: 0.2919 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 398 | ANN: trainLoss: 0.2846 | trainAcc: 87.0739% (613/704)\n",
            "11 13 Epoch: 398 | ANN: trainLoss: 0.2928 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 398 | ANN: trainLoss: 0.3173 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 398 | ANN: testLoss: 0.6738 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 398 | ANN: testLoss: 0.5761 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 398 | ANN: testLoss: 0.5405 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 398 | ANN: testLoss: 0.4238 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 399 | ANN: trainLoss: 0.2764 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 399 | ANN: trainLoss: 0.2987 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 399 | ANN: trainLoss: 0.2878 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 399 | ANN: trainLoss: 0.2878 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 399 | ANN: trainLoss: 0.2854 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 399 | ANN: trainLoss: 0.2859 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 399 | ANN: trainLoss: 0.2832 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 399 | ANN: trainLoss: 0.2860 | trainAcc: 88.0859% (451/512)\n",
            "8 13 Epoch: 399 | ANN: trainLoss: 0.2763 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 399 | ANN: trainLoss: 0.2810 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 399 | ANN: trainLoss: 0.2781 | trainAcc: 88.2102% (621/704)\n",
            "11 13 Epoch: 399 | ANN: trainLoss: 0.2769 | trainAcc: 88.5417% (680/768)\n",
            "12 13 Epoch: 399 | ANN: trainLoss: 0.2955 | trainAcc: 88.4565% (682/771)\n",
            "0 4 Epoch: 399 | ANN: testLoss: 0.5425 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 399 | ANN: testLoss: 0.5051 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 399 | ANN: testLoss: 0.5374 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 399 | ANN: testLoss: 0.5109 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 400 | ANN: trainLoss: 0.3181 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 400 | ANN: trainLoss: 0.2578 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 400 | ANN: trainLoss: 0.2783 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 400 | ANN: trainLoss: 0.2813 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 400 | ANN: trainLoss: 0.2775 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 400 | ANN: trainLoss: 0.2911 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 400 | ANN: trainLoss: 0.3011 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 400 | ANN: trainLoss: 0.3008 | trainAcc: 87.3047% (447/512)\n",
            "8 13 Epoch: 400 | ANN: trainLoss: 0.3185 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 400 | ANN: trainLoss: 0.3179 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 400 | ANN: trainLoss: 0.3125 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 400 | ANN: trainLoss: 0.3057 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 400 | ANN: trainLoss: 0.2859 | trainAcc: 86.7704% (669/771)\n",
            "0 4 Epoch: 400 | ANN: testLoss: 0.5268 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 400 | ANN: testLoss: 0.4906 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 400 | ANN: testLoss: 0.5474 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 400 | ANN: testLoss: 0.4105 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 401 | ANN: trainLoss: 0.1809 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 401 | ANN: trainLoss: 0.1993 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 401 | ANN: trainLoss: 0.2374 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 401 | ANN: trainLoss: 0.2555 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 401 | ANN: trainLoss: 0.2560 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 401 | ANN: trainLoss: 0.2535 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 401 | ANN: trainLoss: 0.2561 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 401 | ANN: trainLoss: 0.2497 | trainAcc: 88.6719% (454/512)\n",
            "8 13 Epoch: 401 | ANN: trainLoss: 0.2528 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 401 | ANN: trainLoss: 0.2619 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 401 | ANN: trainLoss: 0.2598 | trainAcc: 88.2102% (621/704)\n",
            "11 13 Epoch: 401 | ANN: trainLoss: 0.2605 | trainAcc: 88.1510% (677/768)\n",
            "12 13 Epoch: 401 | ANN: trainLoss: 0.3906 | trainAcc: 87.9377% (678/771)\n",
            "0 4 Epoch: 401 | ANN: testLoss: 0.5270 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 401 | ANN: testLoss: 0.5898 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 401 | ANN: testLoss: 0.5437 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 401 | ANN: testLoss: 0.7652 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 402 | ANN: trainLoss: 0.2501 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 402 | ANN: trainLoss: 0.2782 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 402 | ANN: trainLoss: 0.3254 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 402 | ANN: trainLoss: 0.3041 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 402 | ANN: trainLoss: 0.3158 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 402 | ANN: trainLoss: 0.3117 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 402 | ANN: trainLoss: 0.3122 | trainAcc: 86.3839% (387/448)\n",
            "7 13 Epoch: 402 | ANN: trainLoss: 0.3130 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 402 | ANN: trainLoss: 0.3029 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 402 | ANN: trainLoss: 0.3006 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 402 | ANN: trainLoss: 0.2960 | trainAcc: 87.0739% (613/704)\n",
            "11 13 Epoch: 402 | ANN: trainLoss: 0.2966 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 402 | ANN: trainLoss: 0.2946 | trainAcc: 86.7704% (669/771)\n",
            "0 4 Epoch: 402 | ANN: testLoss: 0.6023 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 402 | ANN: testLoss: 0.5154 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 402 | ANN: testLoss: 0.5538 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 402 | ANN: testLoss: 0.5917 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 403 | ANN: trainLoss: 0.3302 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 403 | ANN: trainLoss: 0.3165 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 403 | ANN: trainLoss: 0.3189 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 403 | ANN: trainLoss: 0.3208 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 403 | ANN: trainLoss: 0.3104 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 403 | ANN: trainLoss: 0.3001 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 403 | ANN: trainLoss: 0.3071 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 403 | ANN: trainLoss: 0.3007 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 403 | ANN: trainLoss: 0.2972 | trainAcc: 88.1944% (508/576)\n",
            "9 13 Epoch: 403 | ANN: trainLoss: 0.2878 | trainAcc: 88.7500% (568/640)\n",
            "10 13 Epoch: 403 | ANN: trainLoss: 0.2835 | trainAcc: 89.2045% (628/704)\n",
            "11 13 Epoch: 403 | ANN: trainLoss: 0.2808 | trainAcc: 89.1927% (685/768)\n",
            "12 13 Epoch: 403 | ANN: trainLoss: 0.5035 | trainAcc: 88.9754% (686/771)\n",
            "0 4 Epoch: 403 | ANN: testLoss: 0.7022 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 403 | ANN: testLoss: 0.6224 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 403 | ANN: testLoss: 0.5777 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 403 | ANN: testLoss: 0.7641 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 404 | ANN: trainLoss: 0.2849 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 404 | ANN: trainLoss: 0.2806 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 404 | ANN: trainLoss: 0.2642 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 404 | ANN: trainLoss: 0.3061 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 404 | ANN: trainLoss: 0.2981 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 404 | ANN: trainLoss: 0.3082 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 404 | ANN: trainLoss: 0.2962 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 404 | ANN: trainLoss: 0.3038 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 404 | ANN: trainLoss: 0.2948 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 404 | ANN: trainLoss: 0.2878 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 404 | ANN: trainLoss: 0.2887 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 404 | ANN: trainLoss: 0.2905 | trainAcc: 87.7604% (674/768)\n",
            "12 13 Epoch: 404 | ANN: trainLoss: 0.2831 | trainAcc: 87.8080% (677/771)\n",
            "0 4 Epoch: 404 | ANN: testLoss: 0.6761 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 404 | ANN: testLoss: 0.5872 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 404 | ANN: testLoss: 0.5591 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 404 | ANN: testLoss: 0.4196 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 405 | ANN: trainLoss: 0.2818 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 405 | ANN: trainLoss: 0.2670 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 405 | ANN: trainLoss: 0.2683 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 405 | ANN: trainLoss: 0.2669 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 405 | ANN: trainLoss: 0.2784 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 405 | ANN: trainLoss: 0.3023 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 405 | ANN: trainLoss: 0.2878 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 405 | ANN: trainLoss: 0.2850 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 405 | ANN: trainLoss: 0.2866 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 405 | ANN: trainLoss: 0.2807 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 405 | ANN: trainLoss: 0.2860 | trainAcc: 88.6364% (624/704)\n",
            "11 13 Epoch: 405 | ANN: trainLoss: 0.2901 | trainAcc: 88.2812% (678/768)\n",
            "12 13 Epoch: 405 | ANN: trainLoss: 0.3568 | trainAcc: 88.1971% (680/771)\n",
            "0 4 Epoch: 405 | ANN: testLoss: 0.4263 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 405 | ANN: testLoss: 0.4405 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 405 | ANN: testLoss: 0.5466 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 405 | ANN: testLoss: 0.4101 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 406 | ANN: trainLoss: 0.3282 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 406 | ANN: trainLoss: 0.3297 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 406 | ANN: trainLoss: 0.2842 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 406 | ANN: trainLoss: 0.2750 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 406 | ANN: trainLoss: 0.2929 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 406 | ANN: trainLoss: 0.2904 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 406 | ANN: trainLoss: 0.2894 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 406 | ANN: trainLoss: 0.2908 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 406 | ANN: trainLoss: 0.2848 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 406 | ANN: trainLoss: 0.2871 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 406 | ANN: trainLoss: 0.2958 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 406 | ANN: trainLoss: 0.2852 | trainAcc: 88.6719% (681/768)\n",
            "12 13 Epoch: 406 | ANN: trainLoss: 0.2965 | trainAcc: 88.7160% (684/771)\n",
            "0 4 Epoch: 406 | ANN: testLoss: 0.6267 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 406 | ANN: testLoss: 0.6091 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 406 | ANN: testLoss: 0.5446 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 406 | ANN: testLoss: 0.4085 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 407 | ANN: trainLoss: 0.2294 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 407 | ANN: trainLoss: 0.2287 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 407 | ANN: trainLoss: 0.2460 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 407 | ANN: trainLoss: 0.2399 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 407 | ANN: trainLoss: 0.2308 | trainAcc: 90.0000% (288/320)\n",
            "5 13 Epoch: 407 | ANN: trainLoss: 0.2371 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 407 | ANN: trainLoss: 0.2495 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 407 | ANN: trainLoss: 0.2548 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 407 | ANN: trainLoss: 0.2525 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 407 | ANN: trainLoss: 0.2588 | trainAcc: 88.7500% (568/640)\n",
            "10 13 Epoch: 407 | ANN: trainLoss: 0.2657 | trainAcc: 88.6364% (624/704)\n",
            "11 13 Epoch: 407 | ANN: trainLoss: 0.2617 | trainAcc: 88.6719% (681/768)\n",
            "12 13 Epoch: 407 | ANN: trainLoss: 0.2825 | trainAcc: 88.5863% (683/771)\n",
            "0 4 Epoch: 407 | ANN: testLoss: 0.5641 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 407 | ANN: testLoss: 0.5465 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 407 | ANN: testLoss: 0.5311 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 407 | ANN: testLoss: 0.5771 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 408 | ANN: trainLoss: 0.2153 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 408 | ANN: trainLoss: 0.2308 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 408 | ANN: trainLoss: 0.2705 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 408 | ANN: trainLoss: 0.2869 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 408 | ANN: trainLoss: 0.2933 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 408 | ANN: trainLoss: 0.2720 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 408 | ANN: trainLoss: 0.2764 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 408 | ANN: trainLoss: 0.2887 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 408 | ANN: trainLoss: 0.2966 | trainAcc: 87.1528% (502/576)\n",
            "9 13 Epoch: 408 | ANN: trainLoss: 0.2850 | trainAcc: 87.8125% (562/640)\n",
            "10 13 Epoch: 408 | ANN: trainLoss: 0.2903 | trainAcc: 87.7841% (618/704)\n",
            "11 13 Epoch: 408 | ANN: trainLoss: 0.2930 | trainAcc: 87.3698% (671/768)\n",
            "12 13 Epoch: 408 | ANN: trainLoss: 0.2949 | trainAcc: 87.2892% (673/771)\n",
            "0 4 Epoch: 408 | ANN: testLoss: 0.5515 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 408 | ANN: testLoss: 0.5075 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 408 | ANN: testLoss: 0.5370 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 408 | ANN: testLoss: 0.4028 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 409 | ANN: trainLoss: 0.2357 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 409 | ANN: trainLoss: 0.2327 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 409 | ANN: trainLoss: 0.2523 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 409 | ANN: trainLoss: 0.2826 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 409 | ANN: trainLoss: 0.2938 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 409 | ANN: trainLoss: 0.2815 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 409 | ANN: trainLoss: 0.2816 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 409 | ANN: trainLoss: 0.3031 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 409 | ANN: trainLoss: 0.3089 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 409 | ANN: trainLoss: 0.3096 | trainAcc: 86.2500% (552/640)\n",
            "10 13 Epoch: 409 | ANN: trainLoss: 0.3027 | trainAcc: 86.5057% (609/704)\n",
            "11 13 Epoch: 409 | ANN: trainLoss: 0.2986 | trainAcc: 86.9792% (668/768)\n",
            "12 13 Epoch: 409 | ANN: trainLoss: 0.3443 | trainAcc: 86.9001% (670/771)\n",
            "0 4 Epoch: 409 | ANN: testLoss: 0.4705 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 409 | ANN: testLoss: 0.6018 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 409 | ANN: testLoss: 0.5500 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 409 | ANN: testLoss: 0.4280 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 410 | ANN: trainLoss: 0.2880 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 410 | ANN: trainLoss: 0.2906 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 410 | ANN: trainLoss: 0.2705 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 410 | ANN: trainLoss: 0.2582 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 410 | ANN: trainLoss: 0.2816 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 410 | ANN: trainLoss: 0.2796 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 410 | ANN: trainLoss: 0.2744 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 410 | ANN: trainLoss: 0.2760 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 410 | ANN: trainLoss: 0.2662 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 410 | ANN: trainLoss: 0.2675 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 410 | ANN: trainLoss: 0.2590 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 410 | ANN: trainLoss: 0.2590 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 410 | ANN: trainLoss: 0.2977 | trainAcc: 89.1051% (687/771)\n",
            "0 4 Epoch: 410 | ANN: testLoss: 0.4487 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 410 | ANN: testLoss: 0.5226 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 410 | ANN: testLoss: 0.5410 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 410 | ANN: testLoss: 0.4072 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 411 | ANN: trainLoss: 0.3472 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 411 | ANN: trainLoss: 0.3725 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 411 | ANN: trainLoss: 0.3210 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 411 | ANN: trainLoss: 0.2972 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 411 | ANN: trainLoss: 0.2853 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 411 | ANN: trainLoss: 0.2980 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 411 | ANN: trainLoss: 0.3042 | trainAcc: 86.6071% (388/448)\n",
            "7 13 Epoch: 411 | ANN: trainLoss: 0.2961 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 411 | ANN: trainLoss: 0.2966 | trainAcc: 86.6319% (499/576)\n",
            "9 13 Epoch: 411 | ANN: trainLoss: 0.2976 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 411 | ANN: trainLoss: 0.2939 | trainAcc: 86.9318% (612/704)\n",
            "11 13 Epoch: 411 | ANN: trainLoss: 0.2938 | trainAcc: 86.8490% (667/768)\n",
            "12 13 Epoch: 411 | ANN: trainLoss: 0.5569 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 411 | ANN: testLoss: 0.5203 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 411 | ANN: testLoss: 0.5153 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 411 | ANN: testLoss: 0.5413 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 411 | ANN: testLoss: 0.4060 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 412 | ANN: trainLoss: 0.2777 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 412 | ANN: trainLoss: 0.2533 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 412 | ANN: trainLoss: 0.2449 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 412 | ANN: trainLoss: 0.2348 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 412 | ANN: trainLoss: 0.2574 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 412 | ANN: trainLoss: 0.2489 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 412 | ANN: trainLoss: 0.2495 | trainAcc: 90.1786% (404/448)\n",
            "7 13 Epoch: 412 | ANN: trainLoss: 0.2623 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 412 | ANN: trainLoss: 0.2724 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 412 | ANN: trainLoss: 0.2692 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 412 | ANN: trainLoss: 0.2768 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 412 | ANN: trainLoss: 0.2779 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 412 | ANN: trainLoss: 0.2719 | trainAcc: 88.4565% (682/771)\n",
            "0 4 Epoch: 412 | ANN: testLoss: 0.4798 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 412 | ANN: testLoss: 0.4702 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 412 | ANN: testLoss: 0.5272 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 412 | ANN: testLoss: 0.4505 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 413 | ANN: trainLoss: 0.1898 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 413 | ANN: trainLoss: 0.2688 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 413 | ANN: trainLoss: 0.2520 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 413 | ANN: trainLoss: 0.2459 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 413 | ANN: trainLoss: 0.2430 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 413 | ANN: trainLoss: 0.2399 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 413 | ANN: trainLoss: 0.2415 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 413 | ANN: trainLoss: 0.2442 | trainAcc: 91.2109% (467/512)\n",
            "8 13 Epoch: 413 | ANN: trainLoss: 0.2495 | trainAcc: 90.2778% (520/576)\n",
            "9 13 Epoch: 413 | ANN: trainLoss: 0.2541 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 413 | ANN: trainLoss: 0.2552 | trainAcc: 90.6250% (638/704)\n",
            "11 13 Epoch: 413 | ANN: trainLoss: 0.2532 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 413 | ANN: trainLoss: 0.2368 | trainAcc: 90.9209% (701/771)\n",
            "0 4 Epoch: 413 | ANN: testLoss: 0.5876 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 413 | ANN: testLoss: 0.5383 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 413 | ANN: testLoss: 0.5269 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 413 | ANN: testLoss: 0.3959 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 414 | ANN: trainLoss: 0.2823 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 414 | ANN: trainLoss: 0.2940 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 414 | ANN: trainLoss: 0.2782 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 414 | ANN: trainLoss: 0.2700 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 414 | ANN: trainLoss: 0.2653 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 414 | ANN: trainLoss: 0.2786 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 414 | ANN: trainLoss: 0.2813 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 414 | ANN: trainLoss: 0.2835 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 414 | ANN: trainLoss: 0.2787 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 414 | ANN: trainLoss: 0.2879 | trainAcc: 87.6562% (561/640)\n",
            "10 13 Epoch: 414 | ANN: trainLoss: 0.2952 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 414 | ANN: trainLoss: 0.3040 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 414 | ANN: trainLoss: 0.4275 | trainAcc: 86.5110% (667/771)\n",
            "0 4 Epoch: 414 | ANN: testLoss: 0.4967 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 414 | ANN: testLoss: 0.4938 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 414 | ANN: testLoss: 0.5295 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 414 | ANN: testLoss: 0.5597 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 415 | ANN: trainLoss: 0.2391 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 415 | ANN: trainLoss: 0.2869 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 415 | ANN: trainLoss: 0.2833 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 415 | ANN: trainLoss: 0.2722 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 415 | ANN: trainLoss: 0.2867 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 415 | ANN: trainLoss: 0.2834 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 415 | ANN: trainLoss: 0.2882 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 415 | ANN: trainLoss: 0.2808 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 415 | ANN: trainLoss: 0.2819 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 415 | ANN: trainLoss: 0.2827 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 415 | ANN: trainLoss: 0.2805 | trainAcc: 87.7841% (618/704)\n",
            "11 13 Epoch: 415 | ANN: trainLoss: 0.2856 | trainAcc: 87.6302% (673/768)\n",
            "12 13 Epoch: 415 | ANN: trainLoss: 0.2736 | trainAcc: 87.6783% (676/771)\n",
            "0 4 Epoch: 415 | ANN: testLoss: 0.6346 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 415 | ANN: testLoss: 0.5676 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 415 | ANN: testLoss: 0.5324 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 415 | ANN: testLoss: 0.3996 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 416 | ANN: trainLoss: 0.2480 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 416 | ANN: trainLoss: 0.2680 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 416 | ANN: trainLoss: 0.2916 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 416 | ANN: trainLoss: 0.2848 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 416 | ANN: trainLoss: 0.2897 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 416 | ANN: trainLoss: 0.2951 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 416 | ANN: trainLoss: 0.2960 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 416 | ANN: trainLoss: 0.2991 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 416 | ANN: trainLoss: 0.2964 | trainAcc: 88.1944% (508/576)\n",
            "9 13 Epoch: 416 | ANN: trainLoss: 0.2963 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 416 | ANN: trainLoss: 0.2913 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 416 | ANN: trainLoss: 0.2840 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 416 | ANN: trainLoss: 0.2744 | trainAcc: 88.4565% (682/771)\n",
            "0 4 Epoch: 416 | ANN: testLoss: 0.6438 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 416 | ANN: testLoss: 0.5861 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 416 | ANN: testLoss: 0.5324 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 416 | ANN: testLoss: 0.4046 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 417 | ANN: trainLoss: 0.3442 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 417 | ANN: trainLoss: 0.3253 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 417 | ANN: trainLoss: 0.2958 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 417 | ANN: trainLoss: 0.2960 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 417 | ANN: trainLoss: 0.3151 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 417 | ANN: trainLoss: 0.3092 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 417 | ANN: trainLoss: 0.3145 | trainAcc: 86.1607% (386/448)\n",
            "7 13 Epoch: 417 | ANN: trainLoss: 0.3102 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 417 | ANN: trainLoss: 0.3008 | trainAcc: 86.6319% (499/576)\n",
            "9 13 Epoch: 417 | ANN: trainLoss: 0.2965 | trainAcc: 87.0312% (557/640)\n",
            "10 13 Epoch: 417 | ANN: trainLoss: 0.3018 | trainAcc: 86.9318% (612/704)\n",
            "11 13 Epoch: 417 | ANN: trainLoss: 0.2924 | trainAcc: 87.6302% (673/768)\n",
            "12 13 Epoch: 417 | ANN: trainLoss: 0.2738 | trainAcc: 87.6783% (676/771)\n",
            "0 4 Epoch: 417 | ANN: testLoss: 0.5119 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 417 | ANN: testLoss: 0.5644 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 417 | ANN: testLoss: 0.5310 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 417 | ANN: testLoss: 0.3983 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 418 | ANN: trainLoss: 0.4001 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 418 | ANN: trainLoss: 0.3284 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 418 | ANN: trainLoss: 0.3667 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 418 | ANN: trainLoss: 0.3393 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 418 | ANN: trainLoss: 0.3267 | trainAcc: 84.3750% (270/320)\n",
            "5 13 Epoch: 418 | ANN: trainLoss: 0.3246 | trainAcc: 84.3750% (324/384)\n",
            "6 13 Epoch: 418 | ANN: trainLoss: 0.3188 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 418 | ANN: trainLoss: 0.3184 | trainAcc: 84.7656% (434/512)\n",
            "8 13 Epoch: 418 | ANN: trainLoss: 0.3178 | trainAcc: 84.8958% (489/576)\n",
            "9 13 Epoch: 418 | ANN: trainLoss: 0.3138 | trainAcc: 85.1562% (545/640)\n",
            "10 13 Epoch: 418 | ANN: trainLoss: 0.3046 | trainAcc: 85.7955% (604/704)\n",
            "11 13 Epoch: 418 | ANN: trainLoss: 0.3104 | trainAcc: 85.4167% (656/768)\n",
            "12 13 Epoch: 418 | ANN: trainLoss: 0.3055 | trainAcc: 85.4734% (659/771)\n",
            "0 4 Epoch: 418 | ANN: testLoss: 0.5493 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 418 | ANN: testLoss: 0.5420 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 418 | ANN: testLoss: 0.5287 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 418 | ANN: testLoss: 0.8317 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 419 | ANN: trainLoss: 0.2526 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 419 | ANN: trainLoss: 0.2509 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 419 | ANN: trainLoss: 0.2486 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 419 | ANN: trainLoss: 0.2335 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 419 | ANN: trainLoss: 0.2657 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 419 | ANN: trainLoss: 0.2646 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 419 | ANN: trainLoss: 0.2605 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 419 | ANN: trainLoss: 0.2652 | trainAcc: 89.4531% (458/512)\n",
            "8 13 Epoch: 419 | ANN: trainLoss: 0.2666 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 419 | ANN: trainLoss: 0.2644 | trainAcc: 89.5312% (573/640)\n",
            "10 13 Epoch: 419 | ANN: trainLoss: 0.2666 | trainAcc: 89.7727% (632/704)\n",
            "11 13 Epoch: 419 | ANN: trainLoss: 0.2621 | trainAcc: 89.9740% (691/768)\n",
            "12 13 Epoch: 419 | ANN: trainLoss: 0.2508 | trainAcc: 90.0130% (694/771)\n",
            "0 4 Epoch: 419 | ANN: testLoss: 0.5754 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 419 | ANN: testLoss: 0.4981 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 419 | ANN: testLoss: 0.5300 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 419 | ANN: testLoss: 0.4599 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 420 | ANN: trainLoss: 0.3593 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 420 | ANN: trainLoss: 0.3013 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 420 | ANN: trainLoss: 0.3118 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 420 | ANN: trainLoss: 0.3542 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 420 | ANN: trainLoss: 0.3243 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 420 | ANN: trainLoss: 0.3200 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 420 | ANN: trainLoss: 0.3227 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 420 | ANN: trainLoss: 0.3288 | trainAcc: 84.7656% (434/512)\n",
            "8 13 Epoch: 420 | ANN: trainLoss: 0.3197 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 420 | ANN: trainLoss: 0.3217 | trainAcc: 85.3125% (546/640)\n",
            "10 13 Epoch: 420 | ANN: trainLoss: 0.3176 | trainAcc: 85.6534% (603/704)\n",
            "11 13 Epoch: 420 | ANN: trainLoss: 0.3137 | trainAcc: 85.8073% (659/768)\n",
            "12 13 Epoch: 420 | ANN: trainLoss: 0.4005 | trainAcc: 85.7328% (661/771)\n",
            "0 4 Epoch: 420 | ANN: testLoss: 0.4825 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 420 | ANN: testLoss: 0.4966 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 420 | ANN: testLoss: 0.5301 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 420 | ANN: testLoss: 0.3983 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 421 | ANN: trainLoss: 0.2981 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 421 | ANN: trainLoss: 0.3205 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 421 | ANN: trainLoss: 0.3557 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 421 | ANN: trainLoss: 0.3217 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 421 | ANN: trainLoss: 0.3247 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 421 | ANN: trainLoss: 0.3072 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 421 | ANN: trainLoss: 0.3074 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 421 | ANN: trainLoss: 0.2993 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 421 | ANN: trainLoss: 0.2901 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 421 | ANN: trainLoss: 0.2906 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 421 | ANN: trainLoss: 0.2840 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 421 | ANN: trainLoss: 0.2881 | trainAcc: 87.5000% (672/768)\n",
            "12 13 Epoch: 421 | ANN: trainLoss: 0.2672 | trainAcc: 87.5486% (675/771)\n",
            "0 4 Epoch: 421 | ANN: testLoss: 0.5596 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 421 | ANN: testLoss: 0.5470 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 421 | ANN: testLoss: 0.5285 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 421 | ANN: testLoss: 0.3982 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 422 | ANN: trainLoss: 0.2125 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 422 | ANN: trainLoss: 0.2654 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 422 | ANN: trainLoss: 0.2687 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 422 | ANN: trainLoss: 0.2855 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 422 | ANN: trainLoss: 0.2815 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 422 | ANN: trainLoss: 0.2955 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 422 | ANN: trainLoss: 0.2817 | trainAcc: 86.1607% (386/448)\n",
            "7 13 Epoch: 422 | ANN: trainLoss: 0.2844 | trainAcc: 85.9375% (440/512)\n",
            "8 13 Epoch: 422 | ANN: trainLoss: 0.2741 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 422 | ANN: trainLoss: 0.2664 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 422 | ANN: trainLoss: 0.2720 | trainAcc: 87.3580% (615/704)\n",
            "11 13 Epoch: 422 | ANN: trainLoss: 0.2662 | trainAcc: 87.8906% (675/768)\n",
            "12 13 Epoch: 422 | ANN: trainLoss: 0.2658 | trainAcc: 87.9377% (678/771)\n",
            "0 4 Epoch: 422 | ANN: testLoss: 0.4491 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 422 | ANN: testLoss: 0.5352 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 422 | ANN: testLoss: 0.5149 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 422 | ANN: testLoss: 0.6616 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 423 | ANN: trainLoss: 0.2092 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 423 | ANN: trainLoss: 0.2273 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 423 | ANN: trainLoss: 0.2276 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 423 | ANN: trainLoss: 0.2425 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 423 | ANN: trainLoss: 0.2476 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 423 | ANN: trainLoss: 0.2611 | trainAcc: 90.8854% (349/384)\n",
            "6 13 Epoch: 423 | ANN: trainLoss: 0.2842 | trainAcc: 89.5089% (401/448)\n",
            "7 13 Epoch: 423 | ANN: trainLoss: 0.2876 | trainAcc: 89.2578% (457/512)\n",
            "8 13 Epoch: 423 | ANN: trainLoss: 0.2836 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 423 | ANN: trainLoss: 0.2901 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 423 | ANN: trainLoss: 0.2899 | trainAcc: 87.7841% (618/704)\n",
            "11 13 Epoch: 423 | ANN: trainLoss: 0.2875 | trainAcc: 87.8906% (675/768)\n",
            "12 13 Epoch: 423 | ANN: trainLoss: 0.2802 | trainAcc: 87.9377% (678/771)\n",
            "0 4 Epoch: 423 | ANN: testLoss: 0.5574 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 423 | ANN: testLoss: 0.5738 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 423 | ANN: testLoss: 0.5308 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 423 | ANN: testLoss: 0.3990 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 424 | ANN: trainLoss: 0.2415 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 424 | ANN: trainLoss: 0.3043 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 424 | ANN: trainLoss: 0.2992 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 424 | ANN: trainLoss: 0.3117 | trainAcc: 85.1562% (218/256)\n",
            "4 13 Epoch: 424 | ANN: trainLoss: 0.3362 | trainAcc: 84.3750% (270/320)\n",
            "5 13 Epoch: 424 | ANN: trainLoss: 0.3240 | trainAcc: 84.3750% (324/384)\n",
            "6 13 Epoch: 424 | ANN: trainLoss: 0.3292 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 424 | ANN: trainLoss: 0.3268 | trainAcc: 84.9609% (435/512)\n",
            "8 13 Epoch: 424 | ANN: trainLoss: 0.3220 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 424 | ANN: trainLoss: 0.3319 | trainAcc: 84.3750% (540/640)\n",
            "10 13 Epoch: 424 | ANN: trainLoss: 0.3178 | trainAcc: 85.2273% (600/704)\n",
            "11 13 Epoch: 424 | ANN: trainLoss: 0.3143 | trainAcc: 85.5469% (657/768)\n",
            "12 13 Epoch: 424 | ANN: trainLoss: 0.3047 | trainAcc: 85.6031% (660/771)\n",
            "0 4 Epoch: 424 | ANN: testLoss: 0.5834 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 424 | ANN: testLoss: 0.4755 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 424 | ANN: testLoss: 0.5249 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 424 | ANN: testLoss: 0.6511 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 425 | ANN: trainLoss: 0.1943 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 425 | ANN: trainLoss: 0.2243 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 425 | ANN: trainLoss: 0.2300 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 425 | ANN: trainLoss: 0.2407 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 425 | ANN: trainLoss: 0.2602 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 425 | ANN: trainLoss: 0.2554 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 425 | ANN: trainLoss: 0.2541 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 425 | ANN: trainLoss: 0.2631 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 425 | ANN: trainLoss: 0.2578 | trainAcc: 88.3681% (509/576)\n",
            "9 13 Epoch: 425 | ANN: trainLoss: 0.2572 | trainAcc: 88.4375% (566/640)\n",
            "10 13 Epoch: 425 | ANN: trainLoss: 0.2609 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 425 | ANN: trainLoss: 0.2712 | trainAcc: 87.6302% (673/768)\n",
            "12 13 Epoch: 425 | ANN: trainLoss: 0.2778 | trainAcc: 87.5486% (675/771)\n",
            "0 4 Epoch: 425 | ANN: testLoss: 0.5582 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 425 | ANN: testLoss: 0.5017 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 425 | ANN: testLoss: 0.5249 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 425 | ANN: testLoss: 0.5947 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 426 | ANN: trainLoss: 0.3590 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 426 | ANN: trainLoss: 0.3187 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 426 | ANN: trainLoss: 0.3145 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 426 | ANN: trainLoss: 0.3079 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 426 | ANN: trainLoss: 0.3235 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 426 | ANN: trainLoss: 0.3165 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 426 | ANN: trainLoss: 0.3127 | trainAcc: 86.3839% (387/448)\n",
            "7 13 Epoch: 426 | ANN: trainLoss: 0.3176 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 426 | ANN: trainLoss: 0.3106 | trainAcc: 86.1111% (496/576)\n",
            "9 13 Epoch: 426 | ANN: trainLoss: 0.3015 | trainAcc: 86.7188% (555/640)\n",
            "10 13 Epoch: 426 | ANN: trainLoss: 0.2946 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 426 | ANN: trainLoss: 0.2903 | trainAcc: 87.6302% (673/768)\n",
            "12 13 Epoch: 426 | ANN: trainLoss: 0.2999 | trainAcc: 87.5486% (675/771)\n",
            "0 4 Epoch: 426 | ANN: testLoss: 0.5860 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 426 | ANN: testLoss: 0.5643 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 426 | ANN: testLoss: 0.5318 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 426 | ANN: testLoss: 0.7167 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 427 | ANN: trainLoss: 0.2971 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 427 | ANN: trainLoss: 0.2884 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 427 | ANN: trainLoss: 0.2792 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 427 | ANN: trainLoss: 0.2700 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 427 | ANN: trainLoss: 0.2764 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 427 | ANN: trainLoss: 0.2894 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 427 | ANN: trainLoss: 0.2814 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 427 | ANN: trainLoss: 0.2806 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 427 | ANN: trainLoss: 0.2787 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 427 | ANN: trainLoss: 0.2807 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 427 | ANN: trainLoss: 0.2825 | trainAcc: 89.2045% (628/704)\n",
            "11 13 Epoch: 427 | ANN: trainLoss: 0.2859 | trainAcc: 88.9323% (683/768)\n",
            "12 13 Epoch: 427 | ANN: trainLoss: 0.2849 | trainAcc: 88.9754% (686/771)\n",
            "0 4 Epoch: 427 | ANN: testLoss: 0.4633 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 427 | ANN: testLoss: 0.4550 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 427 | ANN: testLoss: 0.5374 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 427 | ANN: testLoss: 0.4124 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 428 | ANN: trainLoss: 0.2414 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 428 | ANN: trainLoss: 0.2912 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 428 | ANN: trainLoss: 0.2845 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 428 | ANN: trainLoss: 0.2654 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 428 | ANN: trainLoss: 0.2773 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 428 | ANN: trainLoss: 0.2620 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 428 | ANN: trainLoss: 0.2609 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 428 | ANN: trainLoss: 0.2623 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 428 | ANN: trainLoss: 0.2646 | trainAcc: 88.7153% (511/576)\n",
            "9 13 Epoch: 428 | ANN: trainLoss: 0.2750 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 428 | ANN: trainLoss: 0.2746 | trainAcc: 88.2102% (621/704)\n",
            "11 13 Epoch: 428 | ANN: trainLoss: 0.2747 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 428 | ANN: trainLoss: 0.3506 | trainAcc: 87.9377% (678/771)\n",
            "0 4 Epoch: 428 | ANN: testLoss: 0.6112 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 428 | ANN: testLoss: 0.5289 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 428 | ANN: testLoss: 0.5351 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 428 | ANN: testLoss: 0.7072 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 429 | ANN: trainLoss: 0.2958 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 429 | ANN: trainLoss: 0.2448 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 429 | ANN: trainLoss: 0.2431 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 429 | ANN: trainLoss: 0.2915 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 429 | ANN: trainLoss: 0.2996 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 429 | ANN: trainLoss: 0.2913 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 429 | ANN: trainLoss: 0.2940 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 429 | ANN: trainLoss: 0.2971 | trainAcc: 86.9141% (445/512)\n",
            "8 13 Epoch: 429 | ANN: trainLoss: 0.2921 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 429 | ANN: trainLoss: 0.2869 | trainAcc: 87.6562% (561/640)\n",
            "10 13 Epoch: 429 | ANN: trainLoss: 0.2743 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 429 | ANN: trainLoss: 0.2767 | trainAcc: 88.1510% (677/768)\n",
            "12 13 Epoch: 429 | ANN: trainLoss: 0.2707 | trainAcc: 88.1971% (680/771)\n",
            "0 4 Epoch: 429 | ANN: testLoss: 0.4687 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 429 | ANN: testLoss: 0.5199 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 429 | ANN: testLoss: 0.5179 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 429 | ANN: testLoss: 0.6580 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 430 | ANN: trainLoss: 0.2008 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 430 | ANN: trainLoss: 0.2523 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 430 | ANN: trainLoss: 0.2879 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 430 | ANN: trainLoss: 0.2946 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 430 | ANN: trainLoss: 0.2833 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 430 | ANN: trainLoss: 0.2665 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 430 | ANN: trainLoss: 0.2576 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 430 | ANN: trainLoss: 0.2699 | trainAcc: 88.0859% (451/512)\n",
            "8 13 Epoch: 430 | ANN: trainLoss: 0.2653 | trainAcc: 88.1944% (508/576)\n",
            "9 13 Epoch: 430 | ANN: trainLoss: 0.2610 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 430 | ANN: trainLoss: 0.2628 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 430 | ANN: trainLoss: 0.2597 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 430 | ANN: trainLoss: 0.2552 | trainAcc: 88.8457% (685/771)\n",
            "0 4 Epoch: 430 | ANN: testLoss: 0.5420 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 430 | ANN: testLoss: 0.5074 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 430 | ANN: testLoss: 0.5279 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 430 | ANN: testLoss: 0.3960 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 431 | ANN: trainLoss: 0.2842 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 431 | ANN: trainLoss: 0.2574 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 431 | ANN: trainLoss: 0.2994 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 431 | ANN: trainLoss: 0.2916 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 431 | ANN: trainLoss: 0.2767 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 431 | ANN: trainLoss: 0.2744 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 431 | ANN: trainLoss: 0.2708 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 431 | ANN: trainLoss: 0.2710 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 431 | ANN: trainLoss: 0.2828 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 431 | ANN: trainLoss: 0.2801 | trainAcc: 87.1875% (558/640)\n",
            "10 13 Epoch: 431 | ANN: trainLoss: 0.2742 | trainAcc: 87.3580% (615/704)\n",
            "11 13 Epoch: 431 | ANN: trainLoss: 0.2817 | trainAcc: 86.8490% (667/768)\n",
            "12 13 Epoch: 431 | ANN: trainLoss: 0.3603 | trainAcc: 86.6407% (668/771)\n",
            "0 4 Epoch: 431 | ANN: testLoss: 0.4619 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 431 | ANN: testLoss: 0.5048 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 431 | ANN: testLoss: 0.5282 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 431 | ANN: testLoss: 0.3962 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 432 | ANN: trainLoss: 0.2977 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 432 | ANN: trainLoss: 0.2597 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 432 | ANN: trainLoss: 0.2418 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 432 | ANN: trainLoss: 0.2452 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 432 | ANN: trainLoss: 0.2745 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 432 | ANN: trainLoss: 0.2815 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 432 | ANN: trainLoss: 0.2843 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 432 | ANN: trainLoss: 0.2712 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 432 | ANN: trainLoss: 0.2641 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 432 | ANN: trainLoss: 0.2628 | trainAcc: 87.5000% (560/640)\n",
            "10 13 Epoch: 432 | ANN: trainLoss: 0.2599 | trainAcc: 87.7841% (618/704)\n",
            "11 13 Epoch: 432 | ANN: trainLoss: 0.2578 | trainAcc: 88.1510% (677/768)\n",
            "12 13 Epoch: 432 | ANN: trainLoss: 0.2414 | trainAcc: 88.1971% (680/771)\n",
            "0 4 Epoch: 432 | ANN: testLoss: 0.5012 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 432 | ANN: testLoss: 0.5845 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 432 | ANN: testLoss: 0.5293 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 432 | ANN: testLoss: 0.5717 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 433 | ANN: trainLoss: 0.2349 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 433 | ANN: trainLoss: 0.2474 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 433 | ANN: trainLoss: 0.2753 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 433 | ANN: trainLoss: 0.2675 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 433 | ANN: trainLoss: 0.2913 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 433 | ANN: trainLoss: 0.2816 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 433 | ANN: trainLoss: 0.2790 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 433 | ANN: trainLoss: 0.2706 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 433 | ANN: trainLoss: 0.2712 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 433 | ANN: trainLoss: 0.2722 | trainAcc: 88.4375% (566/640)\n",
            "10 13 Epoch: 433 | ANN: trainLoss: 0.2733 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 433 | ANN: trainLoss: 0.2686 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 433 | ANN: trainLoss: 0.2886 | trainAcc: 88.3268% (681/771)\n",
            "0 4 Epoch: 433 | ANN: testLoss: 0.5960 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 433 | ANN: testLoss: 0.5468 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 433 | ANN: testLoss: 0.5318 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 433 | ANN: testLoss: 0.4105 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 434 | ANN: trainLoss: 0.3123 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 434 | ANN: trainLoss: 0.3243 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 434 | ANN: trainLoss: 0.3077 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 434 | ANN: trainLoss: 0.3180 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 434 | ANN: trainLoss: 0.3192 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 434 | ANN: trainLoss: 0.3039 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 434 | ANN: trainLoss: 0.2993 | trainAcc: 87.2768% (391/448)\n",
            "7 13 Epoch: 434 | ANN: trainLoss: 0.2923 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 434 | ANN: trainLoss: 0.2949 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 434 | ANN: trainLoss: 0.2890 | trainAcc: 87.5000% (560/640)\n",
            "10 13 Epoch: 434 | ANN: trainLoss: 0.2928 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 434 | ANN: trainLoss: 0.3004 | trainAcc: 87.2396% (670/768)\n",
            "12 13 Epoch: 434 | ANN: trainLoss: 0.2848 | trainAcc: 87.2892% (673/771)\n",
            "0 4 Epoch: 434 | ANN: testLoss: 0.3990 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 434 | ANN: testLoss: 0.5415 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 434 | ANN: testLoss: 0.5444 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 434 | ANN: testLoss: 0.4083 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 435 | ANN: trainLoss: 0.3084 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 435 | ANN: trainLoss: 0.3521 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 435 | ANN: trainLoss: 0.3065 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 435 | ANN: trainLoss: 0.2851 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 435 | ANN: trainLoss: 0.2886 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 435 | ANN: trainLoss: 0.2803 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 435 | ANN: trainLoss: 0.2951 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 435 | ANN: trainLoss: 0.2961 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 435 | ANN: trainLoss: 0.3022 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 435 | ANN: trainLoss: 0.3017 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 435 | ANN: trainLoss: 0.3006 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 435 | ANN: trainLoss: 0.2961 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 435 | ANN: trainLoss: 0.2872 | trainAcc: 88.4565% (682/771)\n",
            "0 4 Epoch: 435 | ANN: testLoss: 0.4052 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 435 | ANN: testLoss: 0.5740 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 435 | ANN: testLoss: 0.5290 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 435 | ANN: testLoss: 0.4965 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 436 | ANN: trainLoss: 0.3285 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 436 | ANN: trainLoss: 0.2800 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 436 | ANN: trainLoss: 0.2771 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 436 | ANN: trainLoss: 0.2920 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 436 | ANN: trainLoss: 0.2917 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 436 | ANN: trainLoss: 0.2892 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 436 | ANN: trainLoss: 0.2901 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 436 | ANN: trainLoss: 0.2962 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 436 | ANN: trainLoss: 0.2927 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 436 | ANN: trainLoss: 0.2860 | trainAcc: 87.8125% (562/640)\n",
            "10 13 Epoch: 436 | ANN: trainLoss: 0.2801 | trainAcc: 88.4943% (623/704)\n",
            "11 13 Epoch: 436 | ANN: trainLoss: 0.2718 | trainAcc: 88.6719% (681/768)\n",
            "12 13 Epoch: 436 | ANN: trainLoss: 0.2772 | trainAcc: 88.5863% (683/771)\n",
            "0 4 Epoch: 436 | ANN: testLoss: 0.5046 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 436 | ANN: testLoss: 0.4838 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 436 | ANN: testLoss: 0.5300 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 436 | ANN: testLoss: 0.5098 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 437 | ANN: trainLoss: 0.1957 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 437 | ANN: trainLoss: 0.2373 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 437 | ANN: trainLoss: 0.3042 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 437 | ANN: trainLoss: 0.2644 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 437 | ANN: trainLoss: 0.2589 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 437 | ANN: trainLoss: 0.2640 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 437 | ANN: trainLoss: 0.2698 | trainAcc: 89.5089% (401/448)\n",
            "7 13 Epoch: 437 | ANN: trainLoss: 0.2656 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 437 | ANN: trainLoss: 0.2664 | trainAcc: 89.2361% (514/576)\n",
            "9 13 Epoch: 437 | ANN: trainLoss: 0.2708 | trainAcc: 88.9062% (569/640)\n",
            "10 13 Epoch: 437 | ANN: trainLoss: 0.2643 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 437 | ANN: trainLoss: 0.2620 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 437 | ANN: trainLoss: 0.2476 | trainAcc: 89.3645% (689/771)\n",
            "0 4 Epoch: 437 | ANN: testLoss: 0.4606 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 437 | ANN: testLoss: 0.4920 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 437 | ANN: testLoss: 0.5323 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 437 | ANN: testLoss: 0.3992 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 438 | ANN: trainLoss: 0.2482 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 438 | ANN: trainLoss: 0.2450 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 438 | ANN: trainLoss: 0.2513 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 438 | ANN: trainLoss: 0.2582 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 438 | ANN: trainLoss: 0.2824 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 438 | ANN: trainLoss: 0.2853 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 438 | ANN: trainLoss: 0.2842 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 438 | ANN: trainLoss: 0.2956 | trainAcc: 87.3047% (447/512)\n",
            "8 13 Epoch: 438 | ANN: trainLoss: 0.2881 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 438 | ANN: trainLoss: 0.2942 | trainAcc: 87.5000% (560/640)\n",
            "10 13 Epoch: 438 | ANN: trainLoss: 0.2918 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 438 | ANN: trainLoss: 0.2812 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 438 | ANN: trainLoss: 0.4821 | trainAcc: 87.6783% (676/771)\n",
            "0 4 Epoch: 438 | ANN: testLoss: 0.5558 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 438 | ANN: testLoss: 0.5608 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 438 | ANN: testLoss: 0.5313 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 438 | ANN: testLoss: 0.3985 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 439 | ANN: trainLoss: 0.2709 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 439 | ANN: trainLoss: 0.2563 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 439 | ANN: trainLoss: 0.3230 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 439 | ANN: trainLoss: 0.3096 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 439 | ANN: trainLoss: 0.2874 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 439 | ANN: trainLoss: 0.2785 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 439 | ANN: trainLoss: 0.2622 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 439 | ANN: trainLoss: 0.2647 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 439 | ANN: trainLoss: 0.2802 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 439 | ANN: trainLoss: 0.2770 | trainAcc: 89.3750% (572/640)\n",
            "10 13 Epoch: 439 | ANN: trainLoss: 0.2793 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 439 | ANN: trainLoss: 0.2736 | trainAcc: 89.5833% (688/768)\n",
            "12 13 Epoch: 439 | ANN: trainLoss: 0.3960 | trainAcc: 89.3645% (689/771)\n",
            "0 4 Epoch: 439 | ANN: testLoss: 0.5602 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 439 | ANN: testLoss: 0.5152 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 439 | ANN: testLoss: 0.5298 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 439 | ANN: testLoss: 0.3973 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 440 | ANN: trainLoss: 0.3097 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 440 | ANN: trainLoss: 0.3426 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 440 | ANN: trainLoss: 0.3658 | trainAcc: 80.2083% (154/192)\n",
            "3 13 Epoch: 440 | ANN: trainLoss: 0.3421 | trainAcc: 82.0312% (210/256)\n",
            "4 13 Epoch: 440 | ANN: trainLoss: 0.3266 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 440 | ANN: trainLoss: 0.3044 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 440 | ANN: trainLoss: 0.2996 | trainAcc: 86.1607% (386/448)\n",
            "7 13 Epoch: 440 | ANN: trainLoss: 0.2952 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 440 | ANN: trainLoss: 0.2986 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 440 | ANN: trainLoss: 0.2982 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 440 | ANN: trainLoss: 0.2951 | trainAcc: 86.3636% (608/704)\n",
            "11 13 Epoch: 440 | ANN: trainLoss: 0.2933 | trainAcc: 86.8490% (667/768)\n",
            "12 13 Epoch: 440 | ANN: trainLoss: 0.2977 | trainAcc: 86.7704% (669/771)\n",
            "0 4 Epoch: 440 | ANN: testLoss: 0.4927 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 440 | ANN: testLoss: 0.5030 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 440 | ANN: testLoss: 0.5266 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 440 | ANN: testLoss: 0.9588 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 441 | ANN: trainLoss: 0.2660 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 441 | ANN: trainLoss: 0.2385 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 441 | ANN: trainLoss: 0.2468 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 441 | ANN: trainLoss: 0.2402 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 441 | ANN: trainLoss: 0.2340 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 441 | ANN: trainLoss: 0.2491 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 441 | ANN: trainLoss: 0.2702 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 441 | ANN: trainLoss: 0.2616 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 441 | ANN: trainLoss: 0.2660 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 441 | ANN: trainLoss: 0.2623 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 441 | ANN: trainLoss: 0.2607 | trainAcc: 89.2045% (628/704)\n",
            "11 13 Epoch: 441 | ANN: trainLoss: 0.2604 | trainAcc: 89.4531% (687/768)\n",
            "12 13 Epoch: 441 | ANN: trainLoss: 0.4282 | trainAcc: 89.2348% (688/771)\n",
            "0 4 Epoch: 441 | ANN: testLoss: 0.4905 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 441 | ANN: testLoss: 0.4877 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 441 | ANN: testLoss: 0.5190 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 441 | ANN: testLoss: 0.4695 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 442 | ANN: trainLoss: 0.3191 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 442 | ANN: trainLoss: 0.3122 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 442 | ANN: trainLoss: 0.2887 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 442 | ANN: trainLoss: 0.2904 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 442 | ANN: trainLoss: 0.3007 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 442 | ANN: trainLoss: 0.2956 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 442 | ANN: trainLoss: 0.3068 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 442 | ANN: trainLoss: 0.3031 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 442 | ANN: trainLoss: 0.2977 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 442 | ANN: trainLoss: 0.2835 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 442 | ANN: trainLoss: 0.2860 | trainAcc: 86.7898% (611/704)\n",
            "11 13 Epoch: 442 | ANN: trainLoss: 0.2804 | trainAcc: 86.9792% (668/768)\n",
            "12 13 Epoch: 442 | ANN: trainLoss: 0.3824 | trainAcc: 86.7704% (669/771)\n",
            "0 4 Epoch: 442 | ANN: testLoss: 0.5578 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 442 | ANN: testLoss: 0.5586 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 442 | ANN: testLoss: 0.5394 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 442 | ANN: testLoss: 0.4089 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 443 | ANN: trainLoss: 0.2560 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 443 | ANN: trainLoss: 0.2644 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 443 | ANN: trainLoss: 0.2575 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 443 | ANN: trainLoss: 0.2849 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 443 | ANN: trainLoss: 0.2881 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 443 | ANN: trainLoss: 0.2811 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 443 | ANN: trainLoss: 0.2926 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 443 | ANN: trainLoss: 0.2853 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 443 | ANN: trainLoss: 0.2795 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 443 | ANN: trainLoss: 0.2852 | trainAcc: 88.4375% (566/640)\n",
            "10 13 Epoch: 443 | ANN: trainLoss: 0.2866 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 443 | ANN: trainLoss: 0.2806 | trainAcc: 88.5417% (680/768)\n",
            "12 13 Epoch: 443 | ANN: trainLoss: 0.2772 | trainAcc: 88.5863% (683/771)\n",
            "0 4 Epoch: 443 | ANN: testLoss: 0.4079 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 443 | ANN: testLoss: 0.4948 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 443 | ANN: testLoss: 0.5481 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 443 | ANN: testLoss: 0.5746 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 444 | ANN: trainLoss: 0.2235 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 444 | ANN: trainLoss: 0.2702 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 444 | ANN: trainLoss: 0.2537 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 444 | ANN: trainLoss: 0.2759 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 444 | ANN: trainLoss: 0.2571 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 444 | ANN: trainLoss: 0.2501 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 444 | ANN: trainLoss: 0.2625 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 444 | ANN: trainLoss: 0.2673 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 444 | ANN: trainLoss: 0.2622 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 444 | ANN: trainLoss: 0.2674 | trainAcc: 89.5312% (573/640)\n",
            "10 13 Epoch: 444 | ANN: trainLoss: 0.2741 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 444 | ANN: trainLoss: 0.2743 | trainAcc: 88.9323% (683/768)\n",
            "12 13 Epoch: 444 | ANN: trainLoss: 0.2909 | trainAcc: 88.8457% (685/771)\n",
            "0 4 Epoch: 444 | ANN: testLoss: 0.6005 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 444 | ANN: testLoss: 0.5248 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 444 | ANN: testLoss: 0.5374 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 444 | ANN: testLoss: 0.4030 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 445 | ANN: trainLoss: 0.3319 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 445 | ANN: trainLoss: 0.2890 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 445 | ANN: trainLoss: 0.3141 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 445 | ANN: trainLoss: 0.2939 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 445 | ANN: trainLoss: 0.3102 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 445 | ANN: trainLoss: 0.2885 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 445 | ANN: trainLoss: 0.2858 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 445 | ANN: trainLoss: 0.2810 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 445 | ANN: trainLoss: 0.2790 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 445 | ANN: trainLoss: 0.2810 | trainAcc: 87.1875% (558/640)\n",
            "10 13 Epoch: 445 | ANN: trainLoss: 0.2867 | trainAcc: 86.7898% (611/704)\n",
            "11 13 Epoch: 445 | ANN: trainLoss: 0.2919 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 445 | ANN: trainLoss: 0.2786 | trainAcc: 86.7704% (669/771)\n",
            "0 4 Epoch: 445 | ANN: testLoss: 0.5133 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 445 | ANN: testLoss: 0.5140 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 445 | ANN: testLoss: 0.5304 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 445 | ANN: testLoss: 0.4590 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 446 | ANN: trainLoss: 0.2415 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 446 | ANN: trainLoss: 0.2744 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 446 | ANN: trainLoss: 0.2501 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 446 | ANN: trainLoss: 0.2414 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 446 | ANN: trainLoss: 0.2262 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 446 | ANN: trainLoss: 0.2337 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 446 | ANN: trainLoss: 0.2427 | trainAcc: 90.8482% (407/448)\n",
            "7 13 Epoch: 446 | ANN: trainLoss: 0.2495 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 446 | ANN: trainLoss: 0.2505 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 446 | ANN: trainLoss: 0.2517 | trainAcc: 89.8438% (575/640)\n",
            "10 13 Epoch: 446 | ANN: trainLoss: 0.2502 | trainAcc: 90.3409% (636/704)\n",
            "11 13 Epoch: 446 | ANN: trainLoss: 0.2458 | trainAcc: 90.7552% (697/768)\n",
            "12 13 Epoch: 446 | ANN: trainLoss: 0.2304 | trainAcc: 90.7912% (700/771)\n",
            "0 4 Epoch: 446 | ANN: testLoss: 0.4588 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 446 | ANN: testLoss: 0.5246 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 446 | ANN: testLoss: 0.5475 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 446 | ANN: testLoss: 0.4106 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 447 | ANN: trainLoss: 0.2811 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 447 | ANN: trainLoss: 0.2663 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 447 | ANN: trainLoss: 0.3086 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 447 | ANN: trainLoss: 0.2830 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 447 | ANN: trainLoss: 0.2744 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 447 | ANN: trainLoss: 0.2713 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 447 | ANN: trainLoss: 0.2753 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 447 | ANN: trainLoss: 0.2695 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 447 | ANN: trainLoss: 0.2775 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 447 | ANN: trainLoss: 0.2794 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 447 | ANN: trainLoss: 0.2684 | trainAcc: 87.6420% (617/704)\n",
            "11 13 Epoch: 447 | ANN: trainLoss: 0.2704 | trainAcc: 87.5000% (672/768)\n",
            "12 13 Epoch: 447 | ANN: trainLoss: 0.3186 | trainAcc: 87.2892% (673/771)\n",
            "0 4 Epoch: 447 | ANN: testLoss: 0.5408 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 447 | ANN: testLoss: 0.5253 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 447 | ANN: testLoss: 0.5281 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 447 | ANN: testLoss: 0.3980 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 448 | ANN: trainLoss: 0.2530 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 448 | ANN: trainLoss: 0.2714 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 448 | ANN: trainLoss: 0.2925 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 448 | ANN: trainLoss: 0.2839 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 448 | ANN: trainLoss: 0.2658 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 448 | ANN: trainLoss: 0.2811 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 448 | ANN: trainLoss: 0.2729 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 448 | ANN: trainLoss: 0.2619 | trainAcc: 89.2578% (457/512)\n",
            "8 13 Epoch: 448 | ANN: trainLoss: 0.2805 | trainAcc: 88.7153% (511/576)\n",
            "9 13 Epoch: 448 | ANN: trainLoss: 0.2826 | trainAcc: 88.4375% (566/640)\n",
            "10 13 Epoch: 448 | ANN: trainLoss: 0.2774 | trainAcc: 88.7784% (625/704)\n",
            "11 13 Epoch: 448 | ANN: trainLoss: 0.2754 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 448 | ANN: trainLoss: 0.2795 | trainAcc: 88.7160% (684/771)\n",
            "0 4 Epoch: 448 | ANN: testLoss: 0.5188 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 448 | ANN: testLoss: 0.4983 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 448 | ANN: testLoss: 0.5279 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 448 | ANN: testLoss: 0.5057 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 449 | ANN: trainLoss: 0.2557 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 449 | ANN: trainLoss: 0.2937 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 449 | ANN: trainLoss: 0.2688 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 449 | ANN: trainLoss: 0.2924 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 449 | ANN: trainLoss: 0.2782 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 449 | ANN: trainLoss: 0.2786 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 449 | ANN: trainLoss: 0.2727 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 449 | ANN: trainLoss: 0.2720 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 449 | ANN: trainLoss: 0.2750 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 449 | ANN: trainLoss: 0.2826 | trainAcc: 87.6562% (561/640)\n",
            "10 13 Epoch: 449 | ANN: trainLoss: 0.2775 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 449 | ANN: trainLoss: 0.2858 | trainAcc: 87.7604% (674/768)\n",
            "12 13 Epoch: 449 | ANN: trainLoss: 0.2854 | trainAcc: 87.8080% (677/771)\n",
            "0 4 Epoch: 449 | ANN: testLoss: 0.5432 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 449 | ANN: testLoss: 0.5093 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 449 | ANN: testLoss: 0.5262 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 449 | ANN: testLoss: 0.3946 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 450 | ANN: trainLoss: 0.2559 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 450 | ANN: trainLoss: 0.2651 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 450 | ANN: trainLoss: 0.2865 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 450 | ANN: trainLoss: 0.2956 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 450 | ANN: trainLoss: 0.2784 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 450 | ANN: trainLoss: 0.2845 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 450 | ANN: trainLoss: 0.2903 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 450 | ANN: trainLoss: 0.2825 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 450 | ANN: trainLoss: 0.2712 | trainAcc: 88.3681% (509/576)\n",
            "9 13 Epoch: 450 | ANN: trainLoss: 0.2761 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 450 | ANN: trainLoss: 0.2814 | trainAcc: 87.6420% (617/704)\n",
            "11 13 Epoch: 450 | ANN: trainLoss: 0.2778 | trainAcc: 87.7604% (674/768)\n",
            "12 13 Epoch: 450 | ANN: trainLoss: 0.2782 | trainAcc: 87.6783% (676/771)\n",
            "0 4 Epoch: 450 | ANN: testLoss: 0.6349 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 450 | ANN: testLoss: 0.5657 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 450 | ANN: testLoss: 0.5264 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 450 | ANN: testLoss: 0.6109 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 451 | ANN: trainLoss: 0.2900 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 451 | ANN: trainLoss: 0.2436 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 451 | ANN: trainLoss: 0.2527 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 451 | ANN: trainLoss: 0.2828 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 451 | ANN: trainLoss: 0.2810 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 451 | ANN: trainLoss: 0.3070 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 451 | ANN: trainLoss: 0.3058 | trainAcc: 87.2768% (391/448)\n",
            "7 13 Epoch: 451 | ANN: trainLoss: 0.2997 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 451 | ANN: trainLoss: 0.2923 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 451 | ANN: trainLoss: 0.2876 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 451 | ANN: trainLoss: 0.2855 | trainAcc: 87.9261% (619/704)\n",
            "11 13 Epoch: 451 | ANN: trainLoss: 0.2844 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 451 | ANN: trainLoss: 0.2948 | trainAcc: 87.9377% (678/771)\n",
            "0 4 Epoch: 451 | ANN: testLoss: 0.5649 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 451 | ANN: testLoss: 0.5774 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 451 | ANN: testLoss: 0.5338 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 451 | ANN: testLoss: 0.5443 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 452 | ANN: trainLoss: 0.1883 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 452 | ANN: trainLoss: 0.2025 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 452 | ANN: trainLoss: 0.2270 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 452 | ANN: trainLoss: 0.2271 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 452 | ANN: trainLoss: 0.2417 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 452 | ANN: trainLoss: 0.2358 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 452 | ANN: trainLoss: 0.2421 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 452 | ANN: trainLoss: 0.2675 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 452 | ANN: trainLoss: 0.2622 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 452 | ANN: trainLoss: 0.2687 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 452 | ANN: trainLoss: 0.2603 | trainAcc: 88.9205% (626/704)\n",
            "11 13 Epoch: 452 | ANN: trainLoss: 0.2584 | trainAcc: 89.0625% (684/768)\n",
            "12 13 Epoch: 452 | ANN: trainLoss: 0.2832 | trainAcc: 88.9754% (686/771)\n",
            "0 4 Epoch: 452 | ANN: testLoss: 0.4504 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 452 | ANN: testLoss: 0.5149 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 452 | ANN: testLoss: 0.5319 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 452 | ANN: testLoss: 0.3989 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 453 | ANN: trainLoss: 0.3029 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 453 | ANN: trainLoss: 0.3398 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 453 | ANN: trainLoss: 0.3031 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 453 | ANN: trainLoss: 0.3257 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 453 | ANN: trainLoss: 0.3217 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 453 | ANN: trainLoss: 0.3254 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 453 | ANN: trainLoss: 0.3263 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 453 | ANN: trainLoss: 0.3232 | trainAcc: 84.9609% (435/512)\n",
            "8 13 Epoch: 453 | ANN: trainLoss: 0.3122 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 453 | ANN: trainLoss: 0.3063 | trainAcc: 85.4688% (547/640)\n",
            "10 13 Epoch: 453 | ANN: trainLoss: 0.3069 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 453 | ANN: trainLoss: 0.3071 | trainAcc: 85.8073% (659/768)\n",
            "12 13 Epoch: 453 | ANN: trainLoss: 0.3997 | trainAcc: 85.6031% (660/771)\n",
            "0 4 Epoch: 453 | ANN: testLoss: 0.4274 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 453 | ANN: testLoss: 0.5010 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 453 | ANN: testLoss: 0.5237 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 453 | ANN: testLoss: 0.3928 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 454 | ANN: trainLoss: 0.2782 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 454 | ANN: trainLoss: 0.2801 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 454 | ANN: trainLoss: 0.2460 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 454 | ANN: trainLoss: 0.2550 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 454 | ANN: trainLoss: 0.2581 | trainAcc: 90.0000% (288/320)\n",
            "5 13 Epoch: 454 | ANN: trainLoss: 0.2468 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 454 | ANN: trainLoss: 0.2376 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 454 | ANN: trainLoss: 0.2377 | trainAcc: 90.2344% (462/512)\n",
            "8 13 Epoch: 454 | ANN: trainLoss: 0.2485 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 454 | ANN: trainLoss: 0.2542 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 454 | ANN: trainLoss: 0.2528 | trainAcc: 89.4886% (630/704)\n",
            "11 13 Epoch: 454 | ANN: trainLoss: 0.2519 | trainAcc: 89.8438% (690/768)\n",
            "12 13 Epoch: 454 | ANN: trainLoss: 0.2568 | trainAcc: 89.7536% (692/771)\n",
            "0 4 Epoch: 454 | ANN: testLoss: 0.5697 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 454 | ANN: testLoss: 0.5275 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 454 | ANN: testLoss: 0.5380 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 454 | ANN: testLoss: 0.4035 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 455 | ANN: trainLoss: 0.3297 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 455 | ANN: trainLoss: 0.3263 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 455 | ANN: trainLoss: 0.2818 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 455 | ANN: trainLoss: 0.2706 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 455 | ANN: trainLoss: 0.2474 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 455 | ANN: trainLoss: 0.2635 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 455 | ANN: trainLoss: 0.2621 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 455 | ANN: trainLoss: 0.2492 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 455 | ANN: trainLoss: 0.2706 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 455 | ANN: trainLoss: 0.2800 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 455 | ANN: trainLoss: 0.2859 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 455 | ANN: trainLoss: 0.2827 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 455 | ANN: trainLoss: 0.3197 | trainAcc: 88.3268% (681/771)\n",
            "0 4 Epoch: 455 | ANN: testLoss: 0.4965 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 455 | ANN: testLoss: 0.5295 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 455 | ANN: testLoss: 0.5430 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 455 | ANN: testLoss: 0.4072 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 456 | ANN: trainLoss: 0.2503 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 456 | ANN: trainLoss: 0.3264 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 456 | ANN: trainLoss: 0.3135 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 456 | ANN: trainLoss: 0.2831 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 456 | ANN: trainLoss: 0.2804 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 456 | ANN: trainLoss: 0.2710 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 456 | ANN: trainLoss: 0.2625 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 456 | ANN: trainLoss: 0.2630 | trainAcc: 89.0625% (456/512)\n",
            "8 13 Epoch: 456 | ANN: trainLoss: 0.2724 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 456 | ANN: trainLoss: 0.2764 | trainAcc: 88.4375% (566/640)\n",
            "10 13 Epoch: 456 | ANN: trainLoss: 0.2793 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 456 | ANN: trainLoss: 0.2818 | trainAcc: 88.2812% (678/768)\n",
            "12 13 Epoch: 456 | ANN: trainLoss: 0.2644 | trainAcc: 88.3268% (681/771)\n",
            "0 4 Epoch: 456 | ANN: testLoss: 0.6228 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 456 | ANN: testLoss: 0.4904 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 456 | ANN: testLoss: 0.5302 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 456 | ANN: testLoss: 0.6750 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 457 | ANN: trainLoss: 0.2720 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 457 | ANN: trainLoss: 0.2889 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 457 | ANN: trainLoss: 0.2578 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 457 | ANN: trainLoss: 0.2467 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 457 | ANN: trainLoss: 0.2362 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 457 | ANN: trainLoss: 0.2365 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 457 | ANN: trainLoss: 0.2419 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 457 | ANN: trainLoss: 0.2399 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 457 | ANN: trainLoss: 0.2440 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 457 | ANN: trainLoss: 0.2421 | trainAcc: 88.7500% (568/640)\n",
            "10 13 Epoch: 457 | ANN: trainLoss: 0.2423 | trainAcc: 89.0625% (627/704)\n",
            "11 13 Epoch: 457 | ANN: trainLoss: 0.2404 | trainAcc: 89.1927% (685/768)\n",
            "12 13 Epoch: 457 | ANN: trainLoss: 0.3080 | trainAcc: 89.1051% (687/771)\n",
            "0 4 Epoch: 457 | ANN: testLoss: 0.4980 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 457 | ANN: testLoss: 0.5122 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 457 | ANN: testLoss: 0.5308 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 457 | ANN: testLoss: 0.5167 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 458 | ANN: trainLoss: 0.3066 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 458 | ANN: trainLoss: 0.2662 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 458 | ANN: trainLoss: 0.2761 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 458 | ANN: trainLoss: 0.2636 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 458 | ANN: trainLoss: 0.2539 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 458 | ANN: trainLoss: 0.2505 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 458 | ANN: trainLoss: 0.2446 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 458 | ANN: trainLoss: 0.2523 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 458 | ANN: trainLoss: 0.2520 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 458 | ANN: trainLoss: 0.2495 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 458 | ANN: trainLoss: 0.2456 | trainAcc: 88.6364% (624/704)\n",
            "11 13 Epoch: 458 | ANN: trainLoss: 0.2474 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 458 | ANN: trainLoss: 0.3388 | trainAcc: 88.1971% (680/771)\n",
            "0 4 Epoch: 458 | ANN: testLoss: 0.5409 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 458 | ANN: testLoss: 0.4983 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 458 | ANN: testLoss: 0.5443 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 458 | ANN: testLoss: 0.4082 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 459 | ANN: trainLoss: 0.3714 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 459 | ANN: trainLoss: 0.3425 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 459 | ANN: trainLoss: 0.3122 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 459 | ANN: trainLoss: 0.2868 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 459 | ANN: trainLoss: 0.2751 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 459 | ANN: trainLoss: 0.2536 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 459 | ANN: trainLoss: 0.2531 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 459 | ANN: trainLoss: 0.2587 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 459 | ANN: trainLoss: 0.2710 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 459 | ANN: trainLoss: 0.2772 | trainAcc: 87.8125% (562/640)\n",
            "10 13 Epoch: 459 | ANN: trainLoss: 0.2742 | trainAcc: 87.9261% (619/704)\n",
            "11 13 Epoch: 459 | ANN: trainLoss: 0.2783 | trainAcc: 87.6302% (673/768)\n",
            "12 13 Epoch: 459 | ANN: trainLoss: 0.3455 | trainAcc: 87.5486% (675/771)\n",
            "0 4 Epoch: 459 | ANN: testLoss: 0.4127 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 459 | ANN: testLoss: 0.4845 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 459 | ANN: testLoss: 0.5465 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 459 | ANN: testLoss: 0.4103 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 460 | ANN: trainLoss: 0.2358 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 460 | ANN: trainLoss: 0.2353 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 460 | ANN: trainLoss: 0.2333 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 460 | ANN: trainLoss: 0.2461 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 460 | ANN: trainLoss: 0.2567 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 460 | ANN: trainLoss: 0.2683 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 460 | ANN: trainLoss: 0.2709 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 460 | ANN: trainLoss: 0.2828 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 460 | ANN: trainLoss: 0.2720 | trainAcc: 88.1944% (508/576)\n",
            "9 13 Epoch: 460 | ANN: trainLoss: 0.2808 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 460 | ANN: trainLoss: 0.2780 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 460 | ANN: trainLoss: 0.2745 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 460 | ANN: trainLoss: 0.3091 | trainAcc: 88.5863% (683/771)\n",
            "0 4 Epoch: 460 | ANN: testLoss: 0.5650 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 460 | ANN: testLoss: 0.5155 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 460 | ANN: testLoss: 0.5364 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 460 | ANN: testLoss: 0.7166 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 461 | ANN: trainLoss: 0.3257 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 461 | ANN: trainLoss: 0.2741 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 461 | ANN: trainLoss: 0.2421 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 461 | ANN: trainLoss: 0.2558 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 461 | ANN: trainLoss: 0.2482 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 461 | ANN: trainLoss: 0.2285 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 461 | ANN: trainLoss: 0.2489 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 461 | ANN: trainLoss: 0.2429 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 461 | ANN: trainLoss: 0.2579 | trainAcc: 89.5833% (516/576)\n",
            "9 13 Epoch: 461 | ANN: trainLoss: 0.2643 | trainAcc: 89.5312% (573/640)\n",
            "10 13 Epoch: 461 | ANN: trainLoss: 0.2649 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 461 | ANN: trainLoss: 0.2639 | trainAcc: 89.4531% (687/768)\n",
            "12 13 Epoch: 461 | ANN: trainLoss: 0.2627 | trainAcc: 89.4942% (690/771)\n",
            "0 4 Epoch: 461 | ANN: testLoss: 0.6496 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 461 | ANN: testLoss: 0.5765 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 461 | ANN: testLoss: 0.5355 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 461 | ANN: testLoss: 0.4930 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 462 | ANN: trainLoss: 0.3026 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 462 | ANN: trainLoss: 0.2972 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 462 | ANN: trainLoss: 0.3133 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 462 | ANN: trainLoss: 0.2961 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 462 | ANN: trainLoss: 0.2900 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 462 | ANN: trainLoss: 0.2964 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 462 | ANN: trainLoss: 0.2868 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 462 | ANN: trainLoss: 0.3025 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 462 | ANN: trainLoss: 0.3049 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 462 | ANN: trainLoss: 0.2960 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 462 | ANN: trainLoss: 0.2934 | trainAcc: 86.5057% (609/704)\n",
            "11 13 Epoch: 462 | ANN: trainLoss: 0.2946 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 462 | ANN: trainLoss: 0.4230 | trainAcc: 86.5110% (667/771)\n",
            "0 4 Epoch: 462 | ANN: testLoss: 0.5263 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 462 | ANN: testLoss: 0.5410 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 462 | ANN: testLoss: 0.5296 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 462 | ANN: testLoss: 0.5617 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 463 | ANN: trainLoss: 0.2727 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 463 | ANN: trainLoss: 0.2361 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 463 | ANN: trainLoss: 0.2486 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 463 | ANN: trainLoss: 0.2423 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 463 | ANN: trainLoss: 0.2539 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 463 | ANN: trainLoss: 0.2587 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 463 | ANN: trainLoss: 0.2715 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 463 | ANN: trainLoss: 0.2655 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 463 | ANN: trainLoss: 0.2527 | trainAcc: 88.7153% (511/576)\n",
            "9 13 Epoch: 463 | ANN: trainLoss: 0.2518 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 463 | ANN: trainLoss: 0.2571 | trainAcc: 88.7784% (625/704)\n",
            "11 13 Epoch: 463 | ANN: trainLoss: 0.2655 | trainAcc: 88.1510% (677/768)\n",
            "12 13 Epoch: 463 | ANN: trainLoss: 0.2740 | trainAcc: 88.0674% (679/771)\n",
            "0 4 Epoch: 463 | ANN: testLoss: 0.4827 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 463 | ANN: testLoss: 0.5798 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 463 | ANN: testLoss: 0.5391 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 463 | ANN: testLoss: 0.7387 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 464 | ANN: trainLoss: 0.2719 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 464 | ANN: trainLoss: 0.3005 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 464 | ANN: trainLoss: 0.2791 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 464 | ANN: trainLoss: 0.2624 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 464 | ANN: trainLoss: 0.2996 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 464 | ANN: trainLoss: 0.2880 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 464 | ANN: trainLoss: 0.2888 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 464 | ANN: trainLoss: 0.2813 | trainAcc: 89.4531% (458/512)\n",
            "8 13 Epoch: 464 | ANN: trainLoss: 0.2759 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 464 | ANN: trainLoss: 0.2866 | trainAcc: 88.9062% (569/640)\n",
            "10 13 Epoch: 464 | ANN: trainLoss: 0.2827 | trainAcc: 89.2045% (628/704)\n",
            "11 13 Epoch: 464 | ANN: trainLoss: 0.2760 | trainAcc: 89.4531% (687/768)\n",
            "12 13 Epoch: 464 | ANN: trainLoss: 0.2733 | trainAcc: 89.4942% (690/771)\n",
            "0 4 Epoch: 464 | ANN: testLoss: 0.5275 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 464 | ANN: testLoss: 0.5550 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 464 | ANN: testLoss: 0.5309 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 464 | ANN: testLoss: 0.5867 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 465 | ANN: trainLoss: 0.2898 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 465 | ANN: trainLoss: 0.2409 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 465 | ANN: trainLoss: 0.2454 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 465 | ANN: trainLoss: 0.2493 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 465 | ANN: trainLoss: 0.2554 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 465 | ANN: trainLoss: 0.2466 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 465 | ANN: trainLoss: 0.2569 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 465 | ANN: trainLoss: 0.2684 | trainAcc: 89.0625% (456/512)\n",
            "8 13 Epoch: 465 | ANN: trainLoss: 0.2733 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 465 | ANN: trainLoss: 0.2714 | trainAcc: 88.7500% (568/640)\n",
            "10 13 Epoch: 465 | ANN: trainLoss: 0.2700 | trainAcc: 88.7784% (625/704)\n",
            "11 13 Epoch: 465 | ANN: trainLoss: 0.2745 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 465 | ANN: trainLoss: 0.3044 | trainAcc: 87.9377% (678/771)\n",
            "0 4 Epoch: 465 | ANN: testLoss: 0.6585 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 465 | ANN: testLoss: 0.5797 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 465 | ANN: testLoss: 0.5490 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 465 | ANN: testLoss: 0.4118 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 466 | ANN: trainLoss: 0.2927 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 466 | ANN: trainLoss: 0.2704 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 466 | ANN: trainLoss: 0.2820 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 466 | ANN: trainLoss: 0.2724 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 466 | ANN: trainLoss: 0.2958 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 466 | ANN: trainLoss: 0.2906 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 466 | ANN: trainLoss: 0.2983 | trainAcc: 86.3839% (387/448)\n",
            "7 13 Epoch: 466 | ANN: trainLoss: 0.2967 | trainAcc: 86.9141% (445/512)\n",
            "8 13 Epoch: 466 | ANN: trainLoss: 0.2946 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 466 | ANN: trainLoss: 0.2916 | trainAcc: 87.6562% (561/640)\n",
            "10 13 Epoch: 466 | ANN: trainLoss: 0.2969 | trainAcc: 87.3580% (615/704)\n",
            "11 13 Epoch: 466 | ANN: trainLoss: 0.2899 | trainAcc: 87.7604% (674/768)\n",
            "12 13 Epoch: 466 | ANN: trainLoss: 0.3007 | trainAcc: 87.6783% (676/771)\n",
            "0 4 Epoch: 466 | ANN: testLoss: 0.4866 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 466 | ANN: testLoss: 0.5406 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 466 | ANN: testLoss: 0.5366 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 466 | ANN: testLoss: 0.4025 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 467 | ANN: trainLoss: 0.3331 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 467 | ANN: trainLoss: 0.3426 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 467 | ANN: trainLoss: 0.3007 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 467 | ANN: trainLoss: 0.2779 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 467 | ANN: trainLoss: 0.2862 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 467 | ANN: trainLoss: 0.2831 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 467 | ANN: trainLoss: 0.2786 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 467 | ANN: trainLoss: 0.2749 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 467 | ANN: trainLoss: 0.2725 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 467 | ANN: trainLoss: 0.2638 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 467 | ANN: trainLoss: 0.2645 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 467 | ANN: trainLoss: 0.2685 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 467 | ANN: trainLoss: 0.2502 | trainAcc: 89.3645% (689/771)\n",
            "0 4 Epoch: 467 | ANN: testLoss: 0.6040 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 467 | ANN: testLoss: 0.5828 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 467 | ANN: testLoss: 0.5319 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 467 | ANN: testLoss: 0.7263 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 468 | ANN: trainLoss: 0.2185 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 468 | ANN: trainLoss: 0.2833 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 468 | ANN: trainLoss: 0.2947 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 468 | ANN: trainLoss: 0.2737 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 468 | ANN: trainLoss: 0.2660 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 468 | ANN: trainLoss: 0.2517 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 468 | ANN: trainLoss: 0.2513 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 468 | ANN: trainLoss: 0.2562 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 468 | ANN: trainLoss: 0.2719 | trainAcc: 86.9792% (501/576)\n",
            "9 13 Epoch: 468 | ANN: trainLoss: 0.2677 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 468 | ANN: trainLoss: 0.2809 | trainAcc: 87.3580% (615/704)\n",
            "11 13 Epoch: 468 | ANN: trainLoss: 0.2737 | trainAcc: 87.8906% (675/768)\n",
            "12 13 Epoch: 468 | ANN: trainLoss: 0.3526 | trainAcc: 87.8080% (677/771)\n",
            "0 4 Epoch: 468 | ANN: testLoss: 0.5099 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 468 | ANN: testLoss: 0.5855 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 468 | ANN: testLoss: 0.5447 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 468 | ANN: testLoss: 0.5499 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 469 | ANN: trainLoss: 0.4310 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 469 | ANN: trainLoss: 0.3483 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 469 | ANN: trainLoss: 0.3219 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 469 | ANN: trainLoss: 0.3017 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 469 | ANN: trainLoss: 0.2967 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 469 | ANN: trainLoss: 0.3170 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 469 | ANN: trainLoss: 0.3207 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 469 | ANN: trainLoss: 0.3260 | trainAcc: 85.1562% (436/512)\n",
            "8 13 Epoch: 469 | ANN: trainLoss: 0.3226 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 469 | ANN: trainLoss: 0.3161 | trainAcc: 85.6250% (548/640)\n",
            "10 13 Epoch: 469 | ANN: trainLoss: 0.3203 | trainAcc: 85.2273% (600/704)\n",
            "11 13 Epoch: 469 | ANN: trainLoss: 0.3156 | trainAcc: 85.5469% (657/768)\n",
            "12 13 Epoch: 469 | ANN: trainLoss: 0.5688 | trainAcc: 85.3437% (658/771)\n",
            "0 4 Epoch: 469 | ANN: testLoss: 0.4479 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 469 | ANN: testLoss: 0.5165 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 469 | ANN: testLoss: 0.5427 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 469 | ANN: testLoss: 0.5888 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 470 | ANN: trainLoss: 0.3201 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 470 | ANN: trainLoss: 0.2895 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 470 | ANN: trainLoss: 0.2749 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 470 | ANN: trainLoss: 0.2861 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 470 | ANN: trainLoss: 0.2717 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 470 | ANN: trainLoss: 0.2642 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 470 | ANN: trainLoss: 0.2648 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 470 | ANN: trainLoss: 0.2724 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 470 | ANN: trainLoss: 0.2801 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 470 | ANN: trainLoss: 0.2793 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 470 | ANN: trainLoss: 0.2716 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 470 | ANN: trainLoss: 0.2736 | trainAcc: 88.2812% (678/768)\n",
            "12 13 Epoch: 470 | ANN: trainLoss: 0.2875 | trainAcc: 88.1971% (680/771)\n",
            "0 4 Epoch: 470 | ANN: testLoss: 0.6199 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 470 | ANN: testLoss: 0.5743 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 470 | ANN: testLoss: 0.5301 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 470 | ANN: testLoss: 1.2177 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 471 | ANN: trainLoss: 0.2660 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 471 | ANN: trainLoss: 0.3215 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 471 | ANN: trainLoss: 0.2782 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 471 | ANN: trainLoss: 0.2866 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 471 | ANN: trainLoss: 0.2696 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 471 | ANN: trainLoss: 0.3068 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 471 | ANN: trainLoss: 0.2939 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 471 | ANN: trainLoss: 0.2964 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 471 | ANN: trainLoss: 0.2850 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 471 | ANN: trainLoss: 0.2777 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 471 | ANN: trainLoss: 0.2737 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 471 | ANN: trainLoss: 0.2713 | trainAcc: 88.5417% (680/768)\n",
            "12 13 Epoch: 471 | ANN: trainLoss: 0.3834 | trainAcc: 88.4565% (682/771)\n",
            "0 4 Epoch: 471 | ANN: testLoss: 0.7136 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 471 | ANN: testLoss: 0.6178 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 471 | ANN: testLoss: 0.5332 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 471 | ANN: testLoss: 0.4000 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 472 | ANN: trainLoss: 0.2533 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 472 | ANN: trainLoss: 0.2737 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 472 | ANN: trainLoss: 0.2666 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 472 | ANN: trainLoss: 0.3153 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 472 | ANN: trainLoss: 0.3168 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 472 | ANN: trainLoss: 0.2979 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 472 | ANN: trainLoss: 0.2915 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 472 | ANN: trainLoss: 0.2938 | trainAcc: 86.7188% (444/512)\n",
            "8 13 Epoch: 472 | ANN: trainLoss: 0.2899 | trainAcc: 86.6319% (499/576)\n",
            "9 13 Epoch: 472 | ANN: trainLoss: 0.2914 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 472 | ANN: trainLoss: 0.2849 | trainAcc: 86.9318% (612/704)\n",
            "11 13 Epoch: 472 | ANN: trainLoss: 0.2868 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 472 | ANN: trainLoss: 0.2801 | trainAcc: 86.7704% (669/771)\n",
            "0 4 Epoch: 472 | ANN: testLoss: 0.6202 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 472 | ANN: testLoss: 0.5532 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 472 | ANN: testLoss: 0.5320 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 472 | ANN: testLoss: 0.3990 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 473 | ANN: trainLoss: 0.2801 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 473 | ANN: trainLoss: 0.2915 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 473 | ANN: trainLoss: 0.2958 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 473 | ANN: trainLoss: 0.3005 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 473 | ANN: trainLoss: 0.3025 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 473 | ANN: trainLoss: 0.3066 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 473 | ANN: trainLoss: 0.3093 | trainAcc: 86.3839% (387/448)\n",
            "7 13 Epoch: 473 | ANN: trainLoss: 0.3028 | trainAcc: 86.9141% (445/512)\n",
            "8 13 Epoch: 473 | ANN: trainLoss: 0.2893 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 473 | ANN: trainLoss: 0.2837 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 473 | ANN: trainLoss: 0.2835 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 473 | ANN: trainLoss: 0.2831 | trainAcc: 87.8906% (675/768)\n",
            "12 13 Epoch: 473 | ANN: trainLoss: 0.2749 | trainAcc: 87.9377% (678/771)\n",
            "0 4 Epoch: 473 | ANN: testLoss: 0.5175 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 473 | ANN: testLoss: 0.5514 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 473 | ANN: testLoss: 0.5395 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 473 | ANN: testLoss: 0.5491 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 474 | ANN: trainLoss: 0.1782 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 474 | ANN: trainLoss: 0.2415 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 474 | ANN: trainLoss: 0.2530 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 474 | ANN: trainLoss: 0.2591 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 474 | ANN: trainLoss: 0.2648 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 474 | ANN: trainLoss: 0.2852 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 474 | ANN: trainLoss: 0.2728 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 474 | ANN: trainLoss: 0.2630 | trainAcc: 89.0625% (456/512)\n",
            "8 13 Epoch: 474 | ANN: trainLoss: 0.2679 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 474 | ANN: trainLoss: 0.2593 | trainAcc: 89.3750% (572/640)\n",
            "10 13 Epoch: 474 | ANN: trainLoss: 0.2588 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 474 | ANN: trainLoss: 0.2586 | trainAcc: 89.5833% (688/768)\n",
            "12 13 Epoch: 474 | ANN: trainLoss: 0.2510 | trainAcc: 89.6239% (691/771)\n",
            "0 4 Epoch: 474 | ANN: testLoss: 0.5141 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 474 | ANN: testLoss: 0.5946 | testAcc: 62.5000% (80/128)\n",
            "2 4 Epoch: 474 | ANN: testLoss: 0.5362 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 474 | ANN: testLoss: 0.4023 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 475 | ANN: trainLoss: 0.2764 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 475 | ANN: trainLoss: 0.2891 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 475 | ANN: trainLoss: 0.2797 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 475 | ANN: trainLoss: 0.2869 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 475 | ANN: trainLoss: 0.2902 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 475 | ANN: trainLoss: 0.2827 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 475 | ANN: trainLoss: 0.2842 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 475 | ANN: trainLoss: 0.2752 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 475 | ANN: trainLoss: 0.2718 | trainAcc: 86.9792% (501/576)\n",
            "9 13 Epoch: 475 | ANN: trainLoss: 0.2726 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 475 | ANN: trainLoss: 0.2633 | trainAcc: 87.7841% (618/704)\n",
            "11 13 Epoch: 475 | ANN: trainLoss: 0.2659 | trainAcc: 87.5000% (672/768)\n",
            "12 13 Epoch: 475 | ANN: trainLoss: 0.3047 | trainAcc: 87.4189% (674/771)\n",
            "0 4 Epoch: 475 | ANN: testLoss: 0.6505 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 475 | ANN: testLoss: 0.5917 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 475 | ANN: testLoss: 0.5361 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 475 | ANN: testLoss: 0.4823 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 476 | ANN: trainLoss: 0.2344 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 476 | ANN: trainLoss: 0.2731 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 476 | ANN: trainLoss: 0.2616 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 476 | ANN: trainLoss: 0.2856 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 476 | ANN: trainLoss: 0.3109 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 476 | ANN: trainLoss: 0.3115 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 476 | ANN: trainLoss: 0.3185 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 476 | ANN: trainLoss: 0.3059 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 476 | ANN: trainLoss: 0.3026 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 476 | ANN: trainLoss: 0.3092 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 476 | ANN: trainLoss: 0.2995 | trainAcc: 86.5057% (609/704)\n",
            "11 13 Epoch: 476 | ANN: trainLoss: 0.2915 | trainAcc: 87.1094% (669/768)\n",
            "12 13 Epoch: 476 | ANN: trainLoss: 0.3093 | trainAcc: 87.0298% (671/771)\n",
            "0 4 Epoch: 476 | ANN: testLoss: 0.6014 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 476 | ANN: testLoss: 0.5437 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 476 | ANN: testLoss: 0.5314 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 476 | ANN: testLoss: 0.3986 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 477 | ANN: trainLoss: 0.2106 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 477 | ANN: trainLoss: 0.2381 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 477 | ANN: trainLoss: 0.2749 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 477 | ANN: trainLoss: 0.2703 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 477 | ANN: trainLoss: 0.2789 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 477 | ANN: trainLoss: 0.2663 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 477 | ANN: trainLoss: 0.2633 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 477 | ANN: trainLoss: 0.2598 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 477 | ANN: trainLoss: 0.2558 | trainAcc: 89.5833% (516/576)\n",
            "9 13 Epoch: 477 | ANN: trainLoss: 0.2669 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 477 | ANN: trainLoss: 0.2664 | trainAcc: 88.6364% (624/704)\n",
            "11 13 Epoch: 477 | ANN: trainLoss: 0.2605 | trainAcc: 89.0625% (684/768)\n",
            "12 13 Epoch: 477 | ANN: trainLoss: 0.2455 | trainAcc: 89.1051% (687/771)\n",
            "0 4 Epoch: 477 | ANN: testLoss: 0.5542 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 477 | ANN: testLoss: 0.5313 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 477 | ANN: testLoss: 0.5384 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 477 | ANN: testLoss: 0.4038 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 478 | ANN: trainLoss: 0.2797 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 478 | ANN: trainLoss: 0.2526 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 478 | ANN: trainLoss: 0.2583 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 478 | ANN: trainLoss: 0.2596 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 478 | ANN: trainLoss: 0.2699 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 478 | ANN: trainLoss: 0.2864 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 478 | ANN: trainLoss: 0.2876 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 478 | ANN: trainLoss: 0.2838 | trainAcc: 87.3047% (447/512)\n",
            "8 13 Epoch: 478 | ANN: trainLoss: 0.2844 | trainAcc: 87.5000% (504/576)\n",
            "9 13 Epoch: 478 | ANN: trainLoss: 0.2833 | trainAcc: 87.6562% (561/640)\n",
            "10 13 Epoch: 478 | ANN: trainLoss: 0.2800 | trainAcc: 87.9261% (619/704)\n",
            "11 13 Epoch: 478 | ANN: trainLoss: 0.2743 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 478 | ANN: trainLoss: 0.2584 | trainAcc: 88.4565% (682/771)\n",
            "0 4 Epoch: 478 | ANN: testLoss: 0.4094 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 478 | ANN: testLoss: 0.5390 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 478 | ANN: testLoss: 0.5405 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 478 | ANN: testLoss: 0.6767 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 479 | ANN: trainLoss: 0.2409 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 479 | ANN: trainLoss: 0.2707 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 479 | ANN: trainLoss: 0.2533 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 479 | ANN: trainLoss: 0.2429 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 479 | ANN: trainLoss: 0.2627 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 479 | ANN: trainLoss: 0.2604 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 479 | ANN: trainLoss: 0.2549 | trainAcc: 90.4018% (405/448)\n",
            "7 13 Epoch: 479 | ANN: trainLoss: 0.2592 | trainAcc: 90.0391% (461/512)\n",
            "8 13 Epoch: 479 | ANN: trainLoss: 0.2629 | trainAcc: 89.2361% (514/576)\n",
            "9 13 Epoch: 479 | ANN: trainLoss: 0.2665 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 479 | ANN: trainLoss: 0.2641 | trainAcc: 88.9205% (626/704)\n",
            "11 13 Epoch: 479 | ANN: trainLoss: 0.2646 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 479 | ANN: trainLoss: 0.2459 | trainAcc: 88.8457% (685/771)\n",
            "0 4 Epoch: 479 | ANN: testLoss: 0.5312 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 479 | ANN: testLoss: 0.5339 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 479 | ANN: testLoss: 0.5382 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 479 | ANN: testLoss: 0.4072 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 480 | ANN: trainLoss: 0.2486 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 480 | ANN: trainLoss: 0.2617 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 480 | ANN: trainLoss: 0.2706 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 480 | ANN: trainLoss: 0.2684 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 480 | ANN: trainLoss: 0.2660 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 480 | ANN: trainLoss: 0.2762 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 480 | ANN: trainLoss: 0.2765 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 480 | ANN: trainLoss: 0.2785 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 480 | ANN: trainLoss: 0.2769 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 480 | ANN: trainLoss: 0.2673 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 480 | ANN: trainLoss: 0.2624 | trainAcc: 89.0625% (627/704)\n",
            "11 13 Epoch: 480 | ANN: trainLoss: 0.2645 | trainAcc: 89.0625% (684/768)\n",
            "12 13 Epoch: 480 | ANN: trainLoss: 0.2591 | trainAcc: 89.1051% (687/771)\n",
            "0 4 Epoch: 480 | ANN: testLoss: 0.5541 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 480 | ANN: testLoss: 0.5544 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 480 | ANN: testLoss: 0.5414 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 480 | ANN: testLoss: 0.6863 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 481 | ANN: trainLoss: 0.1850 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 481 | ANN: trainLoss: 0.2466 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 481 | ANN: trainLoss: 0.2528 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 481 | ANN: trainLoss: 0.2444 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 481 | ANN: trainLoss: 0.2438 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 481 | ANN: trainLoss: 0.2428 | trainAcc: 90.6250% (348/384)\n",
            "6 13 Epoch: 481 | ANN: trainLoss: 0.2508 | trainAcc: 90.1786% (404/448)\n",
            "7 13 Epoch: 481 | ANN: trainLoss: 0.2663 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 481 | ANN: trainLoss: 0.2704 | trainAcc: 89.2361% (514/576)\n",
            "9 13 Epoch: 481 | ANN: trainLoss: 0.2665 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 481 | ANN: trainLoss: 0.2735 | trainAcc: 88.7784% (625/704)\n",
            "11 13 Epoch: 481 | ANN: trainLoss: 0.2705 | trainAcc: 88.9323% (683/768)\n",
            "12 13 Epoch: 481 | ANN: trainLoss: 0.2654 | trainAcc: 88.9754% (686/771)\n",
            "0 4 Epoch: 481 | ANN: testLoss: 0.4755 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 481 | ANN: testLoss: 0.5795 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 481 | ANN: testLoss: 0.5412 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 481 | ANN: testLoss: 0.6518 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 482 | ANN: trainLoss: 0.2640 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 482 | ANN: trainLoss: 0.2860 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 482 | ANN: trainLoss: 0.2742 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 482 | ANN: trainLoss: 0.2715 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 482 | ANN: trainLoss: 0.2612 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 482 | ANN: trainLoss: 0.2853 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 482 | ANN: trainLoss: 0.2899 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 482 | ANN: trainLoss: 0.2803 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 482 | ANN: trainLoss: 0.2838 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 482 | ANN: trainLoss: 0.2847 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 482 | ANN: trainLoss: 0.2784 | trainAcc: 88.2102% (621/704)\n",
            "11 13 Epoch: 482 | ANN: trainLoss: 0.2826 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 482 | ANN: trainLoss: 0.3451 | trainAcc: 87.8080% (677/771)\n",
            "0 4 Epoch: 482 | ANN: testLoss: 0.5666 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 482 | ANN: testLoss: 0.6002 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 482 | ANN: testLoss: 0.5274 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 482 | ANN: testLoss: 0.6782 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 483 | ANN: trainLoss: 0.1968 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 483 | ANN: trainLoss: 0.1691 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 483 | ANN: trainLoss: 0.2092 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 483 | ANN: trainLoss: 0.2444 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 483 | ANN: trainLoss: 0.2491 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 483 | ANN: trainLoss: 0.2537 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 483 | ANN: trainLoss: 0.2545 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 483 | ANN: trainLoss: 0.2626 | trainAcc: 88.6719% (454/512)\n",
            "8 13 Epoch: 483 | ANN: trainLoss: 0.2672 | trainAcc: 88.7153% (511/576)\n",
            "9 13 Epoch: 483 | ANN: trainLoss: 0.2642 | trainAcc: 88.9062% (569/640)\n",
            "10 13 Epoch: 483 | ANN: trainLoss: 0.2660 | trainAcc: 88.7784% (625/704)\n",
            "11 13 Epoch: 483 | ANN: trainLoss: 0.2712 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 483 | ANN: trainLoss: 0.3141 | trainAcc: 88.5863% (683/771)\n",
            "0 4 Epoch: 483 | ANN: testLoss: 0.4857 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 483 | ANN: testLoss: 0.4778 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 483 | ANN: testLoss: 0.5318 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 483 | ANN: testLoss: 0.3998 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 484 | ANN: trainLoss: 0.3075 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 484 | ANN: trainLoss: 0.2992 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 484 | ANN: trainLoss: 0.2944 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 484 | ANN: trainLoss: 0.3166 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 484 | ANN: trainLoss: 0.3131 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 484 | ANN: trainLoss: 0.3117 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 484 | ANN: trainLoss: 0.3104 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 484 | ANN: trainLoss: 0.3021 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 484 | ANN: trainLoss: 0.3061 | trainAcc: 85.5903% (493/576)\n",
            "9 13 Epoch: 484 | ANN: trainLoss: 0.2986 | trainAcc: 85.7812% (549/640)\n",
            "10 13 Epoch: 484 | ANN: trainLoss: 0.2906 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 484 | ANN: trainLoss: 0.2881 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 484 | ANN: trainLoss: 0.3327 | trainAcc: 86.3813% (666/771)\n",
            "0 4 Epoch: 484 | ANN: testLoss: 0.3955 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 484 | ANN: testLoss: 0.5075 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 484 | ANN: testLoss: 0.5354 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 484 | ANN: testLoss: 0.4015 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 485 | ANN: trainLoss: 0.1956 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 485 | ANN: trainLoss: 0.2374 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 485 | ANN: trainLoss: 0.2669 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 485 | ANN: trainLoss: 0.2540 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 485 | ANN: trainLoss: 0.2737 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 485 | ANN: trainLoss: 0.2683 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 485 | ANN: trainLoss: 0.2708 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 485 | ANN: trainLoss: 0.2616 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 485 | ANN: trainLoss: 0.2611 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 485 | ANN: trainLoss: 0.2582 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 485 | ANN: trainLoss: 0.2564 | trainAcc: 88.4943% (623/704)\n",
            "11 13 Epoch: 485 | ANN: trainLoss: 0.2654 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 485 | ANN: trainLoss: 0.2814 | trainAcc: 87.9377% (678/771)\n",
            "0 4 Epoch: 485 | ANN: testLoss: 0.6021 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 485 | ANN: testLoss: 0.6039 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 485 | ANN: testLoss: 0.5596 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 485 | ANN: testLoss: 0.4198 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 486 | ANN: trainLoss: 0.1873 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 486 | ANN: trainLoss: 0.1831 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 486 | ANN: trainLoss: 0.2046 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 486 | ANN: trainLoss: 0.2214 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 486 | ANN: trainLoss: 0.2486 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 486 | ANN: trainLoss: 0.2516 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 486 | ANN: trainLoss: 0.2475 | trainAcc: 90.4018% (405/448)\n",
            "7 13 Epoch: 486 | ANN: trainLoss: 0.2531 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 486 | ANN: trainLoss: 0.2519 | trainAcc: 89.5833% (516/576)\n",
            "9 13 Epoch: 486 | ANN: trainLoss: 0.2460 | trainAcc: 90.1562% (577/640)\n",
            "10 13 Epoch: 486 | ANN: trainLoss: 0.2517 | trainAcc: 90.1989% (635/704)\n",
            "11 13 Epoch: 486 | ANN: trainLoss: 0.2529 | trainAcc: 89.8438% (690/768)\n",
            "12 13 Epoch: 486 | ANN: trainLoss: 0.2501 | trainAcc: 89.8833% (693/771)\n",
            "0 4 Epoch: 486 | ANN: testLoss: 0.5159 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 486 | ANN: testLoss: 0.5224 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 486 | ANN: testLoss: 0.5386 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 486 | ANN: testLoss: 0.6233 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 487 | ANN: trainLoss: 0.1994 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 487 | ANN: trainLoss: 0.2077 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 487 | ANN: trainLoss: 0.2201 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 487 | ANN: trainLoss: 0.2248 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 487 | ANN: trainLoss: 0.2319 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 487 | ANN: trainLoss: 0.2348 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 487 | ANN: trainLoss: 0.2311 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 487 | ANN: trainLoss: 0.2380 | trainAcc: 90.0391% (461/512)\n",
            "8 13 Epoch: 487 | ANN: trainLoss: 0.2436 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 487 | ANN: trainLoss: 0.2442 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 487 | ANN: trainLoss: 0.2478 | trainAcc: 88.9205% (626/704)\n",
            "11 13 Epoch: 487 | ANN: trainLoss: 0.2537 | trainAcc: 88.5417% (680/768)\n",
            "12 13 Epoch: 487 | ANN: trainLoss: 0.2541 | trainAcc: 88.5863% (683/771)\n",
            "0 4 Epoch: 487 | ANN: testLoss: 0.5039 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 487 | ANN: testLoss: 0.5364 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 487 | ANN: testLoss: 0.5522 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 487 | ANN: testLoss: 0.4196 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 488 | ANN: trainLoss: 0.1878 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 488 | ANN: trainLoss: 0.1923 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 488 | ANN: trainLoss: 0.1938 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 488 | ANN: trainLoss: 0.1910 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 488 | ANN: trainLoss: 0.2182 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 488 | ANN: trainLoss: 0.2328 | trainAcc: 90.8854% (349/384)\n",
            "6 13 Epoch: 488 | ANN: trainLoss: 0.2485 | trainAcc: 90.1786% (404/448)\n",
            "7 13 Epoch: 488 | ANN: trainLoss: 0.2544 | trainAcc: 89.4531% (458/512)\n",
            "8 13 Epoch: 488 | ANN: trainLoss: 0.2644 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 488 | ANN: trainLoss: 0.2665 | trainAcc: 88.9062% (569/640)\n",
            "10 13 Epoch: 488 | ANN: trainLoss: 0.2690 | trainAcc: 88.7784% (625/704)\n",
            "11 13 Epoch: 488 | ANN: trainLoss: 0.2600 | trainAcc: 89.1927% (685/768)\n",
            "12 13 Epoch: 488 | ANN: trainLoss: 0.2621 | trainAcc: 89.2348% (688/771)\n",
            "0 4 Epoch: 488 | ANN: testLoss: 0.4889 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 488 | ANN: testLoss: 0.5807 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 488 | ANN: testLoss: 0.5427 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 488 | ANN: testLoss: 0.7039 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 489 | ANN: trainLoss: 0.1768 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 489 | ANN: trainLoss: 0.2159 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 489 | ANN: trainLoss: 0.2393 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 489 | ANN: trainLoss: 0.2333 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 489 | ANN: trainLoss: 0.2301 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 489 | ANN: trainLoss: 0.2546 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 489 | ANN: trainLoss: 0.2466 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 489 | ANN: trainLoss: 0.2557 | trainAcc: 88.0859% (451/512)\n",
            "8 13 Epoch: 489 | ANN: trainLoss: 0.2627 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 489 | ANN: trainLoss: 0.2612 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 489 | ANN: trainLoss: 0.2650 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 489 | ANN: trainLoss: 0.2739 | trainAcc: 87.7604% (674/768)\n",
            "12 13 Epoch: 489 | ANN: trainLoss: 0.2992 | trainAcc: 87.6783% (676/771)\n",
            "0 4 Epoch: 489 | ANN: testLoss: 0.4998 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 489 | ANN: testLoss: 0.5222 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 489 | ANN: testLoss: 0.5295 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 489 | ANN: testLoss: 0.5705 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 490 | ANN: trainLoss: 0.2138 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 490 | ANN: trainLoss: 0.2175 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 490 | ANN: trainLoss: 0.2280 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 490 | ANN: trainLoss: 0.2266 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 490 | ANN: trainLoss: 0.2329 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 490 | ANN: trainLoss: 0.2459 | trainAcc: 89.5833% (344/384)\n",
            "6 13 Epoch: 490 | ANN: trainLoss: 0.2471 | trainAcc: 89.5089% (401/448)\n",
            "7 13 Epoch: 490 | ANN: trainLoss: 0.2447 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 490 | ANN: trainLoss: 0.2499 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 490 | ANN: trainLoss: 0.2482 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 490 | ANN: trainLoss: 0.2419 | trainAcc: 89.6307% (631/704)\n",
            "11 13 Epoch: 490 | ANN: trainLoss: 0.2420 | trainAcc: 89.9740% (691/768)\n",
            "12 13 Epoch: 490 | ANN: trainLoss: 0.2480 | trainAcc: 90.0130% (694/771)\n",
            "0 4 Epoch: 490 | ANN: testLoss: 0.4733 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 490 | ANN: testLoss: 0.5851 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 490 | ANN: testLoss: 0.5437 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 490 | ANN: testLoss: 0.4078 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 491 | ANN: trainLoss: 0.2190 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 491 | ANN: trainLoss: 0.2463 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 491 | ANN: trainLoss: 0.2783 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 491 | ANN: trainLoss: 0.2894 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 491 | ANN: trainLoss: 0.2806 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 491 | ANN: trainLoss: 0.2865 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 491 | ANN: trainLoss: 0.2743 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 491 | ANN: trainLoss: 0.2748 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 491 | ANN: trainLoss: 0.2707 | trainAcc: 88.3681% (509/576)\n",
            "9 13 Epoch: 491 | ANN: trainLoss: 0.2709 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 491 | ANN: trainLoss: 0.2728 | trainAcc: 88.4943% (623/704)\n",
            "11 13 Epoch: 491 | ANN: trainLoss: 0.2674 | trainAcc: 88.9323% (683/768)\n",
            "12 13 Epoch: 491 | ANN: trainLoss: 0.2661 | trainAcc: 88.9754% (686/771)\n",
            "0 4 Epoch: 491 | ANN: testLoss: 0.6285 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 491 | ANN: testLoss: 0.5711 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 491 | ANN: testLoss: 0.5472 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 491 | ANN: testLoss: 0.4104 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 492 | ANN: trainLoss: 0.2826 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 492 | ANN: trainLoss: 0.3268 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 492 | ANN: trainLoss: 0.2808 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 492 | ANN: trainLoss: 0.2713 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 492 | ANN: trainLoss: 0.2751 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 492 | ANN: trainLoss: 0.2890 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 492 | ANN: trainLoss: 0.3018 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 492 | ANN: trainLoss: 0.2879 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 492 | ANN: trainLoss: 0.2872 | trainAcc: 88.1944% (508/576)\n",
            "9 13 Epoch: 492 | ANN: trainLoss: 0.2861 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 492 | ANN: trainLoss: 0.2792 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 492 | ANN: trainLoss: 0.2709 | trainAcc: 88.5417% (680/768)\n",
            "12 13 Epoch: 492 | ANN: trainLoss: 0.2644 | trainAcc: 88.5863% (683/771)\n",
            "0 4 Epoch: 492 | ANN: testLoss: 0.5187 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 492 | ANN: testLoss: 0.4921 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 492 | ANN: testLoss: 0.5454 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 492 | ANN: testLoss: 0.7234 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 493 | ANN: trainLoss: 0.2933 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 493 | ANN: trainLoss: 0.2910 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 493 | ANN: trainLoss: 0.2760 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 493 | ANN: trainLoss: 0.2863 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 493 | ANN: trainLoss: 0.2872 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 493 | ANN: trainLoss: 0.2950 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 493 | ANN: trainLoss: 0.2858 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 493 | ANN: trainLoss: 0.2754 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 493 | ANN: trainLoss: 0.2844 | trainAcc: 87.1528% (502/576)\n",
            "9 13 Epoch: 493 | ANN: trainLoss: 0.2809 | trainAcc: 87.5000% (560/640)\n",
            "10 13 Epoch: 493 | ANN: trainLoss: 0.2799 | trainAcc: 87.3580% (615/704)\n",
            "11 13 Epoch: 493 | ANN: trainLoss: 0.2775 | trainAcc: 87.6302% (673/768)\n",
            "12 13 Epoch: 493 | ANN: trainLoss: 0.2763 | trainAcc: 87.6783% (676/771)\n",
            "0 4 Epoch: 493 | ANN: testLoss: 0.5710 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 493 | ANN: testLoss: 0.5254 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 493 | ANN: testLoss: 0.5387 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 493 | ANN: testLoss: 0.5762 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 494 | ANN: trainLoss: 0.2887 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 494 | ANN: trainLoss: 0.2637 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 494 | ANN: trainLoss: 0.2939 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 494 | ANN: trainLoss: 0.2868 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 494 | ANN: trainLoss: 0.2851 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 494 | ANN: trainLoss: 0.2843 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 494 | ANN: trainLoss: 0.2769 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 494 | ANN: trainLoss: 0.2775 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 494 | ANN: trainLoss: 0.2902 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 494 | ANN: trainLoss: 0.2825 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 494 | ANN: trainLoss: 0.2917 | trainAcc: 87.6420% (617/704)\n",
            "11 13 Epoch: 494 | ANN: trainLoss: 0.2948 | trainAcc: 87.1094% (669/768)\n",
            "12 13 Epoch: 494 | ANN: trainLoss: 0.2864 | trainAcc: 87.1595% (672/771)\n",
            "0 4 Epoch: 494 | ANN: testLoss: 0.4186 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 494 | ANN: testLoss: 0.4991 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 494 | ANN: testLoss: 0.5484 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 494 | ANN: testLoss: 0.4116 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 495 | ANN: trainLoss: 0.3219 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 495 | ANN: trainLoss: 0.3105 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 495 | ANN: trainLoss: 0.2942 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 495 | ANN: trainLoss: 0.2979 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 495 | ANN: trainLoss: 0.2943 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 495 | ANN: trainLoss: 0.2877 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 495 | ANN: trainLoss: 0.2933 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 495 | ANN: trainLoss: 0.2747 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 495 | ANN: trainLoss: 0.2680 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 495 | ANN: trainLoss: 0.2663 | trainAcc: 88.9062% (569/640)\n",
            "10 13 Epoch: 495 | ANN: trainLoss: 0.2731 | trainAcc: 88.6364% (624/704)\n",
            "11 13 Epoch: 495 | ANN: trainLoss: 0.2687 | trainAcc: 88.5417% (680/768)\n",
            "12 13 Epoch: 495 | ANN: trainLoss: 0.2539 | trainAcc: 88.5863% (683/771)\n",
            "0 4 Epoch: 495 | ANN: testLoss: 0.5914 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 495 | ANN: testLoss: 0.5668 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 495 | ANN: testLoss: 0.5463 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 495 | ANN: testLoss: 0.5020 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 496 | ANN: trainLoss: 0.3381 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 496 | ANN: trainLoss: 0.2905 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 496 | ANN: trainLoss: 0.2720 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 496 | ANN: trainLoss: 0.2750 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 496 | ANN: trainLoss: 0.2880 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 496 | ANN: trainLoss: 0.2787 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 496 | ANN: trainLoss: 0.2673 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 496 | ANN: trainLoss: 0.2574 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 496 | ANN: trainLoss: 0.2518 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 496 | ANN: trainLoss: 0.2535 | trainAcc: 90.1562% (577/640)\n",
            "10 13 Epoch: 496 | ANN: trainLoss: 0.2576 | trainAcc: 90.0568% (634/704)\n",
            "11 13 Epoch: 496 | ANN: trainLoss: 0.2694 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 496 | ANN: trainLoss: 0.3041 | trainAcc: 89.2348% (688/771)\n",
            "0 4 Epoch: 496 | ANN: testLoss: 0.5141 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 496 | ANN: testLoss: 0.5546 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 496 | ANN: testLoss: 0.5357 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 496 | ANN: testLoss: 0.5118 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 497 | ANN: trainLoss: 0.2838 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 497 | ANN: trainLoss: 0.3101 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 497 | ANN: trainLoss: 0.2916 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 497 | ANN: trainLoss: 0.3026 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 497 | ANN: trainLoss: 0.3100 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 497 | ANN: trainLoss: 0.2887 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 497 | ANN: trainLoss: 0.2840 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 497 | ANN: trainLoss: 0.2954 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 497 | ANN: trainLoss: 0.2970 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 497 | ANN: trainLoss: 0.2960 | trainAcc: 87.5000% (560/640)\n",
            "10 13 Epoch: 497 | ANN: trainLoss: 0.2872 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 497 | ANN: trainLoss: 0.2852 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 497 | ANN: trainLoss: 0.2826 | trainAcc: 88.4565% (682/771)\n",
            "0 4 Epoch: 497 | ANN: testLoss: 0.4255 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 497 | ANN: testLoss: 0.5116 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 497 | ANN: testLoss: 0.5451 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 497 | ANN: testLoss: 0.4089 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 498 | ANN: trainLoss: 0.3364 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 498 | ANN: trainLoss: 0.3377 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 498 | ANN: trainLoss: 0.2933 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 498 | ANN: trainLoss: 0.2789 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 498 | ANN: trainLoss: 0.2741 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 498 | ANN: trainLoss: 0.2820 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 498 | ANN: trainLoss: 0.2679 | trainAcc: 89.5089% (401/448)\n",
            "7 13 Epoch: 498 | ANN: trainLoss: 0.2735 | trainAcc: 89.2578% (457/512)\n",
            "8 13 Epoch: 498 | ANN: trainLoss: 0.2734 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 498 | ANN: trainLoss: 0.2737 | trainAcc: 89.3750% (572/640)\n",
            "10 13 Epoch: 498 | ANN: trainLoss: 0.2703 | trainAcc: 89.7727% (632/704)\n",
            "11 13 Epoch: 498 | ANN: trainLoss: 0.2722 | trainAcc: 89.5833% (688/768)\n",
            "12 13 Epoch: 498 | ANN: trainLoss: 0.2717 | trainAcc: 89.6239% (691/771)\n",
            "0 4 Epoch: 498 | ANN: testLoss: 0.4908 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 498 | ANN: testLoss: 0.5154 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 498 | ANN: testLoss: 0.5348 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 498 | ANN: testLoss: 0.6826 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 499 | ANN: trainLoss: 0.3280 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 499 | ANN: trainLoss: 0.2974 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 499 | ANN: trainLoss: 0.2869 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 499 | ANN: trainLoss: 0.2928 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 499 | ANN: trainLoss: 0.2870 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 499 | ANN: trainLoss: 0.3099 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 499 | ANN: trainLoss: 0.2880 | trainAcc: 89.5089% (401/448)\n",
            "7 13 Epoch: 499 | ANN: trainLoss: 0.2883 | trainAcc: 89.4531% (458/512)\n",
            "8 13 Epoch: 499 | ANN: trainLoss: 0.2833 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 499 | ANN: trainLoss: 0.2787 | trainAcc: 89.5312% (573/640)\n",
            "10 13 Epoch: 499 | ANN: trainLoss: 0.2768 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 499 | ANN: trainLoss: 0.2774 | trainAcc: 89.1927% (685/768)\n",
            "12 13 Epoch: 499 | ANN: trainLoss: 0.3467 | trainAcc: 89.1051% (687/771)\n",
            "0 4 Epoch: 499 | ANN: testLoss: 0.6133 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 499 | ANN: testLoss: 0.5841 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 499 | ANN: testLoss: 0.5362 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 499 | ANN: testLoss: 0.5954 | testAcc: 68.9119% (133/193)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 408.13it/s]\n",
            "100%|██████████| 13/13 [00:00<00:00, 421.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 68.9100%\n",
            "SNN accuracy: max_norm: 67.8756%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7o4VT4r0r-xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "outputId": "aff001ae-acee-400e-dac8-a3e4ffb2620d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for all models...\n",
            "Evaluating LENet CNN...\n",
            "Evaluating LENet SNN...\n",
            "Evaluating LENet_FCL CNN...\n",
            "Evaluating LENet_FCL SNN...\n",
            "All predictions generated.\n",
            "LENet CNN|LENet SNN|LENet_FCL CNN|LENet_FCL SNN|"
          ]
        }
      ],
      "source": [
        "# @title Evaluate models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib # For colormaps access\n",
        "\n",
        "# Ensure all necessary variables from previous cells are available:\n",
        "# cnn_model_lenet_ccb, snn_model_lenet, cnn_model_lenet_fcl, snn_model_lenet_fcl\n",
        "# test_data, test_label (from the last train_test_split)\n",
        "# BATCH_SIZE, TIME_STEPS, device\n",
        "# data_loader function\n",
        "\n",
        "# 0. Prepare data loader for evaluation (using test_data from the last split)\n",
        "eval_test_loader = data_loader(test_data, test_label, batch=BATCH_SIZE, shuffle=False, drop=False)\n",
        "\n",
        "true_labels_list = []\n",
        "for _, targets_batch in eval_test_loader:\n",
        "    true_labels_list.extend(targets_batch.cpu().numpy())\n",
        "true_labels_np = np.array(true_labels_list)\n",
        "\n",
        "# --- 1. Get predictions for all models ---\n",
        "print(\"Generating predictions for all models...\")\n",
        "all_model_predictions = {}\n",
        "model_objects = {\n",
        "    \"LENet CNN\": cnn_model_lenet_ccb,\n",
        "    \"LENet SNN\": snn_model_lenet,\n",
        "    \"LENet_FCL CNN\": cnn_model_lenet_fcl,\n",
        "    \"LENet_FCL SNN\": snn_model_lenet_fcl\n",
        "}\n",
        "\n",
        "for model_name, model_obj in model_objects.items():\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    model_obj.eval().to(device)\n",
        "    current_preds = []\n",
        "    is_snn = \"SNN\" in model_name\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in eval_test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if is_snn:\n",
        "                for m_module in model_obj.modules():\n",
        "                    if hasattr(m_module, 'reset'):\n",
        "                        m_module.reset()\n",
        "                accumulated_outputs = None\n",
        "                for t in range(TIME_STEPS):\n",
        "                    outputs_t = model_obj(inputs)\n",
        "                    if accumulated_outputs is None:\n",
        "                        accumulated_outputs = outputs_t.clone()\n",
        "                    else:\n",
        "                        accumulated_outputs += outputs_t\n",
        "                _, predicted = accumulated_outputs.max(1)\n",
        "            else: # ANN\n",
        "                outputs = model_obj(inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "            current_preds.extend(predicted.cpu().numpy())\n",
        "    all_model_predictions[model_name] = current_preds\n",
        "print(\"All predictions generated.\")\n",
        "\n",
        "# --- 2. Calculate Accuracies for all models ---\n",
        "accuracy_results = {}\n",
        "class_names = ['Rest', 'Elbow', 'Hand'] # Corresponds to labels 0, 1, 2\n",
        "\n",
        "for model_name, predictions in all_model_predictions.items():\n",
        "    overall_acc = accuracy_score(true_labels_np, predictions)\n",
        "    accuracy_results[model_name] = {\"Overall\": overall_acc}\n",
        "    for class_idx, class_name_key in enumerate(class_names):\n",
        "        class_indices = np.where(true_labels_np == class_idx)[0]\n",
        "        if len(class_indices) > 0:\n",
        "            class_true = true_labels_np[class_indices]\n",
        "            class_pred = np.array(predictions)[class_indices]\n",
        "            class_acc = accuracy_score(class_true, class_pred)\n",
        "            accuracy_results[model_name][class_name_key] = class_acc\n",
        "        else:\n",
        "            accuracy_results[model_name][class_name_key] = np.nan\n",
        "for model_name in all_model_predictions:\n",
        "  print(model_name, end=\"|\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title visualize results (Custom confusion matrix, green good, red bad)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib # For colormaps access\n",
        "\n",
        "# Ensure all necessary variables from previous cells are available:\n",
        "# cnn_model_lenet_ccb, snn_model_lenet, cnn_model_lenet_fcl, snn_model_lenet_fcl\n",
        "# test_data, test_label (from the last train_test_split)\n",
        "# EPOCHS, DROP_OUT, TEST_SIZE, BATCH_SIZE, TIME_STEPS, device (from training blocks)\n",
        "# channel_count, num_classes, data_length (from data loading cell)\n",
        "# all_data, all_label (from data loading cell, for full dataset stats)\n",
        "# all_model_predictions, accuracy_results, true_labels_np (from previous evaluation steps)\n",
        "# data_loader function (defined in a previous cell)\n",
        "\n",
        "# --- 0. Report Header: Hyperparameters and Data Specifications (Table Format) ---\n",
        "print(\"--- Experiment Configuration & Data Specifications ---\")\n",
        "\n",
        "# Collect all parameters\n",
        "config_params = {\n",
        "    \"Dataset File\": \"sub-002_eeg90hz_7-35.mat\",\n",
        "    \"EEG Channels\": channel_count if 'channel_count' in locals() else \"N/A\",\n",
        "    \"Data Shape (Full Dataset)\": (\n",
        "        f\"Samples: {all_data.shape[0]}, Channels: {all_data.shape[1]}, Length: {all_data.shape[2]}\"\n",
        "        if 'all_data' in locals() and hasattr(all_data, 'shape') and len(all_data.shape) == 3\n",
        "        else \"N/A\"),\n",
        "    \"Class Distribution (Full Dataset)\": (\n",
        "        f\"Rest: {np.sum(all_label == 0)}, \"\n",
        "        f\"Elbow: {np.sum(all_label == 1)}, \"\n",
        "        f\"Hand: {np.sum(all_label == 2)}\"\n",
        "    ) if 'all_label' in locals() and isinstance(all_label, np.ndarray) and all_label.ndim >=1 else \"N/A\",\n",
        "    \"Bandpass Filter (Reported)\": \"7-35Hz\", #Change in case other filtering technique was used\n",
        "    \"Sampling Ratio (Reported)\": \"90Hz\", #Change in case other sampling was used\n",
        "    \"TRAINING EPOCHS\": EPOCHS if 'EPOCHS' in locals() else \"N/A\",\n",
        "    \"Dropout\": DROP_OUT if 'DROP_OUT' in locals() else \"N/A\",\n",
        "    \"Test Split\": TEST_SIZE if 'TEST_SIZE' in locals() else \"N/A\",\n",
        "    \"Batch Size (Evaluation)\": BATCH_SIZE if 'BATCH_SIZE' in locals() else \"N/A\",\n",
        "    \"SNN Time Steps\": TIME_STEPS if 'TIME_STEPS' in locals() else \"N/A\"\n",
        "}\n",
        "\n",
        "# Determine column widths\n",
        "max_key_len = max(len(key) for key in config_params.keys())\n",
        "# Ensure values are strings for len() calculation\n",
        "str_config_values = [str(val) for val in config_params.values()]\n",
        "max_val_len = max(len(val) for val in str_config_values)\n",
        "\n",
        "key_col_width = max_key_len + 2 # Add some padding\n",
        "val_col_width = max_val_len + 2\n",
        "\n",
        "# Print table header\n",
        "print(\"+\" + \"-\" * (key_col_width) + \"+\" + \"-\" * (val_col_width) + \"+\")\n",
        "print(f\"| {'Parameter':<{key_col_width-1}} | {'Value':<{val_col_width-1}} |\")\n",
        "print(\"+\" + \"=\" * (key_col_width) + \"+\" + \"=\" * (val_col_width) + \"+\")\n",
        "\n",
        "# Print table rows\n",
        "for idx, (key, value) in enumerate(config_params.items()):\n",
        "    print(f\"| {key:<{key_col_width-1}} | {str(value):<{val_col_width-1}} |\")\n",
        "\n",
        "print(\"+\" + \"-\" * (key_col_width) + \"+\" + \"-\" * (val_col_width) + \"+\")\n",
        "print(\"\\n\") # Add a bit of space before the main output\n",
        "\n",
        "# --- 1. Plot Confusion Matrices and Print Descriptions ---\n",
        "# This section assumes all_model_predictions, true_labels_np, accuracy_results are pre-computed\n",
        "# from previous cells if this cell is run in isolation.\n",
        "print(\"\\n--- Confusion Matrices and Descriptions ---\")\n",
        "class_names_display = ['Rest', 'Elbow', 'Hand'] # For display purposes in CM\n",
        "figure_counter = 1\n",
        "\n",
        "try:\n",
        "    cmap_greens = matplotlib.colormaps['Greens']\n",
        "    cmap_reds = matplotlib.colormaps['Reds']\n",
        "except AttributeError: # Older matplotlib\n",
        "    cmap_greens = plt.cm.get_cmap('Greens')\n",
        "    cmap_reds = plt.cm.get_cmap('Reds')\n",
        "\n",
        "# Thresholds for color logic\n",
        "threshold_diagonal_good = 0.5  # Above this is green on diagonal\n",
        "threshold_off_diagonal_bad = 0.2 # Above this is red on off-diagonal (significant misclassification)\n",
        "\n",
        "# Check if necessary variables for plotting exist\n",
        "if 'all_model_predictions' in locals() and 'true_labels_np' in locals() and 'accuracy_results' in locals():\n",
        "    for model_name, predictions in all_model_predictions.items():\n",
        "        cm = confusion_matrix(true_labels_np, predictions, labels=range(len(class_names_display)))\n",
        "\n",
        "        # --- Custom Color Logic V2 ---\n",
        "        row_sums = cm.sum(axis=1, keepdims=True)\n",
        "        cm_normalized_row = np.zeros_like(cm, dtype=float)\n",
        "        for r_idx in range(cm.shape[0]):\n",
        "            if row_sums[r_idx, 0] > 0:\n",
        "                cm_normalized_row[r_idx, :] = cm[r_idx, :] / row_sums[r_idx, 0]\n",
        "\n",
        "        num_classes_cm = cm.shape[0]\n",
        "        color_matrix_rgb = np.zeros((num_classes_cm, num_classes_cm, 3))\n",
        "        colormap_input_values = np.zeros((num_classes_cm, num_classes_cm))\n",
        "\n",
        "\n",
        "        for i in range(num_classes_cm):\n",
        "            for j in range(num_classes_cm):\n",
        "                norm_value = cm_normalized_row[i, j]\n",
        "                color_val_for_cmap = 0.0\n",
        "\n",
        "                if row_sums[i, 0] == 0:\n",
        "                    color_matrix_rgb[i, j, :] = [0.95, 0.95, 0.95]\n",
        "                elif i == j:\n",
        "                    if norm_value > threshold_diagonal_good:\n",
        "                        color_val_for_cmap = norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_greens(color_val_for_cmap)[:3]\n",
        "                    else:\n",
        "                        color_val_for_cmap = 1.0 - norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_reds(color_val_for_cmap)[:3]\n",
        "                else:\n",
        "                    if norm_value > threshold_off_diagonal_bad:\n",
        "                        color_val_for_cmap = norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_reds(color_val_for_cmap)[:3]\n",
        "                    else:\n",
        "                        color_val_for_cmap = 1.0 - norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_greens(color_val_for_cmap)[:3]\n",
        "                colormap_input_values[i,j] = color_val_for_cmap\n",
        "        # --- End Custom Color Logic V2 ---\n",
        "\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names_display)\n",
        "        fig, ax = plt.subplots(figsize=(8, 7))\n",
        "        disp.plot(ax=ax, include_values=True, cmap='Greys', colorbar=False, values_format='d')\n",
        "\n",
        "        if ax.images:\n",
        "            ax.images[0].remove()\n",
        "        ax.imshow(color_matrix_rgb)\n",
        "\n",
        "        if disp.text_ is not None:\n",
        "            for i in range(num_classes_cm):\n",
        "                for j in range(num_classes_cm):\n",
        "                    if disp.text_[i, j] is not None:\n",
        "                        text_color = \"white\" if colormap_input_values[i,j] > 0.5 else \"black\"\n",
        "                        if row_sums[i,0] == 0:\n",
        "                            text_color = \"black\"\n",
        "                        disp.text_[i, j].set_color(text_color)\n",
        "\n",
        "        ax.set_title(f'Confusion Matrix - {model_name}\\n(Custom Good/Bad Row-Normalized Colors)')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\nFigure {figure_counter}: Confusion Matrix for {model_name}.\")\n",
        "        print(f\"This matrix visualizes classification performance with custom colors based on row-normalized values\")\n",
        "\n",
        "\n",
        "        # Ensure class_names is defined for accessing accuracy_results\n",
        "        class_names_for_acc = ['Rest', 'Elbow', 'Hand'] # Define or ensure it's passed from global scope\n",
        "\n",
        "        overall_acc_val = accuracy_results[model_name]['Overall'] * 100\n",
        "        print(f\"Overall Accuracy: {overall_acc_val:.2f}%.\")\n",
        "        for class_idx, class_name_label in enumerate(class_names_display):\n",
        "            # Use a consistent key for accuracy_results, assuming class_names_for_acc matches keys\n",
        "            current_class_key = class_names_for_acc[class_idx]\n",
        "            if current_class_key in accuracy_results[model_name]:\n",
        "                class_acc_val = accuracy_results[model_name][current_class_key]\n",
        "                if not np.isnan(class_acc_val):\n",
        "                    print(f\"Accuracy for {class_name_label}: {class_acc_val*100:.2f}%.\")\n",
        "                else:\n",
        "                    print(f\"Accuracy for {class_name_label}: N/A (value is NaN).\")\n",
        "            else:\n",
        "                print(f\"Accuracy for {class_name_label}: N/A (key not found).\")\n",
        "        print(\"-\" * 70)\n",
        "        figure_counter += 1\n",
        "else:\n",
        "    print(\"Required variables (all_model_predictions, true_labels_np, or accuracy_results) not found for plotting confusion matrices.\")\n",
        "\n",
        "\n",
        "# --- 2. Accuracy Table ---\n",
        "print(\"\\n\\n--- Model Performance Summary Table ---\")\n",
        "# Ensure class_names is defined for table header\n",
        "class_names_for_table = ['Rest', 'Elbow', 'Hand'] # Define or ensure it's passed\n",
        "\n",
        "if 'accuracy_results' in locals():\n",
        "    header = f\"| {'Model':<17} | {'Overall Acc.':<15} | {class_names_for_table[0]+' Acc.':<12} | {class_names_for_table[1]+' Acc.':<12} | {class_names_for_table[2]+' Acc.':<12} |\"\n",
        "    separator = \"|-------------------|-----------------|--------------|--------------|--------------|\"\n",
        "    print(header)\n",
        "    print(separator)\n",
        "    for model_name_key in accuracy_results:\n",
        "        overall_str = f\"{accuracy_results[model_name_key]['Overall']*100:.2f}%\"\n",
        "\n",
        "        rest_acc_val = accuracy_results[model_name_key].get(class_names_for_table[0], np.nan)\n",
        "        rest_str = f\"{rest_acc_val*100:.2f}%\" if not np.isnan(rest_acc_val) else \"N/A\"\n",
        "\n",
        "        elbow_acc_val = accuracy_results[model_name_key].get(class_names_for_table[1], np.nan)\n",
        "        elbow_str = f\"{elbow_acc_val*100:.2f}%\" if not np.isnan(elbow_acc_val) else \"N/A\"\n",
        "\n",
        "        hand_acc_val = accuracy_results[model_name_key].get(class_names_for_table[2], np.nan)\n",
        "        hand_str = f\"{hand_acc_val*100:.2f}%\" if not np.isnan(hand_acc_val) else \"N/A\"\n",
        "\n",
        "        row = f\"| {model_name_key:<17} | {overall_str:<15} | {rest_str:<12} | {elbow_str:<12} | {hand_str:<12} |\"\n",
        "        print(row)\n",
        "else:\n",
        "    print(\"Accuracy results not available for summary table.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "collapsed": true,
        "id": "sBup_2nVClAB",
        "outputId": "dbd49552-3c37-4cf5-8a4a-afba937edc0a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Experiment Configuration & Data Specifications ---\n",
            "+-----------------------------------+-----------------------------------------+\n",
            "| Parameter                          | Value                                    |\n",
            "+===================================+=========================================+\n",
            "| Dataset File                       | sub-002_eeg90hz_7-35.mat                 |\n",
            "| EEG Channels                       | 62                                       |\n",
            "| Data Shape (Full Dataset)          | Samples: 964, Channels: 62, Length: 360  |\n",
            "| Class Distribution (Full Dataset)  | Rest: 364, Elbow: 300, Hand: 300         |\n",
            "| Bandpass Filter (Reported)         | 7-35Hz                                   |\n",
            "| Sampling Ratio (Reported)          | 90Hz                                     |\n",
            "| TRAINING EPOCHS                    | 500                                      |\n",
            "| Dropout                            | 0.35                                     |\n",
            "| Test Split                         | 0.2                                      |\n",
            "| Batch Size (Evaluation)            | 64                                       |\n",
            "| SNN Time Steps                     | 100                                      |\n",
            "+-----------------------------------+-----------------------------------------+\n",
            "\n",
            "\n",
            "\n",
            "--- Confusion Matrices and Descriptions ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 1: Confusion Matrix for LENet CNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 69.95%.\n",
            "Accuracy for Rest: 100.00%.\n",
            "Accuracy for Elbow: 47.62%.\n",
            "Accuracy for Hand: 59.68%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 2: Confusion Matrix for LENet SNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 69.95%.\n",
            "Accuracy for Rest: 100.00%.\n",
            "Accuracy for Elbow: 46.03%.\n",
            "Accuracy for Hand: 61.29%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 3: Confusion Matrix for LENet_FCL CNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 68.91%.\n",
            "Accuracy for Rest: 100.00%.\n",
            "Accuracy for Elbow: 47.62%.\n",
            "Accuracy for Hand: 56.45%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAAKyCAYAAADl4AdrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZwdJREFUeJzt3Xd4FFXfxvF70wMhCSUQmqGEFkE6PtSEh65IUxAETBBURKp0BemCoIBYAAUBFQtFQIoiqHREBRKQ3nsvCTX1vH/wZh+WJJBAmFC+n+vaC/bM2ZnfbHazd87MnLUZY4wAAAAACzlldAEAAAB4/BBCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBpGjPnj2qW7eufHx8ZLPZNH/+/HRd/8GDB2Wz2TR9+vR0Xe/DLCQkRCEhIRldBgDcd4RQ4AG3b98+vf766ypUqJA8PDzk7e2tqlWr6qOPPtK1a9fu67ZDQ0O1detWjRgxQl9//bUqVKhwX7dnpbCwMNlsNnl7eyf7PO7Zs0c2m002m00ffPBBmtd//PhxDR48WOHh4elQrTUKFCighg0b3rZP4vOW3M3Dw8Peb8WKFfb2jRs3JrseLy+vu6pzyZIlGjx48F099nb1//LLLw59o6KiNGTIEJUuXVpeXl7y9PRUyZIl1bdvXx0/fjxd9uXMmTPq1q2bihcvLk9PT+XMmVOVKlVS3759dfny5SR1P/XUU0ru27ZtNps6d+5sv5/4B57NZtPcuXOT9B88eLBsNpvOnj17V3UD6cElowsAkLLFixerefPmcnd318svv6ySJUsqJiZGa9asUe/evbVt2zZ9/vnn92Xb165d0/r16/XOO+84fLilp4CAAF27dk2urq73Zf134uLioqtXr2rhwoVq0aKFw7KZM2fKw8ND169fv6t1Hz9+XEOGDFGBAgVUpkyZVD/u119/vavtWcnd3V1TpkxJ0u7s7Jxs/8GDB2vhwoXptv0lS5bo008/vesgmlL9pUuXtv9///79ql27tg4fPqzmzZvrtddek5ubm7Zs2aKpU6dq3rx52r17993ugiTp/PnzqlChgqKiovTKK6+oePHiOnfunLZs2aKJEyfqjTfeSBJut27dqh9//FHPP/98qrczdOhQNWvWTDab7Z7qBdIbIRR4QB04cEAtW7ZUQECAfv/9d+XOndu+7M0339TevXu1ePHi+7b9M2fOSJJ8fX3v2zZuHT2zmru7u6pWrarvvvsuSQj99ttv9eyzzyY7inQ/XL16VZkyZZKbm5sl27sXLi4uatOmTar6lilTRosWLdKmTZtUrly5+1xZ6typ/ri4ODVr1kynTp3SihUrVK1aNYflI0aM0Pvvv3/PdUydOlWHDx/W2rVrVaVKFYdlUVFRSV4Lnp6eyp8/f5pCZZkyZRQeHq558+apWbNm91wzkJ44HA88oEaPHq3Lly9r6tSpDgE0UWBgoLp162a/HxcXp2HDhqlw4cJyd3dXgQIF9Pbbbys6OtrhcYmHXNesWaNKlSrJw8NDhQoV0ldffWXvM3jwYAUEBEiSevfuLZvNpgIFCki6cVgw8f83Szy8d7Nly5apWrVq8vX1lZeXl4oVK6a3337bvjylc0J///13Va9eXZkzZ5avr68aN26sHTt2JLu9vXv3KiwsTL6+vvLx8VG7du109erVlJ/YW7z00kv6+eefdfHiRXvb33//rT179uill15K0v/8+fPq1auXSpUqJS8vL3l7e6tBgwaKiIiw91mxYoUqVqwoSWrXrp39sGjifoaEhKhkyZLauHGjatSooUyZMtmfl1vPCQ0NDZWHh0eS/a9Xr56yZs3qcFj4QdSlSxdlzZo11aOWP//8s/1nnyVLFj377LPatm2bfXlYWJg+/fRTSXI4lJ6e5s6dq4iICL3zzjtJAqgkeXt7a8SIEfe8nX379snZ2Vn/+c9/kt3GrX+gOTk5acCAAdqyZYvmzZuXqm20bNlSRYsW1dChQ5M9jA9kJEIo8IBauHChChUqlGSEJCUdOnTQu+++q3LlymncuHEKDg7WyJEj1bJlyyR99+7dqxdeeEF16tTRhx9+qKxZsyosLMz+Yd+sWTONGzdOktSqVSt9/fXXGj9+fJrq37Ztmxo2bKjo6GgNHTpUH374oRo1aqS1a9fe9nHLly9XvXr1dPr0aQ0ePFhvvfWW1q1bp6pVq+rgwYNJ+rdo0UKXLl3SyJEj1aJFC02fPl1DhgxJdZ2JI0o//vijve3bb79V8eLFkx25279/v+bPn6+GDRtq7Nix6t27t7Zu3arg4GB7ICxRooSGDh0qSXrttdf09ddf6+uvv1aNGjXs6zl37pwaNGigMmXKaPz48apZs2ay9X300Ufy8/NTaGio4uPjJUmTJ0/Wr7/+qo8//lh58uRJ9b6mp7Nnzya5RUVFJenn7e2tHj16aOHChdq0adNt1/n111/r2WeflZeXl95//30NHDhQ27dvV7Vq1ew/+9dff1116tSx90+83Wv9kZGR9mU//fSTJKlt27ZpXm9aBAQEKD4+Pk31v/TSSypSpEiqQ6Wzs7MGDBigiIiIVAdXwDIGwAMnMjLSSDKNGzdOVf/w8HAjyXTo0MGhvVevXkaS+f333+1tAQEBRpJZtWqVve306dPG3d3d9OzZ09524MABI8mMGTPGYZ2hoaEmICAgSQ2DBg0yN/9KGTdunJFkzpw5k2LdiduYNm2ava1MmTImZ86c5ty5c/a2iIgI4+TkZF5++eUk23vllVcc1tm0aVOTPXv2FLd5835kzpzZGGPMCy+8YGrVqmWMMSY+Pt74+/ubIUOGJPscXL9+3cTHxyfZD3d3dzN06FB7299//51k3xIFBwcbSWbSpEnJLgsODnZoW7p0qZFkhg8fbvbv32+8vLxMkyZN7riPaRUQEGCeffbZ2/YJDQ01kpK91atXz97vjz/+MJLM7NmzzcWLF03WrFlNo0aNHNaT+PwbY8ylS5eMr6+vefXVVx22d/LkSePj4+PQ/uabb5q7/fhKqf6bn/OyZcsaHx+fNK3z5n1JrZMnTxo/Pz8jyRQvXtx07NjRfPvtt+bixYu33caMGTOMJPPjjz/al0syb775pv3+za/duLg4U6RIEVO6dGmTkJBgjPnf++d270/gfmMkFHgAJY4oZcmSJVX9lyxZIkl66623HNp79uwpSUnOHQ0KClL16tXt9/38/FSsWDHt37//rmu+VeK5pAsWLFBCQkKqHnPixAmFh4crLCxM2bJls7c/9dRTqlOnjn0/b9axY0eH+9WrV9e5c+eSHZVLyUsvvaQVK1bo5MmT+v3333Xy5MlkD8VLN84jdXK68aszPj5e586ds59qcKeRvlvX065du1T1rVu3rl5//XX7uYAeHh6aPHlyqreV3jw8PLRs2bIkt1GjRiXb38fHR927d9dPP/2kzZs3J9tn2bJlunjxolq1auUwQuns7Kynn35af/zxx32t/8MPP7Qvj4qKSvV7717kypVLERER6tixoy5cuKBJkybppZdeUs6cOTVs2LAURzpbt25916Oh6T3NGnAvCKHAA8jb21uSdOnSpVT1P3TokJycnBQYGOjQ7u/vL19fXx06dMih/YknnkiyjqxZs+rChQt3WXFSL774oqpWraoOHTooV65catmypWbNmnXbQJpYZ7FixZIsK1GihM6ePasrV644tN+6L1mzZpWkNO3LM888oyxZsuiHH37QzJkzVbFixSTPZaKEhASNGzdORYoUkbu7u3LkyCE/Pz9t2bLF4ZDuneTNmzdNFyF98MEHypYtm8LDwzVhwgTlzJnzjo85c+aMTp48ab/dPOXPvXB2dlbt2rWT3G43C0C3bt3k6+ub4rmhe/bskST997//lZ+fn8Pt119/1enTp9Ol9pTqL1++vH25t7d3qt979yp37tyaOHGiTpw4oV27dmnChAny8/PTu+++q6lTp6ZY/4ABAxQeHp7qUNm6dWsFBgZybigeKIRQ4AHk7e2tPHny6N9//03T41J7gUZKU+mk5sMppW0knq+YyNPTU6tWrdLy5cvVtm1bbdmyRS+++KLq1KmTpO+9uJd9SeTu7q5mzZppxowZmjdvXoqjoJL03nvv6a233lKNGjX0zTffaOnSpVq2bJmefPLJVI/4Sjeen7TYvHmzPYht3bo1VY+pWLGicufObb/dzXyn6eVOo6GJz93XX3+d7CjrggULLKu1ePHiioyM1JEjRyzbps1mU9GiRdWlSxetWrVKTk5OmjlzZor90xoqbw6uVj6XwO0QQoEHVMOGDbVv3z6tX7/+jn0DAgKUkJBgH01KdOrUKV28eNF+pXt6yJo1q8OV5IluHW2VblzNW6tWLY0dO1bbt2/XiBEj9Pvvv6d4aDWxzl27diVZtnPnTuXIkUOZM2e+tx1IwUsvvaTNmzfr0qVLyV7MlWjOnDmqWbOmpk6dqpYtW6pu3bqqXbt2kuckPa/YvnLlitq1a6egoCC99tprGj16tP7+++87Pm7mzJkOQe7ll19Ot5ruRvfu3eXr65vshWOFCxeWJOXMmTPZUdabZwy43/NdPvfcc5Kkb7755r5uJyWFChVS1qxZdeLEiRT73E2obNOmjQIDAzVkyBBGQ/FAIIQCD6g+ffooc+bM6tChg06dOpVk+b59+/TRRx9JunE4WVKSK9jHjh0rSXr22WfTra7ChQsrMjJSW7ZssbedOHEiyZW358+fT/LYxMO1t04blSh37twqU6aMZsyY4RDq/v33X/3666/2/bwfatasqWHDhumTTz6Rv79/iv2cnZ2TfIDPnj1bx44dc2hLDMvJBfa06tu3rw4fPqwZM2Zo7NixKlCggEJDQ1N8HhNVrVrVIcgVKlTonmu5F4mjoQsWLEjyTVL16tWTt7e33nvvPcXGxiZ5bOK8tVL6PrfJeeGFF1SqVCmNGDEi2T8CL126pHfeeeeet7Nhw4Ykp5dI0l9//aVz584le1rKzW4Olalxc3BNnAEAyEhMVg88oAoXLqxvv/1WL774okqUKOHwjUnr1q3T7NmzFRYWJunGN72Ehobq888/18WLFxUcHKy//vpLM2bMUJMmTVKc/udutGzZUn379lXTpk3VtWtXXb16VRMnTlTRokUdLswZOnSoVq1apWeffVYBAQE6ffq0PvvsM+XLly/ZuRcTjRkzRg0aNFDlypXVvn17Xbt2TR9//LF8fHzu+htyUiNxDsY7adiwoYYOHap27dqpSpUq2rp1q2bOnJkk4BUuXFi+vr6aNGmSsmTJosyZM+vpp59WwYIF01TX77//rs8++0yDBg2yTxk1bdo0hYSEaODAgRo9enSa1ncne/fu1fDhw5O0ly1b1v7HTFxcXIqjhE2bNr3taHW3bt00btw4RUREOPTz9vbWxIkT1bZtW5UrV04tW7aUn5+fDh8+rMWLF6tq1ar65JNPJMl+/mbXrl1Vr149OTs733b0Oq1cXV31448/qnbt2qpRo4ZatGihqlWrytXVVdu2bdO3336rrFmzOswVGhsbm+zzli1bNnXq1CnZ7Xz99deaOXOmmjZtqvLly8vNzU07duzQl19+KQ8PD4c5dZPj7Oysd955J9UXuEk3DuMPGzbsofo6WTzCMu7CfACpsXv3bvPqq6+aAgUKGDc3N5MlSxZTtWpV8/HHH5vr16/b+8XGxpohQ4aYggULGldXV5M/f37Tv39/hz7GpDwNz61TA6U0RZMxxvz666+mZMmSxs3NzRQrVsx88803SaZo+u2330zjxo1Nnjx5jJubm8mTJ49p1aqV2b17d5Jt3DqN0fLly03VqlWNp6en8fb2Ns8995zZvn27Q5+UppiZNm2akWQOHDiQ4nNqTOqm1UlpiqaePXua3LlzG09PT1O1alWzfv36ZKdWWrBggQkKCjIuLi4O+xkcHGyefPLJZLd583qioqJMQECAKVeunImNjXXo16NHD+Pk5GTWr19/231Ii8Tpu5K7tW/f3hhz+ymabn7eb56i6VaJP7vknv8//vjD1KtXz/j4+BgPDw9TuHBhExYWZv755x97n7i4ONOlSxfj5+dnbDZbmqZrSst0ShcuXDDvvvuuKVWqlMmUKZPx8PAwJUuWNP379zcnTpxwWGdKz0fhwoVTXP+WLVtM7969Tbly5Uy2bNmMi4uLyZ07t2nevLnZtGlTquqOjY01hQsXvu0UTbdKfI8k9/4BrGQzhhNDAAAAYC3OCQUAAIDlOCcUAPDQO3/+vGJiYlJc7uzsLD8/PwsrAnAnHI4HADz0QkJCtHLlyhSXBwQE2L9/HsCDgRAKAHjobdy48bbfkuXp6amqVataWBGAOyGEAgAAwHJcmAQAAADLEUKB/zd69GgVL148Td//jQfHwYMHZbPZNH369IwuxcHgwYPv+9dM4t4l93MqUKCA/QshrDJ9+nTZbLYMO381I/ZZkvr166enn37a8u0iYxFCAUlRUVF6//331bdvXzk5Ob4trl+/rnHjxunpp5+Wj4+PPDw8VLRoUXXu3Fm7d+++L/Vs375dgwcPfmAvpIiKitKIESNUoUIF+fj4yN3dXQEBAXrxxRe1ePHijC4viQsXLsjFxUWzZs2SdOOD1maz2W8eHh4qUqSIevfunezXjVohLCzMoSZ3d3cVLVpU7777rq5fv54hNd0sMeTbbDbNnTs3yfLEEHf27NkMqO7xFR4erjZt2ih//vxyd3dXtmzZVLt2bU2bNk3x8fEZXV6qde/eXREREXyd6GOGKZoASV9++aXi4uLUqlUrh/azZ8+qfv362rhxoxo2bKiXXnpJXl5e2rVrl77//nt9/vnnt50W5m5t375dQ4YMUUhIiAoUKJDu678Xe/fuVb169XTo0CE1bdpUL7/8sry8vHTkyBEtWbJEDRs21FdffaW2bdtmdKl2S5culc1mU926de1tZcqUUc+ePSXd+ENj48aNGj9+vFauXKm//vorQ+p0d3fXlClTJEmRkZFasGCBhg0bpn379mnmzJkZUlNyhg4dqmbNmj3yI7y7du1K8kfpg2TKlCnq2LGjcuXKpbZt26pIkSK6dOmSfvvtN7Vv314nTpy441d/Pij8/f3VuHFjffDBB2rUqFFGlwOLEEIB3fgu7kaNGsnDw8OhPSwsTJs3b9acOXP0/PPPOywbNmyY3nnnHSvLzHBxcXFq2rSpTp06pZUrVya52njQoEH69ddfH7gRmCVLlqhq1ary9fW1t+XNm1dt2rSx3+/QoYO8vLz0wQcfaM+ePSpSpIjldbq4uDjU1KlTJ1WpUkXfffedxo4dq1y5clle063KlCmj8PBwzZs3T82aNbtv27ly5cptv4PeCu7u7hm6/dv5888/1bFjR1WuXFlLlixRlixZ7Mu6d++uf/75R//++28GVnjjjzs3N7dUB/kWLVqoefPm2r9/vwoVKnSfq8OD4MH9Ew+wyIEDB7RlyxbVrl3boX3Dhg1avHix2rdvnySASjc+oD744AP7/ZCQEIWEhCTpFxYWlmQ08/vvv1f58uWVJUsWeXt7q1SpUvroo48k3TgnrHnz5pKkmjVr2g+Brlixwv74zz77TE8++aTc3d2VJ08evfnmm7p48aLDNkJCQlSyZElt2bJFwcHBypQpkwIDAzVnzhxJ0sqVK/X000/L09NTxYoV0/Lly+/4XM2ePVv//vuvBg4cmOJ0N3Xr1lWDBg0c2vbv36/mzZsrW7ZsypQpk/7zn/8ke9j+9OnTat++vXLlyiUPDw+VLl1aM2bMSNLv4sWLCgsLk4+Pj3x9fRUaGppk/xMlJCTol19+0bPPPnvH/fP395d0Iwwm2rJli8LCwlSoUCF5eHjI399fr7zyis6dO5fk8WvWrFHFihXl4eGhwoULa/LkyXfc5u3YbDZVq1ZNxhjt37/fYdmdXgMTJkyQs7OzQ9uHH34om82mt956y94WHx+vLFmyqG/fvqmqqWXLlipatKiGDh2q1EyuMnv2bJUvX16enp7KkSOH2rRpo2PHjjn0CQsLk5eXl/bt26dnnnlGWbJkUevWre3PQefOnTV79mwFBQXJ09NTlStX1tatWyVJkydPVmBgoDw8PBQSEpLkFJbVq1erefPmeuKJJ+Tu7q78+fOrR48eunbt2h1rv/X8yJtPl7j1dvN2d+7cqRdeeEHZsmWTh4eHKlSokOxh5m3btum///2vPD09lS9fPg0fPjzV56QPGTJENptNM2fOdAigiSpUqOBQ+5UrV9SzZ0/7YftixYrpgw8+SNXPMDXv3xUrVshms+n777/XgAEDlDdvXmXKlElRUVGKjY3VkCFDVKRIEXl4eCh79uyqVq2ali1b5rCOxN/BCxYsSNVzgIcfI6F47K1bt06SVK5cOYf2xA+N9D6svGzZMrVq1Uq1atXS+++/L0nasWOH1q5dq27duqlGjRrq2rWrJkyYoLffflslSpSQJPu/gwcP1pAhQ1S7dm298cYb2rVrlyZOnKi///5ba9eulaurq31bFy5cUMOGDdWyZUs1b95cEydOVMuWLTVz5kx1795dHTt21EsvvaQxY8bohRde0JEjR5L9QEu0cOFCSXIYrbuTU6dOqUqVKrp69aq6du2q7Nmza8aMGWrUqJHmzJmjpk2bSpKuXbumkJAQ7d27V507d1bBggU1e/ZshYWF6eLFi+rWrZskyRijxo0ba82aNerYsaNKlCihefPmKTQ0NNnt//333zpz5oyeeeYZh/bY2Fj7+YvXr1/X5s2bNXbsWNWoUUMFCxZ0+Hnt379f7dq1k7+/v7Zt26bPP/9c27Zt059//mk/JL1161bVrVtXfn5+Gjx4sOLi4jRo0KB7Hr1MDDdZs2a1t6XmNVC9enUlJCRozZo1atiwoaQbgczJyUmrV6+2r2vz5s26fPmyatSokap6nJ2dNWDAAL388st3HA2dPn262rVrp4oVK2rkyJE6deqUPvroI61du1abN292GJmOi4tTvXr1VK1aNX3wwQfKlCmTfdnq1av1008/6c0335QkjRw5Ug0bNlSfPn302WefqVOnTrpw4YJGjx6tV155Rb///rv9sbNnz9bVq1f1xhtvKHv27Prrr7/08ccf6+jRo5o9e3aq9jnR119/naRtwIABOn36tLy8vCTdCJZVq1ZV3rx51a9fP2XOnFmzZs1SkyZNNHfuXPvr/eTJk6pZs6bi4uLs/T7//HN5enresY6rV6/qt99+U40aNfTEE0/csb8xRo0aNdIff/yh9u3bq0yZMlq6dKl69+6tY8eOady4cSk+NrXv30TDhg2Tm5ubevXqpejoaLm5uWnw4MEaOXKkOnTooEqVKikqKkr//POPNm3apDp16tgf6+Pjo8KFC2vt2rXq0aPHHfcLjwADPOYGDBhgJJlLly45tDdt2tRIMhcuXEjVeoKDg01wcHCS9tDQUBMQEGC/361bN+Pt7W3i4uJSXNfs2bONJPPHH384tJ8+fdq4ubmZunXrmvj4eHv7J598YiSZL7/80qEeSebbb7+1t+3cudNIMk5OTubPP/+0ty9dutRIMtOmTbvtPpYtW9b4+vomab98+bI5c+aM/RYZGWlf1r17dyPJrF692t526dIlU7BgQVOgQAH7fowfP95IMt988429X0xMjKlcubLx8vIyUVFRxhhj5s+fbySZ0aNH2/vFxcWZ6tWrJ7sPAwcOdHj+jTEmICDASEpyq1q1qjl79qxD36tXrybZ3++++85IMqtWrbK3NWnSxHh4eJhDhw7Z27Zv326cnZ1Nan7VhoaGmsyZM9ufw71795oPPvjA2Gw2U7JkSZOQkGCMSf1rID4+3nh7e5s+ffoYY4xJSEgw2bNnN82bNzfOzs721/vYsWONk5PTHV/nBw4cMJLMmDFjTFxcnClSpIgpXbq0va5BgwYZSebMmTPGmBs/u5w5c5qSJUuaa9eu2dezaNEiI8m8++67DvsuyfTr1y/JdiUZd3d3c+DAAXvb5MmTjSTj7+9vf10YY0z//v2NJIe+yf38Ro4caWw2m8PPKrH+mwUEBJjQ0NAUn5PRo0cbSearr76yt9WqVcuUKlXKXL9+3d6WkJBgqlSpYooUKWJvS3xfbNiwwd52+vRp4+Pjk2QfbhUREWEkmW7duqXY52aJ75nhw4c7tL/wwgvGZrOZvXv32ttu3efUvn//+OMPI8kUKlQoyXNeunRp8+yzz6aq1rp165oSJUqkqi8efhyOx2Pv3LlzcnFxsY9kJIqKipKk244M3g1fX19duXIlyaGo1Fi+fLliYmLUvXt3h/OsXn31VXl7eyc5RObl5aWWLVva7xcrVky+vr4qUaKEw3Qoif+/9ZDvraKiopI8T5L0zjvvyM/Pz3576aWX7MuWLFmiSpUqqVq1ag51vfbaazp48KC2b99u7+fv7+9wcZirq6u6du2qy5cv27+SccmSJXJxcdEbb7xh7+fs7KwuXbokW/OSJUuSPRT/9NNPa9myZVq2bJkWLVqkESNGaNu2bWrUqJHDodqbR6auX7+us2fP6j//+Y8kadOmTZJuHNJeunSpmjRp4jAyVaJECdWrVy/ZupJz5coV+3MYGBioXr16qWrVqlqwYIF9xDW1rwEnJydVqVJFq1atknRjtP3cuXPq16+fjDFav369pBujjCVLlnQYlbyTxNHQiIgIzZ8/P9k+//zzj06fPq1OnTo5nGv97LPPqnjx4smejnHzz/RmtWrVcjilJfH1+vzzzzu8P5N7Hd/887ty5YrOnj2rKlWqyBijzZs333lnU/DHH3+of//+6tKli/1oyfnz5/X777+rRYsWunTpks6ePauzZ8/q3Llzqlevnvbs2WM/FWHJkiX6z3/+o0qVKtnX6efnZz8N4XbS+rtpyZIlcnZ2VteuXR3ae/bsKWOMfv7559s+NjXv30ShoaFJRnN9fX21bds27dmz5461Zs2alRkWHiOEUCAF3t7ekqRLly6l63o7deqkokWLqkGDBsqXL59eeeUV/fLLL6l67KFDhyTdCJM3c3NzU6FChezLE+XLly/JFcw+Pj7Knz9/kjZJt/3aQ+nGh97ly5eT3afEQHfr4edDhw4lqVf63+kFiTUfOnRIRYoUSXIRQ3L9cufOnSQMJ7eNkydPatOmTcmG0Bw5cqh27dqqXbu2nn32Wb399tuaMmWK1q1bZ79CXboRLLp166ZcuXLJ09NTfn5+9sP1kZGRkqQzZ87o2rVryV7MlFxdKfHw8LA/j9OmTVOJEiV0+vRphw/1tLwGqlevro0bN+ratWtavXq1cufOrXLlyql06dL2Q/Jr1qxR9erV7Y85c+aMTp48ab8l9/OWpNatWyswMDDFc0NTqlOSihcvnuS16uLionz58iW7rVsPOSe+XlPzOj58+LDCwsKULVs2eXl5yc/PT8HBwZL+9/NLq6NHj+rFF19U1apVNXbsWHv73r17ZYzRwIEDHf4o8/Pz06BBgyTdOO9Z+t/r/Vapeb2k9XfToUOHlCdPniSh9db3VkqPTc37N9HNp7IkGjp0qC5evKiiRYuqVKlS6t27t7Zs2ZLs9owxj/ysC/gfzgnFYy979uyKi4vTpUuXHH5JFy9eXNKNc/1u/pBOic1mS/bD+NYrxXPmzKnw8HAtXbpUP//8s37++WdNmzZNL7/8crIX4dwLZ2fnNLUnV//NihcvrvDwcB07dkx58+a1txctWlRFixaVpCQzDGSkn3/+WR4eHqpZs2aq+teqVUuStGrVKvvIaosWLbRu3Tr17t1bZcqUkZeXlxISElS/fv10/2IDZ2dnhwvk6tWrp+LFi+v111+/q/kTq1WrptjYWK1fv16rV6+2v46rV6+u1atXa+fOnTpz5ozD67tixYoOwWLQoEEaPHhwsrUOGDBAYWFh6XIhibu7e4pXUd/t6zg+Pl516tTR+fPn1bdvXxUvXlyZM2fWsWPHFBYWdlc/v5iYGL3wwgtyd3fXrFmzHC5iS1xfr169UhwBDwwMTPM2k1uHi4uL/eKsB0ly57TWqFFD+/bt04IFC/Trr79qypQpGjdunCZNmqQOHTo49L1w4YJy5MhhVbnIYIyE4rGXGDYPHDjg0P7cc89Jkr755ptUrSdr1qzJXqGd3CiDm5ubnnvuOX322Wfat2+fXn/9dX311Vfau3evJKU4EhAQECDpxvyFN4uJidGBAwfsy++XxAtc0jJnZUBAQJJ6pRtXECcuT/x3z549SYJBcv1OnDiRZIQuuW0sXrxYNWvWTNXFHtKNi2Mk2dd94cIF/fbbb+rXr5+GDBmipk2bqk6dOkmmj/Hz85Onp2eyhxuTqyu1cufOrR49emjhwoX6888/JaXtNVCpUiW5ublp9erVDiG0Ro0a2rBhg3777Tf7/UQzZ860j8YuW7ZML7/8cor1tWnTRoGBgRoyZEiSP2BSqjOx7X6/VqUbf0Du3r1bH374ofr27avGjRurdu3aypMnz12vs2vXrgoPD9fcuXOTjPonvi5cXV3to+y33hL/0E18vd8qNa+XTJky6b///a9WrVqlI0eO3LF/QECAjh8/nmTk9Nb3VkqPTc37906yZcumdu3a6bvvvtORI0f01FNPJfvHzYEDB+yjrHj0EULx2KtcubKkG+ew3dpev359TZkyJdnz3mJiYtSrVy/7/cKFC9tHlhJFRERo7dq1Do+7dWofJycnPfXUU5Kk6OhoSbLPj3hrqK1du7bc3Nw0YcIEhw/9qVOnKjIyMlXTEN2LFi1aKCgoSMOGDbOHolvdGkaeeeYZ/fXXX/ZzEKUb5+Z9/vnnKlCggIKCguz9Tp48qR9++MHeLy4uTh9//LG8vLzsh1CfeeYZxcXFaeLEifZ+8fHx+vjjjx22Gxsbq2XLlqXpOUm8+r906dKS/jfSdus+jR8/3uG+s7Oz6tWrp/nz5+vw4cP29h07dmjp0qWp3n5yunTpokyZMmnUqFGS0vYa8PDwUMWKFfXdd9/p8OHDDiOh165d04QJE1S4cGHlzp3b/piqVas6hKbbzdeYOBoaHh6eZKS2QoUKypkzpyZNmmR/XUs3Rqd37Nhx31+rifVJjj8/Y4x9OrS0mjZtmiZPnqxPP/3U4VzORDlz5lRISIgmT56sEydOJFl+8++GZ555Rn/++afDFyOcOXMm1X/gDRo0SMYYtW3bNtlTJjZu3Gg/svLMM88oPj5en3zyiUOfcePGyWazJZlS7Wapff/ezq2/87y8vBQYGOjwupBunB6xb98+ValS5Y7rxKOBw/F47BUqVEglS5bU8uXL9corrzgs++qrr1S3bl01a9ZMzz33nGrVqqXMmTNrz549+v7773XixAn7XKGvvPKKxo4dq3r16ql9+/Y6ffq0Jk2apCeffNJ+IYF0Y1L08+fP67///a/y5cunQ4cO6eOPP1aZMmXsIwBlypSRs7Oz3n//fUVGRsrd3V3//e9/lTNnTvXv319DhgxR/fr11ahRI+3atUufffaZKlasmKapk+6Gq6ur5s2bZ59Kp1mzZqpevbr9EOdPP/2kw4cPOwSMfv366bvvvlODBg3UtWtXZcuWTTNmzNCBAwc0d+5c+yHY1157TZMnT1ZYWJg2btyoAgUKaM6cOVq7dq3Gjx9vH0F67rnnVLVqVfXr108HDx5UUFCQfvzxxyTn961Zs0ZRUVEphp1jx47ZR7ljYmIUERGhyZMnK0eOHPZD8d7e3qpRo4ZGjx6t2NhY5c2bV7/++muSUXPpxryNv/zyi6pXr65OnTrZA/STTz6Z4vlvqZE9e3a1a9dOn332mXbs2KESJUqk6TVQvXp1jRo1Sj4+PipVqpSkG2GpWLFi2rVr1z1/T3jr1q01bNgwhYeHO7S7urrq/fffV7t27RQcHKxWrVrZp2gqUKCAJVPwFC9eXIULF1avXr107NgxeXt7a+7cuXc89zk5Z8+eVadOnRQUFCR3d/ckR0iaNm2qzJkz69NPP1W1atVUqlQpvfrqqypUqJBOnTql9evX6+jRo4qIiJAk9enTR19//bXq16+vbt262adoCggISNXrpUqVKvr000/VqVMnFS9e3OEbk1asWKGffvpJw4cPl3TjPVOzZk298847OnjwoEqXLq1ff/1VCxYsUPfu3VW4cOEUt5Pa9+/tBAUFKSQkROXLl1e2bNn0zz//aM6cOercubNDv+XLl9unYMNjwurL8YEH0dixY42Xl1ey07lcvXrVfPDBB6ZixYrGy8vLuLm5mSJFipguXbo4TG1ijDHffPONKVSokHFzczNlypQxS5cuTTJF05w5c0zdunVNzpw5jZubm3niiSfM66+/bk6cOOGwri+++MIUKlTIPsXPzdM1ffLJJ6Z48eLG1dXV5MqVy7zxxhtJptgJDg42Tz75ZJL9CQgISHa6FEnmzTffTMWzZczFixfN0KFDTdmyZe3PSf78+c0LL7xgFi5cmKT/vn37zAsvvGB8fX2Nh4eHqVSpklm0aFGSfqdOnTLt2rUzOXLkMG5ubqZUqVLJTht17tw507ZtW+Pt7W18fHxM27ZtzebNmx2maOrVq5cJCgpKtv5bp2hycnIyOXPmNK1atUryMz169Khp2rSp8fX1NT4+PqZ58+bm+PHjRpIZNGiQQ9+VK1ea8uXLGzc3N1OoUCEzadKkZKf+SU7iFE3J2bdvn3F2dnaYOic1rwFjjFm8eLGRZBo0aODQ3qFDByPJTJ069Y61GeM4RdOtpk2bZn8uE6doSvTDDz+YsmXLGnd3d5MtWzbTunVrc/To0VTve3Kvy5RqSZwmaPbs2fa27du3m9q1axsvLy+TI0cO8+qrr9qnOLr5tXWnKZoSt5nS7eYplfbt22defvll4+/vb1xdXU3evHlNw4YNzZw5cxzWv2XLFhMcHGw8PDxM3rx5zbBhw8zUqVPvOEXTzTZu3GheeuklkydPHuPq6mqyZs1qatWqZWbMmOEwhdelS5dMjx497P2KFClixowZY59iK7l9vnl/7vT+Te65TzR8+HBTqVIl4+vrazw9PU3x4sXNiBEjTExMjEO/F1980VSrVi1V+41Hg82YVHxdAvCIi4yMVKFChTR69Gi1b98+o8tBOggKClLDhg01evTojC4FwB2cPHlSBQsW1Pfff89I6GOEc0IB3ZjapU+fPhozZky6X/EM68XExOjFF19Uu3btMroUAKkwfvx4lSpVigD6mGEkFAAAAJZjJBQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsxWf09SkhI0PHjx5UlS5YUv2oRAADgcWGM0aVLl5QnT57bfqEBIfQeHT9+XPnz58/oMgAAAB4oR44cUb58+VJcTgi9R4lfJahquSQXzm4AAACPubgEac2p/2WkFBBC75H9ELyLEyEUAADg/93pNEVSEwAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwjFQyNPdn993XeCzs7dqquL9mrL58tVvuhT9uWZPTLp487DdeTbv3V10V5tm/K7Xm/YJgMrBqzTqVGoDny9XtcW79WfExaqYrEyGV0SYDneBw+XhyaEhoWFyWazyWazydXVVQULFlSfPn10/fr1e173wYMHZbPZFB4efu+F4r7w9fLR2vHzFBsfqwZvt1VQh5rqOXmoLlyKtPcZ23GQ6lcIUZtRXVWifYjG/zhVn3Qerucq18nAyoH7r0Xwcxr7+rsa8s04lXujgSL2b9fSkd/Izzd7RpcGWIb3wcPnoQmhklS/fn2dOHFC+/fv17hx4zR58mQNGjQoo8uCBfq+2ElHzhzXKx/01N+7wnXw5BEt27hK+08csvepElReM5bN1sot63Xo1FF9sWSmIvZtVyX+EsYj7q3nX9MXP3+n6UtnacfhPer4UT9djb6uV+q1zOjSAMvwPnj4PFQh1N3dXf7+/sqfP7+aNGmi2rVra9myZZKkhIQEjRw5UgULFpSnp6dKly6tOXPm2B974cIFtW7dWn5+fvL09FSRIkU0bdo0SVLBggUlSWXLlpXNZlNISIjl+4bba1S5jv7ZvUWzBk7SqVnh2jTxF3Vo8JJDn3XbN6pR5TrKk91fkhRSuoqK5iukXzeuyoiSAUu4uriqfNFSWr5ptb3NGKPlm1arclC5DKwMsA7vg4eTS0YXcLf+/fdfrVu3TgEBAZKkkSNH6ptvvtGkSZNUpEgRrVq1Sm3atJGfn5+Cg4M1cOBAbd++XT///LNy5MihvXv36tq1a5Kkv/76S5UqVdLy5cv15JNPys3NLSN3DckolPsJvfFcW42d+4Xe+/ZjVSxWRhPeHKqYuBh9tezGHxtdPh2oz7u/r2Pf/6PYuFglJCTo1XF9tHrrhgyuHrh/cvhkk4uzi05dOOPQfurCWRXPH5hBVQHW4n3wcHqoQuiiRYvk5eWluLg4RUdHy8nJSZ988omio6P13nvvafny5apcubIkqVChQlqzZo0mT56s4OBgHT58WGXLllWFChUkSQUKFLCv18/PT5KUPXt2+fv737aG6OhoRUdH2+9HRUWl814iOU42J/2ze4ve+fJ9SVL4vm0qWaCYOjZs+78Q2rid/lOinJ4bGKZDp46pxlNP69MuI3T83Cn9tnlNRpYPAABu8VCF0Jo1a2rixIm6cuWKxo0bJxcXFz3//PPatm2brl69qjp1HC9AiYmJUdmyZSVJb7zxhp5//nlt2rRJdevWVZMmTVSlSpU01zBy5EgNGTIkXfYHqXfi/GltP7zHoW3H4T16vvozkiQPNw+990pfNR3cQUv++l2StPXADpUp/KR6Ne9ICMUj62zkecXFxylXVj+H9lxZc+jkhdMZVBVgLd4HD6eH6pzQzJkzKzAwUKVLl9aXX36pDRs2aOrUqbp8+bIkafHixQoPD7fftm/fbj8vtEGDBjp06JB69Oih48ePq1atWurVq1eaa+jfv78iIyPttyNHjqTrPiJ5a7f9o2L5Cjm0Fc1XSIdOHZUkubq4yM3VTQnGOPSJj4+Xk5PNsjoBq8XGxWrj7q2qVbaavc1ms6lW2Wpav31TBlYGWIf3wcPpoRoJvZmTk5PefvttvfXWW9q9e7fc3d11+PBhBQcHp/gYPz8/hYaGKjQ0VNWrV1fv3r31wQcf2M8BjY+Pv+N23d3d5e7unm77gdQZN/cLrftovvq36qxZKxepUrEyeu2Z1nptfF9J0qWrl7UiYr3GvPqOrkVf16HTRxX81H/0cp0X9NYkRq7xaBs793PN6DNO/+yO0F+7wtW9aQdl9vDUtKU/ZHRpgGV4Hzx8HtoQKknNmzdX7969NXnyZPXq1Us9evRQQkKCqlWrpsjISK1du1be3t4KDQ3Vu+++q/Lly+vJJ59UdHS0Fi1apBIlSkiScubMKU9PT/3yyy/Kly+fPDw85OPjk8F7h5v9sztCTQd30Mj2/fVum+46cPKIuk8crG9/n2fv03JEJ41s308z+3+sbFl8dejUUb0z7X1NWvR1BlYO3H+zVi6Un292DQ3tJf+sfgrft131326r0xfPZnRpgGV4Hzx8bMbccvzyARUWFqaLFy9q/vz5Du2jRo3S2LFjdeDAAU2ZMkUTJ07U/v375evrq3Llyuntt99WjRo1NHz4cH377bc6ePCgPD09Vb16dY0bN84+PdOUKVM0dOhQHTt2TNWrV9eKFStSVVdUVNSNwBqSW3J5qM5uAAAASH9xCdKKE4qMjJS3t3eK3R6aEPqgIoQCAADcJJUhlNQEAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5l4wuAMCj4VrOIhldAgDgARAVG6dcOnHHfoyEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByLqnp9NNPP6V6hY0aNbrrYgAAAPB4SFUIbdKkSapWZrPZFB8ffy/1AAAA4DGQqhCakJBwv+sAAADAY+Sezgm9fv16etUBAACAx0iaQ2h8fLyGDRumvHnzysvLS/v375ckDRw4UFOnTk33AgEAAPDoSXMIHTFihKZPn67Ro0fLzc3N3l6yZElNmTIlXYsDAADAoynNIfSrr77S559/rtatW8vZ2dneXrp0ae3cuTNdiwMAAMCjKc0h9NixYwoMDEzSnpCQoNjY2HQpCgAAAI+2NIfQoKAgrV69Okn7nDlzVLZs2XQpCgAAAI+2VE3RdLN3331XoaGhOnbsmBISEvTjjz9q165d+uqrr7Ro0aL7USMAAAAeMWkeCW3cuLEWLlyo5cuXK3PmzHr33Xe1Y8cOLVy4UHXq1LkfNQIAAOARk+aRUEmqXr26li1blt61AAAA4DFxVyFUkv755x/t2LFD0o3zRMuXL59uRQEAAODRluYQevToUbVq1Upr166Vr6+vJOnixYuqUqWKvv/+e+XLly+9awQAAMAjJs3nhHbo0EGxsbHasWOHzp8/r/Pnz2vHjh1KSEhQhw4d7keNAAAAeMSkeSR05cqVWrdunYoVK2ZvK1asmD7++GNVr149XYsDAADAoynNI6H58+dPdlL6+Ph45cmTJ12KAgAAwKMtzSF0zJgx6tKli/755x972z///KNu3brpgw8+SNfiAAAA8GiyGWPMnTplzZpVNpvNfv/KlSuKi4uTi8uNo/mJ/8+cObPOnz9//6p9AEVFRcnHx0cKyS25pDnTA4+MazmLZHQJAIAHQFRsnHLNXqPIyEh5e3un2C9V54SOHz8+veoCAAAAUhdCQ0ND73cdAAAAeIzc9WT1knT9+nXFxMQ4tN1u2BUAAACQ7uLCpCtXrqhz587KmTOnMmfOrKxZszrcAAAAgDtJcwjt06ePfv/9d02cOFHu7u6aMmWKhgwZojx58uirr766HzUCAADgEZPmw/ELFy7UV199pZCQELVr107Vq1dXYGCgAgICNHPmTLVu3fp+1AkAAIBHSJpHQs+fP69ChQpJunH+Z+KUTNWqVdOqVavStzoAAAA8ktI8ElqoUCEdOHBATzzxhIoXL65Zs2apUqVKWrhwoXx9fe9DiUDKOjUKVe/mHeWfzU8R+3aoy6cD9feu8IwuC7hvnBu9JOcK1WXL84QUE62EPdsU9/3nMieO2PvYcuaRy0sd5VSslOTqqoSIvxU7Y4IUdSEDKwfSj3OtRnKu3Ug2P39Jkjl6UHHzvlJCxF9J+rr2GSXn0k8rZuwAJWxca3WpuI00j4S2a9dOERERkqR+/frp008/lYeHh3r06KHevXuna3E2m03z58+XJB08eFA2m03h4eHpug08vFoEP6exr7+rId+MU7k3Gihi/3YtHfmN/HyzZ3RpwH3jVLy04pfPV8ygNxUzqrfk7CK3fqMld48bHdw95NpvtCSjmPfeUsyQLpKLi9x6jZBu+tIR4GFmzp9R3PdfKOad1xUzoKMStm2W61vDZctbwKGfc/0XpDt/Jw8ySJpDaI8ePdS1a1dJUu3atbVz5059++232rx5s7p165amdYWFhclmsyW51a9fP61l4TH01vOv6Yufv9P0pbO04/Aedfyon65GX9cr9VpmdGnAfRM7uq/iVy2VOXZQ5vA+xU4eJVsOf9kKFpUkORUtKZufv2Invy9z5IDMkQOKnTRKtoLF5BRUNoOrB9JHwub1SojYIHPqmMzJo4qbPVW6fk1OgUH2PraAwnJ5toViPx+dgZXidu5pnlBJCggIUEBAwF0/vn79+po2bZpDm7u7+72WhUecq4uryhctpZHff2JvM8Zo+abVqhxULgMrA6xly5T5xn8uR93418VVMpJiY//XKTZGMkZOxUopYdsmy2sE7iubk5yeDpbcPZSwd9uNNjd3ub45QLHTP5IiOQ3lQZWqEDphwoRUrzBxlDS13N3d5e/vn+r+O3fuVKdOnbRp0yYFBgbq008/VXBwsH35ypUr1bt3b0VERChbtmwKDQ3V8OHD5eLiokWLFqlNmzY6d+6cnJ2dFR4errJly6pv374aNWqUJKlDhw66fv26vvnmmzTtB6yVwyebXJxddOrCGYf2UxfOqnj+wAyqCrCYzSaXtp2VsGurzNGDkqSEvdul6Gtyafma4mZNudHnxVdlc3aWOFUFjxBb/oJyG/yp5OomXb+m2HHvyhw7JElyafOmEnZv4xzQB1yqQui4ceNStTKbzZbmEJpWvXv31vjx4xUUFKSxY8fqueee04EDB5Q9e3YdO3ZMzzzzjMLCwvTVV19p586devXVV+Xh4aHBgwerevXqunTpkjZv3qwKFSpo5cqVypEjh1asWGFf/8qVK9W3b98Utx8dHa3o6Gj7/aioqPu5uwCQIpewbnLKV1DRQ7v8r/FSpGInDJFLu+5yr9dMMkYJ639TwoHdkknIuGKBdGaOH1HM2x0kTy85P11Drh37KWZ4d9ly5ZXTk2UV8/arGV0i7iBVIfTAgQP3rYBFixbJy8vLoe3tt9/W22+/nWz/zp076/nnn5ckTZw4Ub/88oumTp2qPn366LPPPlP+/Pn1ySefyGazqXjx4jp+/Lj69u2rd999Vz4+PipTpoxWrFihChUqaMWKFerRo4eGDBmiy5cvKzIyUnv37nUYWb3VyJEjNWTIkPR7AnBXzkaeV1x8nHJl9XNoz5U1h05eOJ1BVQHWcQntKueylRUzrJt0/qzDsoSt/yjmrTaSl7eUEC9dvSL3T+fKnD6RQdUC90F8nMyp45KkuIO7ZStUXM71npdiomXLmUfuXyxy6O7afYjMzq2KGdEjI6pFMu75nNB7VbNmTU2cONGhLVu2bCn2r1y5sv3/Li4uqlChgnbs2CFJ2rFjhypXrizbTVeAVq1aVZcvX9bRo0f1xBNPKDg4WCtWrFDPnj21evVqjRw5UrNmzdKaNWt0/vx55cmTR0WKFElx+/3799dbb71lvx8VFaX8+fOneb9xb2LjYrVx91bVKltNC9YtlXRjJL5W2Wr6ZMH0jC0OuM9cQrvKuUI1xQzvIXPmZMod//88UaegspK3rxI2rbOoQiAD2Gyyuboqdu40xa9Y7LDI/f1pivvmM94DD5gMD6GZM2dWYKB15/CFhIToyy+/VEREhFxdXVW8eHGFhIRoxYoVunDhwm1HQaUb57By4dSDYezczzWjzzj9sztCf+0KV/emHZTZw1PTlv6Q0aUB941LWHc5V6mlmLEDZK5flXyy3lhw9cqNC5AkOdeor4Tjh6SoSNmKBMm1bWfF/zLHYS5R4GHm8mIHxUf8JZ09JXlmknOVWnIqUUax7/eRIi/IJHMxkjl76vZ/tMFyGR5C0+rPP/9UjRo1JElxcXHauHGjOnfuLEkqUaKE5s6dK2OMfTR07dq1ypIli/LlyydJ9vNCx40bZw+cISEhGjVqlC5cuKCePXtmwF7hbsxauVB+vtk1NLSX/LP6KXzfdtV/u61OXzx75wcDDymXOo0lSe4Dxzu0x04epfhV/39UIHd+ub34quSVRebMScUtmKn4n2dbXSpw/3hnlVvH/pJvNunqFSUc2a/Y9/so4d+NGV0Z0sBmTMbN4hoWFqZTp04lmaLJxcVFOXLkkM1m07x589SkSRMdPHhQBQsW1BNPPKHx48erRIkSGjdunL799lsdOHBAOXLk0LFjx1S0aFG1a9dOnTt31q5du9ShQwe9+eabGjx4sH39ZcuW1datW/XJJ5+oY8eOOn/+vPz9/RUbG6udO3eqWLFiqd6HqKgo+fj4SCG5JZc0T7sKPDKu5Uz5NBYAwOMjKjZOuWavUWRkpLy9vVPsl+Ejob/88oty587t0FasWDHt3Lkz2f6jRo3SqFGjFB4ersDAQP3000/KkSOHJClv3rxasmSJevfurdKlSytbtmxq3769BgwY4LCO4OBghYeHKyQkRNKNc1CDgoJ06tSpNAVQAAAA3J27GgldvXq1Jk+erH379mnOnDnKmzevvv76axUsWFDVqlW7H3U+sBgJBW5gJBQAIKV+JDTNqWnu3LmqV6+ePD09tXnzZvucmZGRkXrvvffuvmIAAAA8NtIcQocPH65Jkybpiy++kKurq729atWq2rSJr4MDAADAnaU5hO7atct+dfrNfHx8dPHixfSoCQAAAI+4NIdQf39/7d27N0n7mjVrVKhQoXQpCgAAAI+2NIfQV199Vd26ddOGDRtks9l0/PhxzZw5U7169dIbb7xxP2oEAADAIybNUzT169dPCQkJqlWrlq5evaoaNWrI3d1dvXr1UpcuXe5HjQAAAHjE3PVk9TExMdq7d68uX76soKAgeXl5pXdtDwWmaAJuYIomAIBkwWT1bm5uCgoKutuHAwAA4DGW5hBas2ZN+/eyJ+f333+/p4IAAADw6EtzCC1TpozD/djYWIWHh+vff/9VaGhoetUFAACAR1iaQ+i4ceOSbR88eLAuX758zwUBAADg0ZduV9K0adNGX375ZXqtDgAAAI+wdAuh69evl4eHR3qtDgAAAI+wNB+Ob9asmcN9Y4xOnDihf/75RwMHDky3wgAAAPDoSnMI9fHxcbjv5OSkYsWKaejQoapbt266FQYAAIBHV5pCaHx8vNq1a6dSpUopa9as96smAAAAPOLSdE6os7Oz6tatq4sXL96ncgAAAPA4SPOFSSVLltT+/fvvRy0AAAB4TKQ5hA4fPly9evXSokWLdOLECUVFRTncAAAAgDtJ9TmhQ4cOVc+ePfXMM89Ikho1auTw9Z3GGNlsNsXHx6d/lQAAAHikpDqEDhkyRB07dtQff/xxP+sBAADAYyDVIdQYI0kKDg6+b8UAAADg8ZCmc0JvPvwOAAAA3K00zRNatGjROwbR8+fP31NBAAAAePSlKYQOGTIkyTcmAQAAAGmVphDasmVL5cyZ837VAgAAgMdEqs8J5XxQAAAApJdUh9DEq+MBAACAe5Xqw/EJCQn3sw4AAAA8RtL8tZ0AAADAvSKEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5l4wuAMCjwSVPjowuAXggvNkgb0aXAGSomCvR0uw1d+zHSCgAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALCcS0YX8KCx2WyaN2+emjRpktGlIBU6NQpV7+Yd5Z/NTxH7dqjLpwP1967wjC4LuG/e37BT83Yf067zl+Tp4qzKebPrvRqlVCxbFnuffRcvq++KLVp77Kyi4xNUr4C/xtcqo1yZPTKwciD91MhbVcF5qyq7RzZJ0okrJ7XowFJtO79DktS6WAuVyFZUPm7eio6P0b7IA/px30Kduno6I8vGLR64kdCwsLBkA+CKFStks9l08eJFy2vCg6lF8HMa+/q7GvLNOJV7o4Ei9m/X0pHfyM83e0aXBtw3q46c0RtlC2tN65r6uXl1xcYn6JnZq3UlJk6SdCUmTs/MXi2bTfq1RbBWtqqpmIQENZm3VgnGZHD1QPq4eP2i5u1bqPf+/kDv/f2hdl7YrU5PtVfuzP6SpMOXjmjGjm81eMMofRQ+STbZ1L3MG7LJlsGV42YPXAgFUuut51/TFz9/p+lLZ2nH4T3q+FE/XY2+rlfqtczo0oD7ZvEL1RVasoCezOGj0jl9NbVBRR2+dFWbTl2QJK07flYHo65oav2KKuXno1J+PvqyQUVtPHlBfxxmFAiPhi3ntunfczt0+tpZnb52Rgv2L1F0fLQKeQdIklYfX689F/fr3PXzOnL5qBbsX6xsHlmV3TNbBleOmz2UIfTcuXNq1aqV8ubNq0yZMqlUqVL67rvvHPqEhISoa9eu6tOnj7JlyyZ/f38NHjzYoc+ePXtUo0YNeXh4KCgoSMuWLbNwL3AvXF1cVb5oKS3ftNreZozR8k2rVTmoXAZWBlgrMjpWkpTVw02SFB2fIJtscnf+3693D2cnOdlsWnv0bIbUCNxPNtlUIWdZuTm7a3/kwSTL3ZzcVCX30zpz7awuXL9oeX1I2UN5Tuj169dVvnx59e3bV97e3lq8eLHatm2rwoULq1KlSvZ+M2bM0FtvvaUNGzZo/fr1CgsLU9WqVVWnTh0lJCSoWbNmypUrlzZs2KDIyEh1794943YKaZLDJ5tcnF106sIZh/ZTF86qeP7ADKoKsFaCMer5R7iq5M2ukn4+kqSnc2dXZldn9V+1VcOrl5Qx0turtyreGJ24cj2DKwbST57MudW3fHe5OrkoOj5Gk7ZO1Ymrp+zLg/NWVbPCjeTh4q6TV05pfPhExZv4DKwYt3ogQ+iiRYvk5eXl0BYf/78XTt68edWrVy/7/S5dumjp0qWaNWuWQwh96qmnNGjQIElSkSJF9Mknn+i3335TnTp1tHz5cu3cuVNLly5Vnjx5JEnvvfeeGjRocNvaoqOjFR0dbb8fFRV19zsKAPegy/LN2nY2Sitahdjb/DK56/tG/1HnZZv1yaa9crLZ9GKJ/Cqby1dONs6Hw6Pj1NXTGv73GHm6eKicXxmFlWitDzd9bA+iG05u1I7zu+Tj7q06+f+r154M0+hNHykuIS6DK0eiBzKE1qxZUxMnTnRo27Bhg9q0aSPpRiB97733NGvWLB07dkwxMTGKjo5WpkyZHB7z1FNPOdzPnTu3Tp++cU7Ujh07lD9/fnsAlaTKlSvfsbaRI0dqyJAhd7VfSD9nI88rLj5OubL6ObTnyppDJy9w3hsefV2Xb9aS/Sf0+4shypfF8XdfnQL+2vVqA529Gi0XJ5t8PdyU77OFKlQscwZVC6S/eBOvM9dunGJy+NJRFfDOr//mD9bMXbMkSdfjr+v6tes6fe2s9kce0rga76ms31P6+9SmjCwbN3kgzwnNnDmzAgMDHW558+a1Lx8zZow++ugj9e3bV3/88YfCw8NVr149xcTEOKzH1dXV4b7NZlNCQsI91da/f39FRkbab0eOHLmn9eHuxMbFauPurapVtpq9zWazqVbZalq/nV8weHQZY9R1+WYt2HtMv7aooYK+KQfLHJnc5evhpj8On9bpq9FqGJgnxb7Aw85ms8nFKfmxNZtunDvqYnsgx94eWw/lT2Pt2rVq3LixfWQ0ISFBu3fvVlBQUKrXUaJECR05ckQnTpxQ7ty5JUl//vnnHR/n7u4ud3f3uysc6Wrs3M81o884/bM7Qn/tClf3ph2U2cNT05b+kNGlAfdNl+Wb9f3OI/qxSRVlcXPVyf8/z9PHzVWers6SpOlbD6p49izyy+SuP4+f01u/R6hb+SIOc4kCD7MmhRpq2/ntOn/9otyd3VUpV3kV9Q3UhPBJyuGRXRVyldX28zt1Keaysrr7qn5AbcUkxOrfc9szunTc5KEMoUWKFNGcOXO0bt06Zc2aVWPHjtWpU6fSFEJr166tokWLKjQ0VGPGjFFUVJTeeeed+1g10tuslQvl55tdQ0N7yT+rn8L3bVf9t9vq9EWuAMaja3LEfklSrR9WOrRPqV9BoSULSJJ2X7ikAau36vz1GBXwyax+/ymu7uWLWF0qcN9kcfNSWIk28nH31rW4azp2+bgmhE/Sjgu75ePmrUCfQqqVP1iZXDwVFXNJey7u0+iNH+lS7OWMLh03eShD6IABA7R//37Vq1dPmTJl0muvvaYmTZooMjIy1etwcnLSvHnz1L59e1WqVEkFChTQhAkTVL9+/ftYOdLbpwum69MF0zO6DMAysb1euGOf92qU0ns1SllQDZAxvt75fYrLImOi9MmWzy2sBnfLZgxfoXEvoqKi5OPjI4XkllweyFNsAUvElrnzhX3A4+DNBnnv3Al4hMVcidb0RpMUGRkpb2/vFPuRmgAAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALEcIBQAAgOUIoQAAALAcIRQAAACWI4QCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAAADLEUIBAABgOUIoAAAALOeS0QU87IwxN/4Tl5CxhQAZLCo6NqNLAB4IMVeiM7oEIEPFXI2RdFNGSoHN3KkHbuvo0aPKnz9/RpcBAADwQDly5Ijy5cuX4nJC6D1KSEjQ8ePHlSVLFtlstowu57EUFRWl/Pnz68iRI/L29s7ocoAMwfsAuIH3QsYzxujSpUvKkyePnJxSPvOTw/H3yMnJ6bYpH9bx9vbmFw4ee7wPgBt4L2QsHx+fO/bhwiQAAABYjhAKAAAAyxFC8dBzd3fXoEGD5O7untGlABmG9wFwA++FhwcXJgEAAMByjIQCAADAcoRQAAAAWI4QCgAPMJvNpvnz50uSDh48KJvNpvDw8AytCXiU3fyew/1FCMUDJSwsTDabTTabTa6uripYsKD69Omj69ev3/O6+QDHg+jm1/zNt/r162d0aUCGCAsLU5MmTZK0r1ixQjabTRcvXrS8JtwfTFaPB079+vU1bdo0xcbGauPGjQoNDZXNZtP777+f0aUB90Xia/5mXNkL4FHHSCgeOO7u7vL391f+/PnVpEkT1a5dW8uWLZN042tSR44cqYIFC8rT01OlS5fWnDlz7I+9cOGCWrduLT8/P3l6eqpIkSL2D/eCBQtKksqWLSubzaaQkBDL9w1ITuJr/uZb1qxZU+y/c+dOValSRR4eHipZsqRWrlzpsHzlypWqVKmS3N3dlTt3bvXr109xcXGSpEWLFsnX11fx8fGSpPDwcNlsNvXr18/++A4dOqhNmzb3YU+B9HHu3Dm1atVKefPmVaZMmVSqVCl99913Dn1CQkLUtWtX9enTR9myZZO/v78GDx7s0GfPnj2qUaOGPDw8FBQUZP+sgTUIoXig/fvvv1q3bp3c3NwkSSNHjtRXX32lSZMmadu2berRo4fatGlj/xAeOHCgtm/frp9//lk7duzQxIkTlSNHDknSX3/9JUlavny5Tpw4oR9//DFjdgq4R71791bPnj21efNmVa5cWc8995zOnTsnSTp27JieeeYZVaxYUREREZo4caKmTp2q4cOHS5KqV6+uS5cuafPmzZJuBNYcOXJoxYoV9vWvXLmSP9LwQLt+/brKly+vxYsX699//9Vrr72mtm3b2n/PJ5oxY4YyZ86sDRs2aPTo0Ro6dKjDoEazZs3k5uamDRs2aNKkSerbt29G7M7jywAPkNDQUOPs7GwyZ85s3N3djSTj5ORk5syZY65fv24yZcpk1q1b5/CY9u3bm1atWhljjHnuuedMu3btkl33gQMHjCSzefPm+70bQKrd/Jq/+TZixAhjjDGSzLx584wx/3sNjxo1yv742NhYky9fPvP+++8bY4x5++23TbFixUxCQoK9z6effmq8vLxMfHy8McaYcuXKmTFjxhhjjGnSpIkZMWKEcXNzM5cuXTJHjx41kszu3but2H0giZTeEx4eHkaSuXDhQrKPe/bZZ03Pnj3t94ODg021atUc+lSsWNH07dvXGGPM0qVLjYuLizl27Jh9+c8//+zwnsP9xTmheODUrFlTEydO1JUrVzRu3Di5uLjo+eef17Zt23T16lXVqVPHoX9MTIzKli0rSXrjjTf0/PPPa9OmTapbt66aNGmiKlWqZMRuAKmW+Jq/WbZs2VLsX7lyZfv/XVxcVKFCBe3YsUOStGPHDlWuXFk2m83ep2rVqrp8+bKOHj2qJ554QsHBwVqxYoV69uyp1atXa+TIkZo1a5bWrFmj8+fPK0+ePCpSpEg67yWQesm9JzZs2GA/TSQ+Pl7vvfeeZs2apWPHjikmJkbR0dHKlCmTw2Oeeuoph/u5c+fW6dOnJd14r+TPn1958uSxL7/5vYX7jxCKB07mzJkVGBgoSfryyy9VunRpTZ06VSVLlpQkLV68WHnz5nV4TOJFHA0aNNChQ4e0ZMkSLVu2TLVq1dKbb76pDz74wNqdANLg5te8FUJCQvTll18qIiJCrq6uKl68uEJCQrRixQpduHBBwcHBltUCJCe598TRo0ft/x8zZow++ugjjR8/XqVKlVLmzJnVvXt3xcTEODzG1dXV4b7NZlNCQsL9KxxpwjmheKA5OTnp7bff1oABAxQUFCR3d3cdPnxYgYGBDrf8+fPbH+Pn56fQ0FB98803Gj9+vD7//HNJsp9XmnhBBvCw+vPPP+3/j4uL08aNG1WiRAlJUokSJbR+/XqZm76Ree3atcqSJYvy5csn6X/nhY4bN84eOBND6IoVKzgfFA+8tWvXqnHjxmrTpo1Kly6tQoUKaffu3WlaR4kSJXTkyBGdOHHC3nbzewv3HyEUD7zmzZvL2dlZkydPVq9evdSjRw/NmDFD+/bt06ZNm/Txxx9rxowZkqR3331XCxYs0N69e7Vt2zYtWrTI/uGcM2dOeXp66pdfftGpU6cUGRmZkbsF2EVHR+vkyZMOt7Nnz6bY/9NPP9W8efO0c+dOvfnmm7pw4YJeeeUVSVKnTp105MgRdenSRTt37tSCBQs0aNAgvfXWW3JyuvErP2vWrHrqqac0c+ZMe+CsUaOGNm3apN27dzMSigdekSJFtGzZMq1bt047duzQ66+/rlOnTqVpHbVr11bRokUVGhqqiIgIrV69Wu+88859qhjJIYTigefi4qLOnTtr9OjR6t+/vwYOHKiRI0eqRIkSql+/vhYvXmyffsnNzU39+/fXU089pRo1asjZ2Vnff/+9fT0TJkzQ5MmTlSdPHjVu3Dgjdwuw++WXX5Q7d26HW7Vq1VLsP2rUKI0aNUqlS5fWmjVr9NNPP9lngcibN6+WLFmiv/76S6VLl1bHjh3Vvn17DRgwwGEdwcHBio+Pt4fQbNmyKSgoSP7+/ipWrNh921cgPQwYMEDlypVTvXr1FBISIn9//2QnuL8dJycnzZs3T9euXVOlSpXUoUMHjRgx4v4UjGTZzM3HbAAAAAALMBIKAAAAyxFCAQAAYDlCKAAAACxHCAUAAIDlCKEAAACwHCEUAAAAliOEAgAAwHKEUAAAAFiOEAoAFgoLC3P4ZpeQkBB1797d8jpWrFghm82mixcvptjHZrNp/vz5qV7n4MGDVaZMmXuq6+DBg7LZbAoPD7+n9QB48BFCATz2wsLCZLPZZLPZ5ObmpsDAQA0dOlRxcXH3fds//vijhg0blqq+qQmOAPCwcMnoAgDgQVC/fn1NmzZN0dHRWrJkid588025urqqf//+SfrGxMTIzc0tXbabLVu2dFkPADxsGAkFAEnu7u7y9/dXQECA3njjDdWuXVs//fSTpP8dQh8xYoTy5MmjYsWKSZKOHDmiFi1ayNfXV9myZVPjxo118OBB+zrj4+P11ltvydfXV9mzZ1efPn1kjHHY7q2H46Ojo9W3b1/lz59f7u7uCgwM1NSpU3Xw4EHVrFlTkpQ1a1bZbDaFhYVJkhISEjRy5EgVLFhQnp6eKl26tObMmeOwnSVLlqho0aLy9PRUzZo1HepMrb59+6po0aLKlCmTChUqpIEDByo2NjZJv8mTJyt//vzKlCmTWrRoocjISIflU6ZMUYkSJeTh4aHixYvrs88+S3MtAB5+hFAASIanp6diYmLs93/77Tft2rVLy5Yt06JFixQbG6t69eopS5YsWr16tdauXSsvLy/Vr1/f/rgPP/xQ06dP15dffqk1a9bo/Pnzmjdv3m23+/LLL+u7777ThAkTtGPHDk2ePFleXl7Knz+/5s6dK0natWuXTpw4oY8++kiSNHLkSH311VeaNGmStm3bph49eqhNmzZauXKlpBthuVmzZnruuecUHh6uDh06qF+/fml+TrJkyaLp06dr+/bt+uijj/TFF19o3LhxDn327t2rWbNmaeHChfrll1+0efNmderUyb585syZevfddzVixAjt2LFD7733ngYOHKgZM2akuR4ADzkDAI+50NBQ07hxY2OMMQkJCWbZsmXG3d3d9OrVy748V65cJjo62v6Yr7/+2hQrVswkJCTY26Kjo42np6dZunSpMcaY3Llzm9GjR9uXx8bGmnz58tm3ZYwxwcHBplu3bsYYY3bt2mUkmWXLliVb5x9//GEkmQsXLtjbrl+/bjJlymTWrVvn0Ld9+/amVatWxhhj+vfvb4KCghyW9+3bN8m6biXJzJs3L8XlY8aMMeXLl7ffHzRokHF2djZHjx61t/3888/GycnJnDhxwhhjTOHChc23337rsJ5hw4aZypUrG2OMOXDggJFkNm/enOJ2ATwaOCcUACQtWrRIXl5eio2NVUJCgl566SUNHjzYvrxUqVIO54FGRERo7969ypIli8N6rl+/rn379ikyMlInTpzQ008/bV/m4uKiChUqJDkknyg8PFzOzs4KDg5Odd179+7V1atXVadOHYf2mJgYlS1bVpK0Y8cOhzokqXLlyqneRqIffvhBEyZM0L59+3T58mXFxcXJ29vboc8TTzyhvHnzOmwnISFBu3btUpYsWbRv3z61b99er776qr1PXFycfHx80lwPgIcbIRQAJNWsWVMTJ06Um5ub8uTJIxcXx1+PmTNndrh/+fJllS9fXjNnzkyyLj8/v7uqwdPTM82PuXz5siRp8eLFDuFPunGea3pZv369WrdurSFDhqhevXry8fHR999/rw8//DDNtX7xxRdJQrGzs3O61Qrg4UAIBQDdCJmBgYGp7l+uXDn98MMPypkzZ5LRwES5c+fWhg0bVKNGDUk3Rvw2btyocuXKJdu/VKlSSkhI0MqVK1W7du0kyxNHYuPj4+1tQUFBcnd31+HDh1McQS1RooT9IqtEf/7555138ibr1q1TQECA3nnnHXvboUOHkvQ7fPiwjh8/rjx58ti34+TkpGLFiilXrlzKkyeP9u/fr9atW6dp+wAePVyYBAB3oXXr1sqRI4caN26s1atX68CBA1qxYoW6du2qo0ePSpK6deumUaNGaf78+dq5c6c6dep02zk+CxQooNDQUL3yyiuaP3++fZ2zZs2SJAUEBMhms2nRokU6c+aMLl++rCxZsqhXr17q0aOHZsyYoX379mnTpk36+OOP7Rf7dOzYUXv27FHv3r21a9cuffvtt5o+fXqa9rdIkSI6fPiwvv/+e+3bt08TJkxI9iIrDw8PhYaGKiIiQqtXr1bXrl3VokUL+fv7S5KGDBmikSNHasKECdq9e7e2bt2qadOmaezYsWmqB8DDjxAKAHchU6ZMWrVqlZ544gk1a9ZMJUqUUPv27XX9+nX7yGjPnj3Vtm1bhYaGqnLlysqSJYuaNm162/VOnDhRL7zwgjp16qTixYvr1Vdf1ZUrVyRJefPm1ZAhQ9SvXz/lypVLnTt3liQNGzZMAwcO1MiRI1WiRAnVr19fixcvVsGCBSXdOE9z7ty5mj9/vkqXLq1JkybpvffeS9P+NmrUSD169FDnzp1VpkwZrVu3TgMHDkzSLzAwUM2aNdMzzzyjunXr6qmnnnKYgqlDhw6aMmWKpk2bplKlSik4OFjTp0+31wrg8WEzKZ0hDwAAANwnjIQCAADAcoRQAAAAWI4QCgAAAMsRQgEAAGA5QigAAAAsRwgFAACA5QihAAAAsBwhFAAAAJYjhAIAAMByhFAAAABYjhAKAAAAyxFCAQAAYLn/A54ImZwtUC2DAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 4: Confusion Matrix for LENet_FCL SNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 67.36%.\n",
            "Accuracy for Rest: 100.00%.\n",
            "Accuracy for Elbow: 46.03%.\n",
            "Accuracy for Hand: 53.23%.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "--- Model Performance Summary Table ---\n",
            "| Model             | Overall Acc.    | Rest Acc.    | Elbow Acc.   | Hand Acc.    |\n",
            "|-------------------|-----------------|--------------|--------------|--------------|\n",
            "| LENet CNN         | 69.95%          | 100.00%      | 47.62%       | 59.68%       |\n",
            "| LENet SNN         | 69.95%          | 100.00%      | 46.03%       | 61.29%       |\n",
            "| LENet_FCL CNN     | 68.91%          | 100.00%      | 47.62%       | 56.45%       |\n",
            "| LENet_FCL SNN     | 67.36%          | 100.00%      | 46.03%       | 53.23%       |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Energy consumption estimation and comparison\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Note: Ensure that the LENet, LENet_FCL classes, and initialize_weights function\n",
        "# are defined in a previous cell. Also, 'device', 'channel_count',\n",
        "# and 'data_length' should be available from your data loading and setup cells.\n",
        "# If cnn_model_lenet_ccb and cnn_model_lenet_fcl (trained models) exist,\n",
        "# they will be used; otherwise, new instances are created for architecture analysis.\n",
        "\n",
        "# --- Energy Calculation Constants (original in pJ) ---\n",
        "E_MAC_PJ = 4.6  # pJ per MAC operation\n",
        "E_AC_PJ = 0.9   # pJ per AC operation (for SNN spike processing)\n",
        "\n",
        "# Conversion factor from pJ to µJ\n",
        "PJ_TO_UJ = 1e-6\n",
        "\n",
        "def get_macs_and_energy_final(\n",
        "    model, sample_input, model_name=\"Model\", T_snn_param=100, avg_firing_rate=0.02, conversion_factor=1.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates MACs and estimates energy consumption for ANN and SNN versions.\n",
        "    Returns energy in units determined by conversion_factor.\n",
        "    Formats energy values as floats with two decimal places.\n",
        "    Formats Total MACs as integers with commas.\n",
        "    \"\"\"\n",
        "    layer_flops_mac_list = []\n",
        "    hooks = []\n",
        "\n",
        "    def hook_fn_flops(module, input_tensor_tuple, output_tensor):\n",
        "        macs = 0\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            c_in = module.in_channels\n",
        "            c_out = module.out_channels\n",
        "            k_h, k_w = module.kernel_size\n",
        "            h_out, w_out = output_tensor.shape[2], output_tensor.shape[3]\n",
        "            groups = module.groups\n",
        "            macs = (c_in // groups) * k_h * k_w * c_out * h_out * w_out\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            n_in = module.in_features\n",
        "            n_out = module.out_features\n",
        "            macs = n_in * n_out\n",
        "        if macs > 0:\n",
        "            layer_flops_mac_list.append(macs)\n",
        "\n",
        "    for _, module_item in model.named_modules():\n",
        "        if isinstance(module_item, (nn.Conv2d, nn.Linear)):\n",
        "            hooks.append(module_item.register_forward_hook(hook_fn_flops))\n",
        "\n",
        "    model.eval()\n",
        "    model_device = next(model.parameters()).device\n",
        "    with torch.no_grad():\n",
        "        model(sample_input.to(model_device))\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    total_macs_all_layers = sum(layer_flops_mac_list) # Integer\n",
        "    energy_ann_pj = E_MAC_PJ * float(total_macs_all_layers) # Use float for energy calc\n",
        "\n",
        "    if not layer_flops_mac_list:\n",
        "        energy_snn_pj = 0.0\n",
        "    else:\n",
        "        macs_layer1_for_snn = float(layer_flops_mac_list[0])\n",
        "        energy_snn_term1_pj = E_MAC_PJ * macs_layer1_for_snn\n",
        "        sops_sum_layers_2_L = 0.0\n",
        "        for i in range(1, len(layer_flops_mac_list)):\n",
        "            macs_layer_i = float(layer_flops_mac_list[i])\n",
        "            sops_layer_i = avg_firing_rate * T_snn_param * macs_layer_i\n",
        "            sops_sum_layers_2_L += sops_layer_i\n",
        "        energy_snn_term2_pj = E_AC_PJ * sops_sum_layers_2_L\n",
        "        energy_snn_pj = energy_snn_term1_pj + energy_snn_term2_pj\n",
        "\n",
        "    energy_ann_converted = energy_ann_pj * conversion_factor\n",
        "    energy_snn_converted = energy_snn_pj * conversion_factor\n",
        "\n",
        "    output_unit_str = \"µJ\" if conversion_factor == PJ_TO_UJ else \"pJ\"\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Total MACs\": f\"{total_macs_all_layers:,}\", # Integer with commas\n",
        "        f\"ANN Energy ({output_unit_str})\": f\"{energy_ann_converted:,.2f}\",\n",
        "        f\"SNN Energy ({output_unit_str})\": f\"{energy_snn_converted:,.2f}\",\n",
        "    }\n",
        "\n",
        "# --- Prepare models and input for energy calculation ---\n",
        "if 'device' not in locals():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # print(f\"Warning: 'device' not found, defaulting to {device}\") # Silenced for cleaner output\n",
        "if 'channel_count' not in locals():\n",
        "    channel_count = 60\n",
        "    # print(f\"Warning: 'channel_count' not found, defaulting to {channel_count}\")\n",
        "if 'data_length' not in locals():\n",
        "    data_length = 1000\n",
        "    # print(f\"Warning: 'data_length' not found, defaulting to {data_length}\")\n",
        "if 'TIME_STEPS' not in locals():\n",
        "    TIME_STEPS = 100\n",
        "    # print(f\"Warning: 'TIME_STEPS' not found, defaulting to {TIME_STEPS}\")\n",
        "if 'DROP_OUT' not in locals():\n",
        "    DROP_OUT = 0.25\n",
        "    # print(f\"Warning: 'DROP_OUT' not found, defaulting to {DROP_OUT}\")\n",
        "\n",
        "sample_input_tensor = torch.randn(1, 1, channel_count, data_length)\n",
        "\n",
        "# Model 1: LENet (CCB)\n",
        "if 'cnn_model_lenet_ccb' in locals() and isinstance(cnn_model_lenet_ccb, nn.Module):\n",
        "    # print(\"Using existing 'cnn_model_lenet_ccb' for LENet_(CCB) energy calculation.\") # Silenced\n",
        "    model_for_energy_lenet_ccb = cnn_model_lenet_ccb\n",
        "else:\n",
        "    print(\"Warning: 'cnn_model_lenet_ccb' not found. Initializing new LENet for energy calculation.\")\n",
        "    try:\n",
        "        model_for_energy_lenet_ccb = LENet(classes_num=3, channel_count=channel_count, drop_out=DROP_OUT).to(device)\n",
        "        if 'initialize_weights' in locals(): model_for_energy_lenet_ccb.apply(initialize_weights)\n",
        "    except NameError:\n",
        "        print(\"Error: LENet class definition not found.\")\n",
        "        model_for_energy_lenet_ccb = None\n",
        "\n",
        "# Model 2: LENet_FCL\n",
        "if 'cnn_model_lenet_fcl' in locals() and isinstance(cnn_model_lenet_fcl, nn.Module):\n",
        "    # print(\"Using existing 'cnn_model_lenet_fcl' for LENet_FCL energy calculation.\") # Silenced\n",
        "    model_for_energy_lenet_fcl = cnn_model_lenet_fcl\n",
        "    model_for_energy_lenet_fcl.eval()\n",
        "    with torch.no_grad():\n",
        "        model_for_energy_lenet_fcl(sample_input_tensor.to(device))\n",
        "else:\n",
        "    print(\"Warning: 'cnn_model_lenet_fcl' not found. Initializing new LENet_FCL for energy calculation.\")\n",
        "    try:\n",
        "        model_for_energy_lenet_fcl = LENet_FCL(classes_num=3, channel_count=channel_count, drop_out=DROP_OUT).to(device)\n",
        "        if 'initialize_weights' in locals(): model_for_energy_lenet_fcl.apply(initialize_weights)\n",
        "        model_for_energy_lenet_fcl.eval()\n",
        "        with torch.no_grad():\n",
        "            model_for_energy_lenet_fcl(sample_input_tensor.to(device))\n",
        "    except NameError:\n",
        "        print(\"Error: LENet_FCL class definition not found.\")\n",
        "        model_for_energy_lenet_fcl = None\n",
        "\n",
        "# --- Perform Energy Calculations ---\n",
        "AVG_FIRING_RATE = 0.02 # 2%\n",
        "energy_results_list_final = []\n",
        "\n",
        "CURRENT_CONVERSION_FACTOR = PJ_TO_UJ\n",
        "OUTPUT_UNIT_NAME = \"µJ\"\n",
        "\n",
        "if model_for_energy_lenet_ccb:\n",
        "    results_lenet_final = get_macs_and_energy_final(\n",
        "        model_for_energy_lenet_ccb,\n",
        "        sample_input_tensor,\n",
        "        model_name=\"LENet CCB\",\n",
        "        T_snn_param=TIME_STEPS,\n",
        "        avg_firing_rate=AVG_FIRING_RATE,\n",
        "        conversion_factor=CURRENT_CONVERSION_FACTOR\n",
        "    )\n",
        "    energy_results_list_final.append(results_lenet_final)\n",
        "# else: # Silence for cleaner output\n",
        "    # print(\"Skipping LENet_CCB energy calculation as model was not available.\")\n",
        "\n",
        "if model_for_energy_lenet_fcl:\n",
        "    results_lenet_fcl_final = get_macs_and_energy_final(\n",
        "        model_for_energy_lenet_fcl,\n",
        "        sample_input_tensor,\n",
        "        model_name=\"LENet FCL\",\n",
        "        T_snn_param=TIME_STEPS,\n",
        "        avg_firing_rate=AVG_FIRING_RATE,\n",
        "        conversion_factor=CURRENT_CONVERSION_FACTOR\n",
        "    )\n",
        "    energy_results_list_final.append(results_lenet_fcl_final)\n",
        "# else: # Silenced for cleaner output\n",
        "    # print(\"Skipping LENet FCL energy calculation as model was not available.\")\n",
        "\n",
        "# --- Generate Markdown Table ---\n",
        "if energy_results_list_final:\n",
        "    # Table title is now part of the initial print statements from your notebook\n",
        "    # markdown_table_final = f\"Table 2. Estimated Theoretical Energy Consumption per Inference.\\n\"\n",
        "    markdown_table_final = \"\" # Start with an empty string for the table itself\n",
        "\n",
        "    # Adjusted column widths and alignment to match the image\n",
        "    # | Model         | Total MACs   | ANN Energy (µJ) | SNN Energy (µJ) |\n",
        "    # |:--------------|:-------------|:----------------|:----------------|\n",
        "    # Widths: Model: 13, Total MACs: 12, ANN Energy: 15, SNN Energy: 15 (approx from image)\n",
        "\n",
        "    markdown_table_final += f\"| Model       | Total MACs   | ANN Energy ({OUTPUT_UNIT_NAME}) | SNN Energy ({OUTPUT_UNIT_NAME}) |\\n\"\n",
        "    markdown_table_final += f\"|:------------|:-------------|:----------------|:----------------|\\n\"\n",
        "\n",
        "    for result in energy_results_list_final:\n",
        "        markdown_table_final += (\n",
        "            f\"| {result['Model']:<11} | \"  # Adjusted width\n",
        "            f\"{result['Total MACs']:>12} | \"\n",
        "            f\"{result[f'ANN Energy ({OUTPUT_UNIT_NAME})']:>15} | \"\n",
        "            f\"{result[f'SNN Energy ({OUTPUT_UNIT_NAME})']:>15} |\\n\"\n",
        "        )\n",
        "\n",
        "    # The newline after the table is handled by the print statement for constants\n",
        "    # markdown_table_final += \"\\n\"\n",
        "\n",
        "    constants_line = (\n",
        "        f\"Constants used: E_MAC = ({E_MAC_PJ:.1f} pJ), \"\n",
        "        f\"E_AC = ({E_AC_PJ:.1f} pJ). \"\n",
        "        f\"SNN parameters: T = {TIME_STEPS}, \"\n",
        "        f\"Assumed Average Firing Rate (fr) = {AVG_FIRING_RATE*100:.0f}%.\"\n",
        "    )\n",
        "    # Print the table first, then the constants on a new line\n",
        "    print(markdown_table_final)\n",
        "    print(constants_line)\n",
        "else:\n",
        "    print(\"No energy consumption results to display. Please check model availability and definitions.\")"
      ],
      "metadata": {
        "id": "JXy2ibddkwob",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9db57f-6b28-48b0-a439-c8b7c6900f73"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Model       | Total MACs   | ANN Energy (µJ) | SNN Energy (µJ) |\n",
            "|:------------|:-------------|:----------------|:----------------|\n",
            "| LENet CCB   |   33,973,008 |          156.28 |           93.15 |\n",
            "| LENet FCL   |   33,973,008 |          156.28 |           93.15 |\n",
            "\n",
            "Constants used: E_MAC = (4.6 pJ), E_AC = (0.9 pJ). SNN parameters: T = 100, Assumed Average Firing Rate (fr) = 2%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NN Visualization\n",
        "\n",
        "# This block should be placed after the training of the CNN models\n",
        "# you wish to visualize. For example, after the\n",
        "# \"LENet to SNN Conversion Framework execution\" cell (which trains cnn_model_lenet_ccb) and\n",
        "# \"LENet_FCL to SNN Conversion Framework execution\" cell (which trains cnn_model_lenet_fcl).\n",
        "# The `cnn_model_lenet_ccb` and `cnn_model_lenet_fcl` variables from those cells\n",
        "# will be used for visualization.\n",
        "\n",
        "# Install nnviz if you haven't already (uncomment the line below if needed)\n",
        "# !pip install nnviz -q\n",
        "\n",
        "# Import necessary nnviz modules\n",
        "from nnviz import drawing, inspection\n",
        "import torch # Ensure torch is imported\n",
        "import os # For creating directories\n",
        "\n",
        "print(\"Starting Neural Network Visualization...\")\n",
        "\n",
        "# --- Create 'viz' subdirectory if it doesn't exist ---\n",
        "VIZ_SUBFOLDER = \"viz\"\n",
        "if not os.path.exists(VIZ_SUBFOLDER):\n",
        "    os.makedirs(VIZ_SUBFOLDER)\n",
        "    print(f\"Created subfolder: {VIZ_SUBFOLDER}\")\n",
        "else:\n",
        "    print(f\"Subfolder '{VIZ_SUBFOLDER}' already exists.\")\n",
        "\n",
        "# Define output formats\n",
        "OUTPUT_FORMATS = [\"png\", \"svg\", \"pdf\"]\n",
        "\n",
        "\n",
        "def visualize_and_save_model(\n",
        "    model_instance, model_base_name, subfolder, formats\n",
        "):\n",
        "    \"\"\"Helper function to inspect and save model visualization in multiple formats.\"\"\"\n",
        "    print(f\"\\nVisualizing {model_base_name} CNN model...\")\n",
        "    original_device = next(model_instance.parameters()).device\n",
        "    try:\n",
        "        # Move model to CPU for inspection (safer for fx tracing)\n",
        "        model_cpu = model_instance.to(\"cpu\")\n",
        "        print(f\"Moved {model_base_name} to CPU for inspection.\")\n",
        "\n",
        "        # Create an inspector\n",
        "        inspector = inspection.TorchFxInspector()\n",
        "\n",
        "        print(f\"Inspecting {model_base_name} on CPU...\")\n",
        "        graph = inspector.inspect(model_cpu)\n",
        "\n",
        "        for fmt in formats:\n",
        "            viz_filename = os.path.join(\n",
        "                subfolder, f\"{model_base_name}_cnn_architecture.{fmt}\"\n",
        "            )\n",
        "            # nnviz's GraphvizDrawer determines format by filename extension\n",
        "            drawer = drawing.GraphvizDrawer(viz_filename)\n",
        "            drawer.draw(graph)\n",
        "            print(\n",
        "                f\"{model_base_name} CNN model visualization saved to: {viz_filename}\"\n",
        "            )\n",
        "\n",
        "        print(\n",
        "            f\"Note: If output files are empty or show errors, ensure Graphviz \"\n",
        "            f\"(specifically the 'dot' command) is installed and accessible in your system's PATH.\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\n",
        "            f\"An error occurred during {model_base_name} CNN model visualization: {e}\"\n",
        "        )\n",
        "        print(\"Troubleshooting tips:\")\n",
        "        print(\"- Ensure 'nnviz' and 'graphviz' (dot executable) are installed and in PATH.\")\n",
        "        print(\n",
        "            \"- The model structure might contain operations not traceable by torch.fx.\"\n",
        "        )\n",
        "    finally:\n",
        "        # Attempt to move model back to its original device\n",
        "        try:\n",
        "            model_instance.to(original_device)\n",
        "            print(\n",
        "                f\"Moved {model_base_name} back to its original device: {original_device}.\"\n",
        "            )\n",
        "        except Exception as e_move:\n",
        "            print(\n",
        "                f\"Could not move {model_base_name} back to {original_device}: {e_move}\"\n",
        "            )\n",
        "\n",
        "\n",
        "# --- LENet CCB (Classification Convolution Block) CNN Model Visualization ---\n",
        "if \"cnn_model_lenet_ccb\" in locals() and isinstance(\n",
        "    cnn_model_lenet_ccb, torch.nn.Module\n",
        "):\n",
        "    visualize_and_save_model(\n",
        "        cnn_model_lenet_ccb, \"LENet_CCB\", VIZ_SUBFOLDER, OUTPUT_FORMATS\n",
        "    )\n",
        "else:\n",
        "    print(\n",
        "        \"\\nLENet CCB model ('cnn_model_lenet_ccb') not found or not a torch.nn.Module. \"\n",
        "        \"Skipping LENet CCB CNN visualization.\"\n",
        "    )\n",
        "\n",
        "# --- LENet FCL (Fully Connected Layer) CNN Model Visualization ---\n",
        "if \"cnn_model_lenet_fcl\" in locals() and isinstance(\n",
        "    cnn_model_lenet_fcl, torch.nn.Module\n",
        "):\n",
        "    visualize_and_save_model(\n",
        "        cnn_model_lenet_fcl,\n",
        "        \"LENet_FCL\",\n",
        "        VIZ_SUBFOLDER,\n",
        "        OUTPUT_FORMATS,\n",
        "    )\n",
        "else:\n",
        "    print(\n",
        "        \"\\nLENet FCL model ('cnn_model_lenet_fcl') not found or not a torch.nn.Module. \"\n",
        "        \"Skipping LENet FCL CNN visualization.\"\n",
        "    )\n",
        "\n",
        "print(\"\\nNeural Network Visualization process complete.\")\n",
        "\n",
        "# --- Move models back to GPU (if they were on GPU and exist) ---\n",
        "print(\"\\nAttempting to move models back to their original device (e.g., GPU)...\")\n",
        "\n",
        "# Check for the main 'device' variable from your training setup\n",
        "if \"device\" not in locals():\n",
        "    print(\n",
        "        \"Global 'device' variable not found. Cannot determine target device for models.\"\n",
        "    )\n",
        "    # Fallback or define device if necessary, e.g.:\n",
        "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # print(f\"Using fallback device: {device}\")\n",
        "\n",
        "if \"device\" in locals(): # Proceed if 'device' is defined\n",
        "    if \"cnn_model_lenet_ccb\" in locals() and isinstance(\n",
        "        cnn_model_lenet_ccb, torch.nn.Module\n",
        "    ):\n",
        "        try:\n",
        "            current_model_device_ccb = next(\n",
        "                cnn_model_lenet_ccb.parameters()\n",
        "            ).device\n",
        "            if str(current_model_device_ccb) != str(device):\n",
        "                cnn_model_lenet_ccb.to(device)\n",
        "                print(\n",
        "                    f\"Moved 'cnn_model_lenet_ccb' from {current_model_device_ccb} to {device}.\"\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    f\"'cnn_model_lenet_ccb' is already on the target device: {device}.\"\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"Error moving 'cnn_model_lenet_ccb' to {device}: {e}\"\n",
        "            )\n",
        "    else:\n",
        "        print(\n",
        "            \"'cnn_model_lenet_ccb' not found or not a valid model, skipping move.\"\n",
        "        )\n",
        "\n",
        "    if \"cnn_model_lenet_fcl\" in locals() and isinstance(\n",
        "        cnn_model_lenet_fcl, torch.nn.Module\n",
        "    ):\n",
        "        try:\n",
        "            current_model_device_fcl = next(\n",
        "                cnn_model_lenet_fcl.parameters()\n",
        "            ).device\n",
        "            if str(current_model_device_fcl) != str(device):\n",
        "                cnn_model_lenet_fcl.to(device)\n",
        "                print(\n",
        "                    f\"Moved 'cnn_model_lenet_fcl' from {current_model_device_fcl} to {device}.\"\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    f\"'cnn_model_lenet_fcl' is already on the target device: {device}.\"\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"Error moving 'cnn_model_lenet_fcl' to {device}: {e}\"\n",
        "            )\n",
        "    else:\n",
        "        print(\n",
        "            \"'cnn_model_lenet_fcl' not found or not a valid model, skipping move.\"\n",
        "        )\n",
        "else:\n",
        "    print(\"Skipping final model device move as 'device' variable is not defined.\")\n"
      ],
      "metadata": {
        "id": "V8p4PokzI0HX",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "b8e26882-da67-458f-f9ac-7a376e8bf901",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'nnviz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-76b987583285>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Import necessary nnviz modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnnviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrawing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minspection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;31m# Ensure torch is imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m \u001b[0;31m# For creating directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nnviz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}